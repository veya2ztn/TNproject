{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will test several Model proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=torch.randn(16,16)\n",
    "B=torch.randn(16,16)\n",
    "C=torch.randn(16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.sigmoid(C.multiply(torch.sigmoid(A.multiply(B))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sigmoid(torch.sigmoid(C.multiply(A)).multiply(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.8962)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple MPS layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     2,
     38,
     51,
     58
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MPSLinear(nn.Module):\n",
    "    '''\n",
    "    For a naive Linear Layer(in_features,out_features,\n",
    "                             in_physics_bond = 2, out_physics_bond=2, virtual_bond_dim=2, \n",
    "                             bias=True,label_position='center',init_std=1e-10\n",
    "                                       ): \n",
    "        input  (B, in_features)\n",
    "        output (B, out_features)\n",
    "    For s simplest MPSLayer(in_features: int, out_features: int, \n",
    "                            in_physics_bond: int, out_physics_bond: int, virtual_bond_dim:int,\n",
    "                            bias: bool = True, label_position: int or str): \n",
    "        input  (B, in_features , in_physics_bond)\n",
    "        output (B, out_features,out_physics_bond)\n",
    "    '''\n",
    "    def __init__(self, in_features,out_features,\n",
    "                                       in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=2, \n",
    "                                       bias=True,label_position='center',init_std=1e-10):\n",
    "        super(MPSLinear, self).__init__()\n",
    "        if label_position is 'center':\n",
    "            label_position = in_features//2\n",
    "        assert type(label_position) is int\n",
    "        self.in_features   = in_features\n",
    "        self.out_features  = out_features\n",
    "        self.vbd           = virtual_bond_dim\n",
    "        self.ipb           = in_physics_bond\n",
    "        self.opb           = out_physics_bond\n",
    "        self.hn            = label_position\n",
    "        left_num           = self.hn\n",
    "        right_num          = in_features - left_num\n",
    "\n",
    "        bias_mat = torch.eye(self.vbd).unsqueeze(-1).repeat(1,1,self.ipb)\n",
    "        self.left_tensors = nn.Parameter(init_std * torch.randn(left_num         ,self.vbd,self.vbd, self.ipb)+ bias_mat)\n",
    "        self.rigt_tensors = nn.Parameter(init_std * torch.randn(right_num        ,self.vbd,self.vbd, self.ipb)+ bias_mat)\n",
    "        \n",
    "        bias_mat = torch.eye(self.vbd).unsqueeze(-1).repeat(1,1,self.opb)\n",
    "        self.cent_tensors = nn.Parameter(init_std * torch.randn(self.out_features,self.vbd,self.vbd, self.opb)+ bias_mat)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_chain_contraction_fast(tensor):\n",
    "        size   = int(tensor.shape[0])\n",
    "        while size > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "        return tensor.squeeze(0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_chain_contraction_memory_save(tensor):\n",
    "        size      = int(tensor.shape[0])\n",
    "        now_tensor= tensor[0]\n",
    "        for next_tensor in tensor[1:]:\n",
    "            now_tensor = torch.einsum(\"bik,bkj->bij\",now_tensor, next_tensor)\n",
    "        return now_tensor\n",
    "    \n",
    "    def get_chain_contraction(self,tensor):\n",
    "        size   = int(tensor.shape[0])\n",
    "        D      = int(tensor.shape[-1])\n",
    "        print(size)\n",
    "        print(D)\n",
    "        if D>30:\n",
    "            return self.get_chain_contraction_memory_save(tensor)\n",
    "        else:\n",
    "            return self.get_chain_contraction_fast(tensor)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        # the input data shape is (B,L,pd)\n",
    "        # expand to convolution patch\n",
    "        embedded_data= input_data\n",
    "        left_tensors = torch.einsum('wijp,nwp->wnij',self.left_tensors,embedded_data[:,:self.hn])#i.e. (K,NB,b,b)\n",
    "        rigt_tensors = torch.einsum('wijp,nwp->wnij',self.rigt_tensors,embedded_data[:,-self.hn:])#i.e.(K,NB,b,b)\n",
    "\n",
    "        left_tensors = self.get_chain_contraction(left_tensors) #i.e. (NB,b,b)\n",
    "        rigt_tensors = self.get_chain_contraction(rigt_tensors) #i.e. (NB,b,b)\n",
    "\n",
    "        tensor  = torch.einsum('bip,oplt,bli->bot',left_tensors,self.cent_tensors,rigt_tensors)\n",
    "        # (NB,b,b) <-> (T,b,b,o) <-> (NB,b,b) ==> (NB,T,t)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     7,
     28,
     42
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "\n",
    "def batch_grad_proj(X, D, inner='canonical'):\n",
    "    r\"\"\"project grad data onto Stiefel manifold\n",
    "    Args:\n",
    "        X (batch tensor): tensor of parameters in matrix form\n",
    "        dX (batch tensor): grad tensor of parameters in matrix form\n",
    "        inner (str, optional): inner product type on Stiefel manifold (default: 'canonical')\n",
    "    Return:\n",
    "        G (batch tensor): gradient tensor in matrix form\n",
    "        A (batch tensor): the tensor G = AX in matrix form\n",
    "    \"\"\"\n",
    "    XT =  X.transpose(-1,-2)\n",
    "    DT = dX.transpose(-1,-2)\n",
    "    G  = (dX - 0.5*torch.einsum(\"...ab,...bc,...cd->...ad\",X,XT,D)\n",
    "             - 0.5*torch.einsum(\"...ab,...bc,...cd->...ad\",X,DT,X))\n",
    "    \n",
    "    A = (     torch.einsum(\"...ab,...bc->...ac\", D, XT) \n",
    "        -     torch.einsum(\"...ab,...bc->...ac\", X, DT)\n",
    "        + 0.5*torch.einsum(\"...ab,...bc,...cd,...de->...ae\",X,DT,X,XT)\n",
    "        - 0.5*torch.einsum(\"...ab,...bc,...cd,...de->...ae\",X,XT,D,XT)\n",
    "        )\n",
    "    return G, A\n",
    "def grad_proj(X, dX, inner='canonical'):\n",
    "    r\"\"\"project grad data onto Stiefel manifold\n",
    "    Args:\n",
    "        X (tensor): tensor of parameters in matrix form\n",
    "        dX (tensor): grad tensor of parameters in matrix form\n",
    "        inner (str, optional): inner product type on Stiefel manifold (default: 'canonical')\n",
    "    Return:\n",
    "        G (tensor): gradient tensor in matrix form\n",
    "        A (tensor): the tensor G = AX in matrix form\n",
    "    \"\"\"\n",
    "    G = dX - 0.5 * (X @ X.t().conj() @ dX + X @ dX.t().conj() @ X)\n",
    "    A = dX @ X.t().conj() - X @ dX.t().conj() + 0.5 * X @ (dX.t().conj() @ X - X.t().conj() @ dX) @ X.t().conj()\n",
    "    return G, A    \n",
    "\n",
    "def batch_retraction(X, G, A, lr, method='SVD', adapt=False):\n",
    "    r\"\"\"retract the updated tensor onto Stiefel manifold\n",
    "    Args:\n",
    "        X (batch tensor): tensor of parameters in matrix form\n",
    "        G (batch tensor): gradient tensor in matrix form\n",
    "        A (batch tensor): the tensor G = AX in matrix form\n",
    "        lr (float): learning rate\n",
    "        method (str, optional): method for retraction into Stiefel manifold ('SVD', 'Cayley') (default: 'SVD')\n",
    "    \"\"\"\n",
    "    if adapt:\n",
    "        lr = min(lr, 4.0 / (torch.linalg.norm(A).item() + 1e-8)) \n",
    "    if method == 'SVD':\n",
    "        U, _, Vt = LA.svd((X - lr * G).cpu().numpy(), full_matrices=False)\n",
    "        X_out = torch.tensor(U @ Vt)\n",
    "        # U, _, V = svd_(X - lr * G)\n",
    "        # X_out = U @ V.t()\n",
    "\n",
    "    elif method == 'Cayley':\n",
    "        X_out = X - lr * G\n",
    "        for i in range(40):\n",
    "            X_out = X - 0.3 * lr * A @ (X + X_out)\n",
    "\n",
    "    return X_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def grad_proj(X, D, inner='canonical'):\n",
    "    r\"\"\"project grad data onto Stiefel manifold\n",
    "    Args:\n",
    "        X (batch tensor): tensor of parameters in matrix form\n",
    "        dX (batch tensor): grad tensor of parameters in matrix form\n",
    "        inner (str, optional): inner product type on Stiefel manifold (default: 'canonical')\n",
    "    Return:\n",
    "        G (batch tensor): gradient tensor in matrix form\n",
    "        A (batch tensor): the tensor G = AX in matrix form\n",
    "    \"\"\"\n",
    "    XT =  X.transpose(-1,-2)\n",
    "    DT = dX.transpose(-1,-2)\n",
    "    G  = (dX - 0.5*torch.einsum(\"...ab,...bc,...cd->...ad\",X,XT,D)\n",
    "             - 0.5*torch.einsum(\"...ab,...bc,...cd->...ad\",X,DT,X))\n",
    "\n",
    "    A = (     torch.einsum(\"...ab,...bc->...ac\", D, XT) \n",
    "        -     torch.einsum(\"...ab,...bc->...ac\", X, DT)\n",
    "        + 0.5*torch.einsum(\"...ab,...bc,...cd,...de->...ae\",X,DT,X,XT)\n",
    "        - 0.5*torch.einsum(\"...ab,...bc,...cd,...de->...ae\",X,XT,D,XT)\n",
    "        )\n",
    "    return G, A\n",
    "    \n",
    "class Left_Orthogonal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor):\n",
    "        ctx.save_for_backward(tensor)\n",
    "        tensor=\n",
    "        U,_,V = torch.linalg.svd(tensor)\n",
    "        return torch.einsum(\"...ab,...bc->...ac\",U,V)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, D):\n",
    "        X = ctx.saved_tensors\n",
    "        XT =  X.transpose(-1,-2)\n",
    "        DT = dX.transpose(-1,-2)\n",
    "        G  = (dX - 0.5*torch.einsum(\"...ab,...bc,...cd->...ad\",X,XT,D)\n",
    "                 - 0.5*torch.einsum(\"...ab,...bc,...cd->...ad\",X,DT,X))\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X     = torch.randn(2,3,4)\n",
    "dX    = torch.randn(2,3,4)\n",
    "batch_G, batch_A  = batch_grad_proj(X, dX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     3,
     70
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensornetwork as tn\n",
    "from tensornetwork import contractors\n",
    "tn.set_default_backend(\"pytorch\")\n",
    "class MPSLinear_tn_loop(nn.Module):\n",
    "    '''\n",
    "    For s simplest MPSLayer(in_features: int, out_features: int,\n",
    "                            in_physics_bond: int, \n",
    "                            out_physics_bond: int, virtual_bond_dim:int,\n",
    "                            bias: bool = True, label_position: int or str):\n",
    "        input  (B, in_features , in_physics_bond)\n",
    "        output (B, out_features)\n",
    "    '''\n",
    "    def __init__(self, in_features,out_features,\n",
    "                       in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=2,\n",
    "                       bias=True,label_position='center',init_std=1e-10,**kargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if label_position is 'center':\n",
    "            label_position = in_features//2\n",
    "        assert type(label_position) is int\n",
    "        self.in_features   = in_features\n",
    "        self.out_features  = out_features\n",
    "        self.vbd           = virtual_bond_dim\n",
    "        self.ipb           = in_physics_bond\n",
    "        self.opb           = out_physics_bond\n",
    "        self.hn            = label_position\n",
    "        \n",
    "        left_num           = self.hn\n",
    "        right_num          = in_features - left_num\n",
    "\n",
    "        bias_mat     = torch.eye(self.ipb,self.vbd)\n",
    "        left_end     = init_std * torch.randn(self.ipb,self.vbd) + bias_mat\n",
    "\n",
    "        bias_mat     = torch.eye(self.vbd, self.ipb)\n",
    "        right_end    = init_std * torch.randn(self.vbd, self.ipb)+ bias_mat\n",
    "\n",
    "        bias_mat     = torch.eye(self.vbd).unsqueeze(1).repeat(1,self.ipb,1)\n",
    "        left_tensors = init_std * torch.randn(left_num-1 ,self.vbd, self.ipb , self.vbd)+ bias_mat\n",
    "        rigt_tensors = init_std * torch.randn(right_num-1,self.vbd, self.ipb,self.vbd)+ bias_mat\n",
    "\n",
    "\n",
    "        bias_mat     = torch.eye(self.vbd).unsqueeze(1).repeat(1,self.out_features,1)\n",
    "        cent_tensors = init_std * torch.randn(self.vbd,self.out_features,self.vbd)+ bias_mat\n",
    "\n",
    "        mps_var      = [left_end] + list(left_tensors)  + [cent_tensors] + list(rigt_tensors) + [right_end]\n",
    "        self.mps_var = [nn.Parameter(v) for v in mps_var]\n",
    "        self.center  = left_num\n",
    "        for i, v in enumerate(self.mps_var):\n",
    "            self.register_parameter(f'mps{i}', param=v)\n",
    "\n",
    "    def contract_mps_with_input(self,input):\n",
    "        assert len(input) == len(self.mps_var)-1\n",
    "        mps_list_1   = self.mps_var\n",
    "        mps_nodes_1  = [tn.Node(v, name=f\"t{i}\") for i,v in enumerate(mps_list_1)]\n",
    "        mps_edges_1  = [mps_nodes_1[i][-1]^mps_nodes_1[i+1][0] for i in range(len(mps_nodes_1)-1)]\n",
    "        inp_nodes    = [tn.Node(v, name=f\"i{i}\") for i,v in enumerate(input)]\n",
    "        for i,input_node in enumerate(inp_nodes):\n",
    "            j = i if i < self.center else i+1\n",
    "            mps_physicd_edge = mps_nodes_1[j][0] if j==0 else mps_nodes_1[j][1]\n",
    "            inp_physics_edge = input_node[0]\n",
    "            tn.connect(mps_physicd_edge,inp_physics_edge,name=f\"p_{i}\")\n",
    "\n",
    "        ans = contractors.auto(mps_nodes_1+inp_nodes,\n",
    "                              output_edge_order=[mps_nodes_1[self.center][1]]).tensor\n",
    "        return ans\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = torch.stack([self.contract_mps_with_input(single_input) for single_input in inputs])\n",
    "        return out\n",
    "    \n",
    "class MPSLinear_tn_batch(MPSLinear_tn_loop):\n",
    "    def forward(self, inputs):\n",
    "        num        = len(self.mps_var)\n",
    "        mps_nodes  = [tn.Node(v, name=f\"t{i}\") for i,v in enumerate(self.mps_var)]\n",
    "        mps_edges  = [mps_nodes[i][-1]^mps_nodes[i+1][0] for i in range(num-1)]\n",
    "\n",
    "\n",
    "        inputs= inputs.permute(1,2,0)#(B,num,k)->(num,k,B)\n",
    "        out   = torch.diag_embed(inputs)#(num,k,B)->(num,k,B,B)\n",
    "        out   = out.permute(0,2,1,3)#(num,k,B,B)->(num,B,k,B)\n",
    "        out   = [v for v in out]\n",
    "        out[0]= torch.diagonal(out[0], dim1=0, dim2=-1).transpose(1,0)#(B,k,B) -> #(B,k)\n",
    "\n",
    "        inp_nodes=[tn.Node(v, name=f\"i{i}\") for i,v in enumerate(out)]\n",
    "        inp_edges=[inp_nodes[0][0]^inp_nodes[1][0]]+ [\n",
    "            inp_nodes[i][-1]^inp_nodes[i+1][0] for i in range(1,len(inp_nodes)-1)]\n",
    "\n",
    "        for i,input_node in enumerate(inp_nodes):\n",
    "            j = i if i < self.center else i+1\n",
    "            mps_physicd_edge = mps_nodes[j][0] if j==0 else mps_nodes[j][1]\n",
    "            inp_physics_edge = input_node[1]\n",
    "            tn.connect(mps_physicd_edge,inp_physics_edge,name=f\"p_{i}\")\n",
    "\n",
    "        ans = contractors.auto(mps_nodes+inp_nodes,output_edge_order=[inp_nodes[-1][2],mps_nodes[self.center][1]]).tensor\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = MPSLinear_tn_loop(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=10,\n",
    "                  bias=False,label_position='center',init_std=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = MPSLinear_tn_batch(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=10,\n",
    "                  bias=False,label_position='center',init_std=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### AMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7763e-07)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(G,batch_G[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from models.amps import AMPSShare\n",
    "class AMPSShare(nn.Module):\n",
    "    '''\n",
    "    This version may fast, but will cost much more memory\n",
    "    n        : the length of input tensor sequence\n",
    "    bond_dim : the virtual bond dim. The capacity of model.\n",
    "    phys_dim : the feature/class number\n",
    "    ------------------------------\n",
    "    Input:  any data/spin configurations, shape: (B, n ,phys_dim)\n",
    "    Output: prob_matrix of each sample, shape: (B, n, phys_dim), pass softmax to get probility.\n",
    "    -------------------------------\n",
    "    Weight Cost:\n",
    "        n * bond_dim * bond_dim * phys_dim\n",
    "    '''\n",
    "    def __init__(self, n=784, bond_dim=10, phys_dim=2,std=1e-8):\n",
    "        super(AMPSShare, self).__init__()\n",
    "        # Initialize AMPS model parameters, which is a (n, D, D, 2) tensor\n",
    "        self.register_buffer('bias_mat', torch.eye(bond_dim).unsqueeze(-1).repeat(1,1,phys_dim))\n",
    "        # bias_mat: which is realy important when n>>1\n",
    "        self.tensors = nn.Parameter(std * torch.randn(n, bond_dim, bond_dim, phys_dim)+self.bias_mat)\n",
    "        # Set attributes\n",
    "        self.n = n\n",
    "        self.bond_dim = bond_dim\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, embedded_data):\n",
    "\n",
    "        bs = embedded_data.shape[0]\n",
    "        # local feature map, x_j -> [x_j, 1-x_j]\n",
    "        #-> embedded_data = torch.stack([data, 1.0 - data], dim=2)  # (bs, n, 2)\n",
    "        ##logx_hat = torch.zeros_like(embedded_data)\n",
    "        ##logx_hat[:, 0, :] = F.log_softmax(self.tensors[0, 0, 0], dim=0)\n",
    "        prob_matrix = self.tensors[0, 0, 0].repeat((bs,1)).unsqueeze(1) # (bs,1,2)\n",
    "        mats = torch.einsum('lri,bi->blr', self.tensors[0] , embedded_data[:, 0, :])\n",
    "        left_vec = mats[:, 0:1, :]  # (bs,  D)\n",
    "        for idx in range(1, self.n):\n",
    "            # compute p(s_2 | s_1) and so on\n",
    "            logits = torch.einsum('br, ri->bi', left_vec.squeeze(1),self.tensors[idx,:,0,:])\n",
    "            #(bs,D) <-> (D,2) ->(bs,2)\n",
    "            prob_matrix = torch.cat([prob_matrix,logits.unsqueeze(1)], dim=1)\n",
    "            #(bs, n-2, 2) + (bs,1,2) -> (bs, n-1, 2)\n",
    "            ##logx_hat[:, idx, :] = F.log_softmax(logits, dim=1)\n",
    "            mats = torch.einsum('lri,bi->blr', self.tensors[idx, :, :, :] , embedded_data[:, idx, :])\n",
    "            #(D,D,2) <-> (bs,2) ->(bs,D,D)\n",
    "            left_vec = torch.bmm(left_vec, mats)  # (bs, 1, D)\n",
    "        # compute log prob\n",
    "        return prob_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1861e-06)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(A,batch_A[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### naive contractor: face dimenstion explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# tensor = torch.randn(8,8,6,6,6,6)\n",
    "# while tensor.shape[0]>2:\n",
    "#     print(tensor.shape)\n",
    "#     lu_tensor = tensor[0::2,0::2]\n",
    "#     ld_tensor = tensor[0::2,1::2]\n",
    "#     ru_tensor = tensor[1::2,0::2]\n",
    "#     rd_tensor = tensor[1::2,1::2]\n",
    "#     tensor     = torch.einsum(\"xyabcd,xyhdij,xycefg,xyigkl->xyahbefkjl\",\n",
    "#                             lu_tensor,\n",
    "#                             ld_tensor,\n",
    "#                             ru_tensor,\n",
    "#                             rd_tensor).flatten(4,5).flatten(-4,-3).flatten(2,3).flatten(-2,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### cotengra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensornetwork.contractors.opt_einsum_paths.utils import *\n",
    "from opt_einsum.paths import greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import opt_einsum as oe\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import opt_einsum as oe\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"pytorch\")\n",
    "    from tensornetwork.contractors.opt_einsum_paths.utils import *\n",
    "tensor = torch.randn(16,16,2,2,2,2).cuda()\n",
    "\n",
    "node_array = []\n",
    "W,H = tensor.shape[:2]\n",
    "for i in range(W):\n",
    "    node_line = []\n",
    "    for j in range(H):\n",
    "        node = tn.Node(tensor[i][j],name=f\"{i}-{j}\")\n",
    "        node_line.append(node)\n",
    "    node_array.append(node_line)\n",
    "\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        if j==H-1:tn.connect(node_array[i][j][2],node_array[i  ][0  ][0],f\"{i}{j}<->{i}{0}\")\n",
    "        else:     tn.connect(node_array[i][j][2],node_array[i  ][j+1][0],f\"{i}{j}<->{i}{j+1}\")\n",
    "        if i==W-1:tn.connect(node_array[i][j][3],node_array[0  ][j  ][1],f\"{i}{j}<->{0}{j}\")\n",
    "        else:     tn.connect(node_array[i][j][3],node_array[i+1][j  ][1],f\"{i}{j}<->{i+1}{j}\")\n",
    "\n",
    "node_list = [item for sublist in node_array for item in sublist]\n",
    "nodes = node_list\n",
    "input_sets = [set(node.edges) for node in nodes]\n",
    "output_set = get_subgraph_dangling(nodes)\n",
    "size_dict = {edge: edge.dimension for edge in get_all_edges(nodes)}\n",
    "\n",
    "operands = []\n",
    "for node,edge_label in zip(node_list,input_sets):\n",
    "    operands+=[node.tensor,[edge.name for edge in edge_label]]\n",
    "\n",
    "path,info = oe.contract_path(*operands)\n",
    "\n",
    "small_cores =[node.tensor for node in node_list]\n",
    "import tqdm as tqdm\n",
    "import cotengra as ctg\n",
    "\n",
    "sf = ctg.SliceFinder(info, target_size=2**27)\n",
    "inds_to_slice, cost_of_slicing = sf.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "import cotengra as ctg\n",
    "\n",
    "sf = ctg.SliceFinder(info, target_size=2**27)\n",
    "inds_to_slice, cost_of_slicing = sf.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67108864\n",
      "2.9753197715379116\n"
     ]
    }
   ],
   "source": [
    "print(cost_of_slicing.size    ) # the new largest intermediate\n",
    "print(cost_of_slicing.overhead)  # theoretical 'slowdown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/262144 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/262144 [00:00<30:05, 145.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 30/262144 [00:00<44:00, 99.28it/s] \u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 41/262144 [00:00<47:41, 91.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 51/262144 [00:00<49:42, 87.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 60/262144 [00:00<50:56, 85.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 69/262144 [00:00<51:48, 84.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 78/262144 [00:00<52:24, 83.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 87/262144 [00:00<53:13, 82.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 96/262144 [00:01<53:42, 81.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 105/262144 [00:01<53:43, 81.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 114/262144 [00:01<53:46, 81.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 123/262144 [00:01<53:46, 81.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 132/262144 [00:01<53:47, 81.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 141/262144 [00:01<53:47, 81.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 150/262144 [00:01<53:49, 81.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 159/262144 [00:01<53:48, 81.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 168/262144 [00:01<53:57, 80.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 177/262144 [00:02<54:10, 80.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 186/262144 [00:02<54:04, 80.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 195/262144 [00:02<54:04, 80.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 204/262144 [00:02<54:00, 80.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 213/262144 [00:02<54:03, 80.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 222/262144 [00:02<54:02, 80.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 231/262144 [00:02<53:58, 80.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 240/262144 [00:02<53:56, 80.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 249/262144 [00:02<54:02, 80.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 258/262144 [00:03<54:13, 80.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 267/262144 [00:03<54:05, 80.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 276/262144 [00:03<54:01, 80.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 285/262144 [00:03<54:05, 80.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 294/262144 [00:03<54:06, 80.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 303/262144 [00:03<54:04, 80.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 312/262144 [00:03<54:07, 80.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 321/262144 [00:03<54:03, 80.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 330/262144 [00:04<54:05, 80.68it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7c7e88620acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSlicedContractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmall_cores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 100%|██████████| 512/512 [00:55<00:00,  9.30it/s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-7c7e88620acd>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSlicedContractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmall_cores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 100%|██████████| 512/512 [00:55<00:00,  9.30it/s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/cotengra/slicer.py\u001b[0m in \u001b[0;36mcontract_slice\u001b[0;34m(self, i, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m    585\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sliced_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *arrays, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contract_with_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_constants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_constants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_contract\u001b[0;34m(self, arrays, out, backend, evaluate_constants)\u001b[0m\n\u001b[1;32m    696\u001b[0m                               \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                               \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_constants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                               **self.einsum_kwargs)\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_contract_with_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# Contract!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mnew_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtmp_operands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;31m# Build a new view if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/sharing.py\u001b[0m in \u001b[0;36mcached_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached_tensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrently_sharing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# hash based on the (axes_x,axes_y) form of axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \"\"\"\n\u001b[1;32m    373\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensordot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/backends/torch.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TORCH_HAS_TENSORDOT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mxnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, dims)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mdims_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mdims_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_b\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcartesian_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sc = sf.SlicedContractor([*small_cores])\n",
    "result = sum(sc.contract_slice(i) for i in tqdm.trange(sc.nslices))\n",
    "# 100%|██████████| 512/512 [00:55<00:00,  9.30it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import opt_einsum as oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8), (0, 7), (0, 7), (0, 6), (0, 5), (0, 4), (0, 3), (1, 2), (0, 1), (0, 9), (0, 8), (0, 8), (0, 7), (0, 6), (1, 5), (1, 4), (1, 3), (1, 4), (3, 13), (7, 12), (3, 11), (3, 10), (3, 9), (3, 8), (6, 7), (3, 6), (3, 5), (3, 4), (2, 6), (2, 10), (1, 11), (4, 11), (0, 13), (5, 12), (5, 12), (6, 12), (5, 12), (6, 11), (6, 11), (6, 12), (4, 12), (6, 13), (7, 14), (7, 13), (4, 12), (6, 11), (6, 11), (6, 10), (6, 10), (0, 11), (0, 11), (5, 10), (0, 9), (4, 9), (2, 9), (1, 8), (3, 7), (3, 7), (3, 9), (3, 9), (3, 10), (3, 10), (3, 9), (3, 8), (1, 7), (2, 6), (6, 7), (2, 6), (2, 5), (2, 5), (5, 17), (6, 18), (12, 21), (4, 23), (13, 22), (4, 21), (15, 23), (10, 23), (10, 22), (7, 27), (11, 27), (11, 27), (11, 26), (7, 29), (9, 29), (8, 28), (15, 29), (7, 28), (13, 29), (6, 31), (6, 30), (13, 29), (16, 29), (1, 20), (21, 26), (21, 25), (6, 24), (15, 30), (15, 29), (8, 28), (10, 27), (6, 27), (10, 26), (8, 25), (9, 24), (5, 23), (2, 21), (10, 25), (5, 24), (2, 23), (1, 33), (1, 21), (0, 31), (2, 19), (4, 21), (5, 10), (7, 9), (4, 20), (1, 19), (0, 20), (14, 15), (6, 10), (3, 9), (2, 9), (5, 15), (0, 13), (6, 9), (5, 8), (4, 13), (3, 13), (1, 8), (1, 12), (0, 6), (1, 10), (3, 9), (7, 8), (3, 7), (0, 6), (2, 5), (3, 4), (2, 3), (0, 2), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(12,12,4,4,4,4)/2\n",
    "#tensor     = torch.randn(2,2,16,16,16,16)/10\n",
    "computer_vie_tn(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Kite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D = 5\n",
    "W=H=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import opt_einsum as oe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for even size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "physics_tensor = torch.randn(W,H,D,D,D,D)\n",
    "size = W\n",
    "hat_tensor_list=[]\n",
    "while size>1:\n",
    "    hat_tensor = torch.randn(2,2,size//2,size//2,D,D,D)\n",
    "    size       = size//2\n",
    "    hat_tensor_list.append(hat_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "left_top = physics_tensor[0::2,0::2]\n",
    "rigt_top = physics_tensor[0::2,1::2]\n",
    "rigt_bot = physics_tensor[1::2,1::2]\n",
    "left_bot = physics_tensor[1::2,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for hat_tensor in hat_tensor_list:\n",
    "    left_top = physics_tensor[0::2,0::2]\n",
    "    rigt_top = physics_tensor[0::2,1::2]\n",
    "    rigt_bot = physics_tensor[1::2,1::2]\n",
    "    left_bot = physics_tensor[1::2,0::2]\n",
    "    left_hat = hat_tensor[0,0]\n",
    "    topp_hat = hat_tensor[0,1]\n",
    "    rigt_hat = hat_tensor[1,1]\n",
    "    bott_hat = hat_tensor[1,0]\n",
    "    physics_tensor = oe.contract(\"...abcd,...cefg,...hgij,...kdhl,...mak,...bne,...foi,...ljp->...mnop\",\n",
    "                        left_top,rigt_top,rigt_bot,left_bot,\n",
    "                        left_hat,topp_hat,rigt_hat,bott_hat,optimize=path)\n",
    "    if len(physics_tensor)==1:break\n",
    "torch.einsum(\"...aabb->...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physics_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path,info = oe.contract_path(\"...abcd,...cefg,...hgij,...kdhl,...mak,...bne,...foi,...ljp->...mnop\",\n",
    "                             left_top,rigt_top,rigt_bot,left_bot,\n",
    "                             left_hat,topp_hat,rigt_hat,bott_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for odd size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-23-82419a86360d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-82419a86360d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (list(range(0,W//2,2))+list(range(W-W//2,W,2))\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "(list(range(0,W//2,2))+list(range(W-W//2,W,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "W=H=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_corner_idx(W,H):\n",
    "    assert W==H\n",
    "    if W%2 == 0:\n",
    "        left_top_idx = [t.transpose().flatten() for t in np.meshgrid(range(0,W,2),range(0,H,2))]\n",
    "        rigt_top_idx = [t.transpose().flatten() for t in np.meshgrid(range(0,W,2),range(1,H,2))]\n",
    "        rigt_bot_idx = [t.transpose().flatten() for t in np.meshgrid(range(1,W,2),range(1,H,2))]\n",
    "        left_bot_idx = [t.transpose().flatten() for t in np.meshgrid(range(1,W,2),range(0,H,2))]\n",
    "        verical_idx  = [None,None]\n",
    "        horizon_idx  = [None,None]\n",
    "        center_idx   = [None,None]\n",
    "    else:\n",
    "        w_half  = W-W//2+(W%4)//2\n",
    "        h_half  = H-H//2+(H%4)//2\n",
    "        left_top_idx = [t.transpose() for t in np.meshgrid(list(range(0,W//2,2))+list(range(w_half,W,2)) ,\n",
    "                                                                     list(range(0,H//2,2))+list(range(h_half,W,2)))]\n",
    "        rigt_top_idx = left_top_idx[0]    ,left_top_idx[1]+1\n",
    "        rigt_bot_idx = left_top_idx[0] + 1,left_top_idx[1]+1\n",
    "        left_bot_idx = left_top_idx[0] + 1,left_top_idx[1]\n",
    "        \n",
    "        widx = np.array(list(range(0,W//2,2))+list(range(w_half,W,2)))\n",
    "        cent_top_idx = widx, np.array([h_half-1]*len(widx))\n",
    "        cent_bot_idx = cent_top_idx[0]+1, cent_top_idx[1]\n",
    "        hidx = np.array(list(range(0,H//2,2))+list(range(h_half,H,2)))\n",
    "        cent_lft_idx = np.array([w_half-1]*len(hidx)),hidx\n",
    "        cent_rgt_idx = cent_lft_idx[0],cent_lft_idx[1]+1\n",
    "        center_idx   = w_half,h_half\n",
    "    return ((left_top_idx[0].tolist(),left_top_idx[1].tolist()),\n",
    "            (rigt_top_idx[0].tolist(),rigt_top_idx[1].tolist()),\n",
    "            (rigt_bot_idx[0].tolist(),rigt_bot_idx[1].tolist()),\n",
    "            (left_bot_idx[0].tolist(),left_bot_idx[1].tolist()),\n",
    "            (cent_top_idx[0].tolist(),cent_top_idx[1].tolist()),\n",
    "            (cent_bot_idx[0].tolist(),cent_bot_idx[1].tolist()),\n",
    "            (cent_lft_idx[0].tolist(),cent_lft_idx[1].tolist()),\n",
    "            (cent_rgt_idx[0].tolist(),cent_rgt_idx[1].tolist()),\n",
    "             center_idx,\n",
    "           )\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "        cent_top_idx = rigt_top_idx[0], np.array(list(range(0,H//2,2))+list(range(h_half,H,2)))\n",
    "        cent_bot_idx = cent_top_idx[0], cent_top_idx[1]+1\n",
    "        cent_lft_idx = np.array(list(range(0,W//2,2))+list(range(w_half,W,2))),left_top_idx[1]\n",
    "        cent_rgt_idx = cent_lft_idx[0]+1,cent_lft_idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 5], [5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent_top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import opt_einsum as oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W=H=7\n",
    "D=5\n",
    "(left_top_idx,rigt_top_idx,rigt_bot_idx,left_bot_idx,\n",
    " cent_top_idx,cent_bot_idx,cent_lft_idx,cent_rgt_idx,center_idx)= get_corner_idx(W,H)\n",
    "#physics_tensor = torch.arange(W*H).reshape(W,H)\n",
    "physics_tensor = torch.randn(W,H,D,D,D,D)\n",
    "left_top = physics_tensor[left_top_idx[0],left_top_idx[1]]\n",
    "rigt_top = physics_tensor[rigt_top_idx[0],rigt_top_idx[1]]\n",
    "rigt_bot = physics_tensor[rigt_bot_idx[0],rigt_bot_idx[1]]\n",
    "left_bot = physics_tensor[left_bot_idx[0],left_bot_idx[1]]\n",
    "\n",
    "hat_lrlr_tensor= torch.randn(2,2,(W//2),(H//2),D,D,D)\n",
    "left_hat = hat_lrlr_tensor[0,0]\n",
    "topp_hat = hat_lrlr_tensor[0,1]\n",
    "rigt_hat = hat_lrlr_tensor[1,1]\n",
    "bott_hat = hat_lrlr_tensor[1,0]\n",
    "left_top__rigt_top__left_bot__rigt_bot = oe.contract(\"...abcd,...cefg,...hgij,...kdhl,...mak,...bne,...foi,...ljp->...mnop\",\n",
    "                        left_top,rigt_top,rigt_bot,left_bot,\n",
    "                        left_hat,topp_hat,rigt_hat,bott_hat#,optimize=path\n",
    "                                                    )\n",
    "cent_top = physics_tensor[cent_top_idx[0],cent_top_idx[1]]\n",
    "cent_bot = physics_tensor[cent_bot_idx[0],cent_bot_idx[1]]\n",
    "cent_lft = physics_tensor[cent_lft_idx[0],cent_lft_idx[1]]\n",
    "cent_rgt = physics_tensor[cent_rgt_idx[0],cent_rgt_idx[1]]\n",
    "\n",
    "cent_lft_hat,cent_rgt_hat = torch.randn(2,(H//2),D,D,D)\n",
    "cent_vertical_left_top_right_bot = oe.contract(\"...abcd,...edfg,...hae,...cif->...hbig\",\n",
    "                        cent_top,cent_bot,cent_lft_hat,cent_rgt_hat,\n",
    "                        )\n",
    "\n",
    "cent_top_hat,cent_bot_hat = torch.randn(2,(H//2),D,D,D)\n",
    "cent_horenzon_left_top_right_bot = oe.contract(\"...abcd,...cefg,...bhe,...dgi->...ahfi\",\n",
    "                        cent_lft,cent_rgt,cent_top_hat,cent_bot_hat,\n",
    "                        )\n",
    "center   = physics_tensor[center_idx[0],center_idx[1]]\n",
    "\n",
    "W,H= left_top__rigt_top__left_bot__rigt_bot.shape[:2]\n",
    "w_h= W//2 + W%2 \n",
    "h_h= H//2 + H%2 \n",
    "left_top = left_top__rigt_top__left_bot__rigt_bot[:w_h,:h_h]\n",
    "rigt_top = left_top__rigt_top__left_bot__rigt_bot[w_h:,:h_h]\n",
    "rigt_bot = left_top__rigt_top__left_bot__rigt_bot[w_h:,h_h:]\n",
    "left_bot = left_top__rigt_top__left_bot__rigt_bot[:w_h,h_h:]\n",
    "cent_top = cent_vertical_left_top_right_bot[:h_h].unsqueeze(0)\n",
    "cent_bot = cent_vertical_left_top_right_bot[h_h:].unsqueeze(0)\n",
    "cent_lft = cent_horenzon_left_top_right_bot[:w_h].unsqueeze(1)\n",
    "cent_rgt = cent_horenzon_left_top_right_bot[w_h:].unsqueeze(1)\n",
    "cent_cet = center.unsqueeze(0).unsqueeze(0)\n",
    "new_tensor = torch.cat([torch.cat([left_top,cent_top,rigt_top],dim=0),\n",
    "                        torch.cat([cent_lft,cent_cet,cent_rgt],dim=0),\n",
    "                        torch.cat([left_bot,cent_bot,rigt_bot],dim=0)],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=5;W=H=7\n",
    "physics_tensor = torch.arange(W*H).reshape(W,H)\n",
    "(left_top_idx,rigt_top_idx,rigt_bot_idx,left_bot_idx,\n",
    " cent_top_idx,cent_bot_idx,cent_lft_idx,cent_rgt_idx,center_idx)= get_corner_idx(W,H)\n",
    "left_top = physics_tensor[left_top_idx[0],left_top_idx[1]]\n",
    "rigt_top = physics_tensor[rigt_top_idx[0],rigt_top_idx[1]]\n",
    "rigt_bot = physics_tensor[rigt_bot_idx[0],rigt_bot_idx[1]]\n",
    "left_bot = physics_tensor[left_bot_idx[0],left_bot_idx[1]]\n",
    "cent_top = physics_tensor[cent_top_idx[0],cent_top_idx[1]]\n",
    "cent_bot = physics_tensor[cent_bot_idx[0],cent_bot_idx[1]]\n",
    "cent_lft = physics_tensor[cent_lft_idx[0],cent_lft_idx[1]]\n",
    "cent_rgt = physics_tensor[cent_rgt_idx[0],cent_rgt_idx[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12, 13],\n",
       "        [14, 15, 16, 17, 18, 19, 20],\n",
       "        [21, 22, 23, 24, 25, 26, 27],\n",
       "        [28, 29, 30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39, 40, 41],\n",
       "        [42, 43, 44, 45, 46, 47, 48]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physics_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 18, 39])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### arbitary partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.tensornetwork_base import TN_Base\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "import opt_einsum as oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from models.convND import convNd\n",
    "\n",
    "# import traceback\n",
    "\n",
    "# class scaled_Tanh(nn.Module):\n",
    "#     def __init__(self,coef):\n",
    "#         super().__init__()\n",
    "#         self.nonlinear=  nn.Tanh()\n",
    "#         self.coef = coef\n",
    "#     def forward(self,x):\n",
    "#         x = self.nonlinear(x)\n",
    "#         x = x/self.coef\n",
    "#         return x\n",
    "\n",
    "# def get_ConND(in_channels,out_channels,num_dims,bias=True,**kargs):\n",
    "#     if num_dims == 1:\n",
    "#         cnn = torch.nn.Conv1d(in_channels,out_channels,bias=bias,**kargs)\n",
    "#     elif num_dims == 2:\n",
    "#         cnn = torch.nn.Conv2d(in_channels,out_channels,bias=bias,**kargs)\n",
    "#     elif num_dims == 3:\n",
    "#         cnn = torch.nn.Conv3d(in_channels,out_channels,bias=bias,**kargs)\n",
    "#     else:\n",
    "#         cnn = convNd(in_channels=in_channels,out_channels=out_channels,num_dims=num_dims,use_bias=bias,\n",
    "#                       **kargs)\n",
    "#     return cnn\n",
    "\n",
    "# class TensorNetConvND(nn.Module):\n",
    "    \n",
    "#     def reset_parameters(self):\n",
    "#         def init_weights(m):\n",
    "#             if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "#                 torch.nn.init.xavier_uniform_(m.weight)\n",
    "#                 #torch.nn.init.kaiming_normal_(m.weight) for leack ReLU\n",
    "#                 if m.bias is not None:torch.nn.init.zeros_(m.bias)\n",
    "#         self.apply(init_weights)\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         squeeze = False\n",
    "#         if len(x.shape) ==self.num_dims + 1:      \n",
    "#             x = x.unsqueeze(1)\n",
    "#             squeeze=True\n",
    "#         res = x\n",
    "#         out = self.engine(x)\n",
    "#         out+= res\n",
    "#         x = self.resize_layer(out)\n",
    "#         if squeeze:\n",
    "#             x = x.squeeze(1)\n",
    "#         return x\n",
    "\n",
    "# class TensorNetConvND_Single(TensorNetConvND):\n",
    "#     def __init__(self,shape,offset,channels):\n",
    "#         super().__init__()\n",
    "#         num_dims      = len(shape) - offset\n",
    "#         self.num_dims = num_dims\n",
    "#         in_channels = out_channels = channels\n",
    "#         kargs = {'kernel_size':3,\n",
    "#                  'stride':1,\n",
    "#                  'padding':1,\n",
    "#                 }\n",
    "#         active_shape = shape[offset:]\n",
    "#         cnn1 = get_ConND(in_channels,out_channels,num_dims,bias=False,**kargs)\n",
    "#         bn1  = nn.LayerNorm(active_shape)\n",
    "#         self.engine = nn.Sequential(cnn1,bn1)\n",
    "        \n",
    "        \n",
    "#         factor = np.prod(active_shape)\n",
    "#         mi  = 1.0/len(active_shape)\n",
    "#         #coef   = np.sqrt(np.sqrt(factor))\n",
    "#         coef   = np.sqrt(np.power(factor,mi))\n",
    "#         self.resize_layer=scaled_Tanh(coef)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "# class TensorNetConvND_Block_a(TensorNetConvND):\n",
    "#     def __init__(self,shape,offset,channels,interchannels=4):\n",
    "#         super().__init__()\n",
    "#         num_dims      = len(shape) - offset\n",
    "#         self.num_dims = num_dims\n",
    "#         in_channels = out_channels = channels\n",
    "#         kargs = {'kernel_size':3,\n",
    "#                  'stride':1,\n",
    "#                  'padding':1,\n",
    "#                 }\n",
    "#         active_shape = shape[offset:]\n",
    "#         cnn1 = get_ConND(  in_channels,interchannels,num_dims,bias=False,**kargs)\n",
    "#         cnn2 = get_ConND(interchannels,out_channels ,num_dims,bias=False,**kargs)\n",
    "#         relu = nn.Tanh()\n",
    "#         bn1  = nn.LayerNorm(active_shape)\n",
    "#         bn2  = nn.LayerNorm(active_shape)\n",
    "#         self.engine = nn.Sequential(cnn1,bn1,relu,cnn2,bn2)\n",
    "        \n",
    "        \n",
    "#         factor = np.prod(active_shape)\n",
    "#         mi  = 1.0/len(active_shape)\n",
    "#         #coef   = np.sqrt(np.sqrt(factor))\n",
    "#         coef   = np.sqrt(np.power(factor,mi))\n",
    "#         self.resize_layer=scaled_Tanh(coef)\n",
    "#         self.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     2,
     51,
     73,
     78,
     85,
     221,
     251,
     372,
     384,
     385
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.two_dim_model import *\n",
    "from models.extend_model import *\n",
    "class PEPS_einsum_arbitrary_partition_optim(TN_Base):\n",
    "    def __init__(self,out_features=None,\n",
    "                      virtual_bond_dim=None,#\"models/arbitary_shape/arbitary_shape_16x9_2.json\",\n",
    "                      label_position=None,#=(8,4),\n",
    "                       fixed_virtual_dim=None,\n",
    "                       patch_engine=torch.nn.Identity,\n",
    "                       symmetry=None,#\"Z2_16x9\",\n",
    "                       seted_variation=10,\n",
    "                       init_std=1e-10,\n",
    "                       solved_std=None,\n",
    "                       convertPeq1=False,re_cal_path=False,complex_number_mode=False):\n",
    "        super().__init__()\n",
    "        assert out_features is not None\n",
    "        assert virtual_bond_dim is not None\n",
    "        assert label_position is not None\n",
    "        self.convertPeq1  = convertPeq1\n",
    "        self.out_features = out_features\n",
    "        if isinstance(virtual_bond_dim,str):\n",
    "            arbitary_shape_state_dict = torch.load(virtual_bond_dim)\n",
    "        else:\n",
    "            arbitary_shape_state_dict = virtual_bond_dim\n",
    "        assert isinstance(arbitary_shape_state_dict,dict)\n",
    "\n",
    "\n",
    "        info_per_group = arbitary_shape_state_dict['node']\n",
    "        info_per_line  = arbitary_shape_state_dict['line']\n",
    "        info_per_point = arbitary_shape_state_dict['element']\n",
    "\n",
    "\n",
    "\n",
    "        #symmetry_map = FFT_Z2_symmetry_16x9_point\n",
    "        symmetry_map,symmetry_map_point = self.generate_symmetry_map(symmetry,info_per_point)\n",
    "\n",
    "        # if we use the symmetry, we need to make sure not only the tensor shape but also the\n",
    "        # object that each tensor leg point. That is we need to realign the neighbor for each point.\n",
    "        for group_now in range(len(info_per_group)):\n",
    "            neighbors = info_per_group[group_now]['neighbor']# neighbors is also a group\n",
    "            unique_id = [min(symmetry_map[neighbor_id]) for neighbor_id in neighbors]\n",
    "            new_order = np.argsort(unique_id)\n",
    "            #print(f\"group-{group_now}: old neighbors{neighbors} real neighbor {unique_id} new order {new_order}\")\n",
    "            info_per_group[group_now]['neighbor'] = [neighbors[i] for i in new_order]\n",
    "            elements  =  info_per_group[group_now]['element']\n",
    "            unique_id = [min(symmetry_map_point[element]) for element in elements]\n",
    "            new_order = np.argsort(unique_id)\n",
    "            info_per_group[group_now]['element'] = [elements[i] for i in new_order]\n",
    "\n",
    "        # if the element in group is symmetric, for example, if (15,0) and (1,0) in the same group, then the regist weight A(#Element,D,D,D,D)\n",
    "        # should hold same symmetry along #Element dimension. That is, if we set (15,0) to the A[0](D,D,D,D) and (1,0) to the A[1](D,D,D,D),\n",
    "        # then should have A[0]==A[1]\n",
    "        for group_now,pool in info_per_group.items():\n",
    "            in_symmetry_idx     = {}\n",
    "            for idx, point in enumerate(pool['element']):\n",
    "                in_symmetry_idx[idx]= [idx]\n",
    "                for string_id in symmetry_map_point[point]:# the symmetry_map_point return string point position, for example '15-0'\n",
    "                    real_point_pos = tuple([int(s) for s in string_id.split('-')])\n",
    "                    if real_point_pos != point and real_point_pos in pool['element']: # exclude the itself, and its symmetry part in element list\n",
    "                        the_idx_for_symmetry_point_in_element=pool['element'].index(real_point_pos)\n",
    "                        in_symmetry_idx[idx].append(the_idx_for_symmetry_point_in_element)\n",
    "\n",
    "            the_symmetry_permutation=[]\n",
    "            has_symmetry_point = False\n",
    "            for idx, point in enumerate(pool['element']):\n",
    "                if len(in_symmetry_idx[idx])>1:has_symmetry_point=True\n",
    "                if len(in_symmetry_idx[idx])>2:\n",
    "                    raise NotImplezntedError(\"we now don't support the case that have more than 2 symmetric point in one group\")\n",
    "                the_symmetry_permutation.append(in_symmetry_idx[idx][-1])\n",
    "            if has_symmetry_point:\n",
    "                pool['in_group_symmetry_permutation'] = the_symmetry_permutation\n",
    "            else:\n",
    "                pool['in_group_symmetry_permutation'] = None\n",
    "\n",
    "        if fixed_virtual_dim is not None:\n",
    "            for key in info_per_line.keys():\n",
    "                info_per_line[key]['D'] = fixed_virtual_dim\n",
    "        #since we use symmetry than the center group could not in the symmetry part\n",
    "        center_group = None\n",
    "        if label_position != 'no_center':\n",
    "\n",
    "            center_group = info_per_point[label_position]['group']\n",
    "            damgling_num = len(info_per_group)\n",
    "            info_per_group[center_group]['neighbor'].insert(0,damgling_num)\n",
    "            info_per_line[(center_group,damgling_num)]={'D': out_features}\n",
    "            symmetry_map[damgling_num]=set([damgling_num])\n",
    "        else:\n",
    "            assert out_features == 1\n",
    "        operands = []\n",
    "        sublist_list=[]\n",
    "\n",
    "        ranks_list=[]\n",
    "\n",
    "        #absolute_line_id = {}\n",
    "        for group_now in range(len(info_per_group)):\n",
    "            group_info= info_per_group[group_now]\n",
    "            neighbors = group_info['neighbor']\n",
    "            ranks = []\n",
    "            sublist=[]\n",
    "\n",
    "            for neighbor_id in neighbors:\n",
    "                line_tuple = [group_now,neighbor_id]\n",
    "                line_tuple.sort()\n",
    "                line_tuple= tuple(line_tuple)\n",
    "                D = int(info_per_line[line_tuple]['D'])\n",
    "                #if line_tuple not in absolute_line_id:absolute_line_id[line_tuple] = len(absolute_line_id)\n",
    "                #idx = absolute_line_id[line_tuple]\n",
    "                idx = list(info_per_line.keys()).index(line_tuple)\n",
    "\n",
    "                ranks.append(D)\n",
    "                sublist.append(idx)\n",
    "            tensor = np.random.randn(*ranks)\n",
    "            operands+=[tensor,[*sublist]]\n",
    "\n",
    "            ranks_list.append(ranks)\n",
    "            sublist_list.append(sublist)\n",
    "        if center_group is not None:\n",
    "            outlist  = [list(info_per_line.keys()).index((center_group,damgling_num))]\n",
    "        else:\n",
    "            outlist  = []\n",
    "        # line_tuple = (center_group,damgling_num)\n",
    "        # #if line_tuple not in absolute_line_id:absolute_line_id[line_tuple] = len(absolute_line_id)\n",
    "        # idx = absolute_line_id[line_tuple]\n",
    "        # outlist = [idx]\n",
    "        operands+= [[*outlist]]\n",
    "\n",
    "\n",
    "        self.operands_string = full_size_array_string(*operands)\n",
    "        #path,info = oe.contract_path(*operands,optimize='random-greedy-128')\n",
    "        path = get_best_path_via_oe(*operands,store=\"models/arbitrary_shape_path_recorder.json\",re_saveQ=re_cal_path)\n",
    "        self.path         = path\n",
    "        self.sublist_list = sublist_list\n",
    "        self.outlist      = outlist\n",
    "\n",
    "\n",
    "        unique_unit_list = []\n",
    "        # the unique unit list is the real weight for training of the model\n",
    "        # if there is no symmetry assign, then each group/block would allocate a weight\n",
    "        # then the len(unique_unit_list) == len(info_per_group)\n",
    "        # if there is symmetry, then some groups share the weight\n",
    "        # then the len(unique_unit_list) < len(info_per_group)\n",
    "        for i in range(len(sublist_list)):\n",
    "\n",
    "            for the_symmetry_counterpart in symmetry_map[i]:\n",
    "                if 'weight_idx' in info_per_group[the_symmetry_counterpart]:\n",
    "                    info_per_group[i]['weight_idx']=info_per_group[the_symmetry_counterpart][\"weight_idx\"]\n",
    "                    info_per_group[i]['symmetry_indices']=info_per_group[the_symmetry_counterpart][\"symmetry_indices\"]\n",
    "                    break\n",
    "            if 'weight_idx' in info_per_group[i]:\n",
    "                continue\n",
    "            # assume all element for the tensornetwork is indenpendent.\n",
    "            # The bond (include physics) list is l0,l1,l2,...,ln\n",
    "            # All element follow normal distribution X - sigma(0,alpha)\n",
    "            # where alpha is the variation we need to calculated.\n",
    "            # the output after contracting is also a tensor (may 1-rank scalar, 2-rank matrix, etc)\n",
    "            # the element of the output follow the composite normal distribution Y - sigma(0,beta)\n",
    "            # where beta = l0 x l1 x l2 x ... x ln x alpha^(# of tensor)\n",
    "\n",
    "            # if we active the Z2 symmetry, not only the left and right part should be reflected, but also\n",
    "            # the center part. Since the center part link two same region, then it should be the symmetry\n",
    "            # matrix for the bond it connected.\n",
    "            # we would firstly check the neighbor of the group\n",
    "            unique_neighbor_group = [min(symmetry_map[neighbor_id]) for neighbor_id in info_per_group[i]['neighbor']]\n",
    "            symmetry_indices=None\n",
    "            for j in set(unique_neighbor_group):\n",
    "                if unique_neighbor_group.count(j)>1:\n",
    "                    symmetry_leg_pos = list(np.where(np.array(unique_neighbor_group)==j)[0]+1)# plus 1 since we would add physics dim\n",
    "                    if symmetry_indices is not None:\n",
    "                        raise NotImplementedError(\"we now only support one symmetry indices\")\n",
    "                    symmetry_indices = list(permutations((symmetry_leg_pos)))\n",
    "            info_per_group[i]['symmetry_indices'] = symmetry_indices\n",
    "            info_per_group[i]['weight_idx']       = len(unique_unit_list)\n",
    "\n",
    "\n",
    "            shape = ranks_list[i]\n",
    "            P     = len(info_per_group[i]['element'])\n",
    "            if self.convertPeq1:\n",
    "                if self.convertPeq1 == 'all_convert':\n",
    "                    P = 2*P\n",
    "                else:\n",
    "                    P     = 2 if P==1 else P\n",
    "\n",
    "            #control_mat = self.rde2D((P,*shape),0,physics_index=0,offset= 2 if i==center_group else 1)\n",
    "            #bias_mat    = torch.normal(0,solved_std,(P,*shape))\n",
    "            #bias_mat[control_mat.nonzero(as_tuple=True)]=0\n",
    "            #unique_unit_list.append(control_mat+bias_mat)\n",
    "            if complex_number_mode:shape.append(2)\n",
    "            unique_unit_list.append(self.rde2D((P,*shape),init_std,offset=1))\n",
    "            #unique_unit_list.append(torch.normal(mean=0,std=solved_std,size=(P,*shape)))\n",
    "        #assert len(unit_list)==len(sublist_list)\n",
    "        \n",
    "        if complex_number_mode:\n",
    "            self.unique_unit_list         = [nn.Parameter(torch.view_as_complex(v)) for v in unique_unit_list]\n",
    "        else: \n",
    "            self.unique_unit_list         = [nn.Parameter(v) for v in unique_unit_list]\n",
    "        self.unique_groupwise_backend = nn.ModuleList()\n",
    "        if center_group is not None:\n",
    "            center_group_unique_idx = info_per_group[center_group][\"weight_idx\"]\n",
    "        else:\n",
    "            center_group_unique_idx = \"-1\"\n",
    "        for i,v in enumerate(unique_unit_list):\n",
    "            if i == center_group_unique_idx:\n",
    "                offset = 2\n",
    "                inchan = out_features\n",
    "            else:\n",
    "                offset = 1\n",
    "                inchan = 1\n",
    "            if complex_number_mode:\n",
    "                self.unique_groupwise_backend.append(patch_engine(v.shape[offset:-1],inchan))\n",
    "            else:\n",
    "                self.unique_groupwise_backend.append(patch_engine(v.shape[offset:],inchan))\n",
    "        for i, v in enumerate(self.unique_unit_list):self.register_parameter(f'unit_{i}', param=v)\n",
    "        #put the tensor into the AD graph.\n",
    "        #assert len(self.unit_list)==len(sublist_list)\n",
    "        self.info_per_group=info_per_group\n",
    "        self.info_per_line =info_per_line\n",
    "        self.info_per_point=info_per_point\n",
    "        self.symmetry_map  =symmetry_map\n",
    "        self.symmetry_map_point =symmetry_map_point\n",
    "        self.pre_activate_layer = nn.Identity()\n",
    "        self.debugQ=False\n",
    "        self.complex_number_mode = complex_number_mode\n",
    "    def generate_symmetry_map(self,symmetry,info_per_point):\n",
    "        symmetry_map       = {}\n",
    "        symmetry_map_point = {}\n",
    "        for i,j in info_per_point.keys():\n",
    "            group_now  = info_per_point[i,j]['group']\n",
    "            symmetry_map[group_now]= set([group_now])\n",
    "            symmetry_map_point[i,j]= set([f\"{i}-{j}\"])\n",
    "        if symmetry is not None:\n",
    "            if symmetry == \"Z2_16x9\":\n",
    "                for i in list(range(1,8))+list(range(9,16)):\n",
    "                    for j in range(9):\n",
    "                        group_now  = info_per_point[i,j]['group']\n",
    "                        for x, y in [[16-i,j]]:\n",
    "                            group_sym  = info_per_point[x,y]['group']\n",
    "                            symmetry_map[group_now]=symmetry_map[group_now]|set([group_sym])\n",
    "                            symmetry_map_point[i,j]=symmetry_map_point[i,j]|set([f\"{x}-{y}\"])\n",
    "            elif symmetry == \"Z2_16x16\":\n",
    "                for i in range(16):\n",
    "                    for j in range(16):\n",
    "                        group_now  = info_per_point[i,j]['group']\n",
    "\n",
    "                        for x, y in [[i,15-j],[15-i,j],[15-i,15-j]]:\n",
    "                            group_sym  = info_per_point[x,y]['group']\n",
    "                            symmetry_map[group_now]=symmetry_map[group_now]|set([group_sym])\n",
    "                            symmetry_map_point[i,j]=symmetry_map_point[i,j]|set([f\"{x}-{y}\"])\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        return symmetry_map,symmetry_map_point\n",
    "\n",
    "    def weight_init(self,method=None,set_var=1):\n",
    "        if method is None:return\n",
    "        if method == \"Expecatation_Normalization\":\n",
    "            num_of_tensor   = len(self.info_per_group)\n",
    "            list_of_virt_dim= [t['D'] for t in self.info_per_line.values()]\n",
    "            # notice, we cannot count the class label\n",
    "            list_of_virt_dim.append(1/self.out_features)\n",
    "            list_of_phys_dim= [len(t['element']) for t in self.info_per_group.values()]\n",
    "            divider         = np.prod([np.power(t,1/num_of_tensor) for t in list_of_phys_dim+list_of_virt_dim])\n",
    "            solved_var      = np.power(set_var,1/num_of_tensor)/divider\n",
    "            solved_std      = np.sqrt(solved_var)\n",
    "            for i in range(len(self.unique_unit_list)):\n",
    "                self.unique_unit_list[i] = torch.nn.init.normal_(self.unique_unit_list[i],mean=0.0, std=solved_std)\n",
    "        elif method == \"Expecatation_Normalization2\":\n",
    "            num_of_tensor   = len(self.info_per_group)\n",
    "            list_of_virt_dim= [t['D'] for t in self.info_per_line.values()]\n",
    "            # notice, we cannot count the class label\n",
    "            list_of_virt_dim.append(1/self.out_features)\n",
    "            list_of_phys_dim= [len(t['element']) for t in self.info_per_group.values()]\n",
    "            divider         = np.prod([np.power(t,1/num_of_tensor) for t in list_of_virt_dim])\n",
    "            solved_var      = np.power(set_var,1/num_of_tensor)/divider\n",
    "            solved_std      = np.sqrt(solved_var)\n",
    "            for i in range(len(self.unique_unit_list)):\n",
    "                self.unique_unit_list[i] = torch.nn.init.normal_(self.unique_unit_list[i],mean=0.0, std=solved_std)\n",
    "        elif method == \"fixpoint_start\":\n",
    "            for i in range(len(self.unique_unit_list)):\n",
    "                shape = self.unique_unit_list[i].shape\n",
    "                offset= 1\n",
    "                size_shape = shape[:offset]\n",
    "                bias_shape = shape[offset:]\n",
    "                max_dim    = max(bias_shape)\n",
    "                full_rank  = len(bias_shape)\n",
    "                half_rank  = full_rank//2\n",
    "                rest_rank  = full_rank-half_rank\n",
    "                bias_mat   = torch.eye(max_dim**half_rank,max_dim**rest_rank)\n",
    "                bias_mat   = bias_mat.reshape(*([max_dim]*full_rank))\n",
    "                for j,d in enumerate(bias_shape):\n",
    "                    bias_mat   = torch.index_select(bias_mat, j,torch.arange(d))\n",
    "                norm   = np.sqrt(np.prod(bias_shape))\n",
    "                norm  *= np.sqrt(size_shape[0])\n",
    "                #print(norm)\n",
    "\n",
    "                bias_mat  /= norm\n",
    "                bias_mat   = bias_mat.repeat(*size_shape,*([1]*len(bias_shape)))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.unique_unit_list[i] = torch.nn.init.normal_(self.unique_unit_list[i],mean=0.0, std=np.sqrt(set_var))\n",
    "                    self.unique_unit_list[i].add_(bias_mat)\n",
    "        elif method == \"normlization_to_one\":\n",
    "            for i in range(len(self.unique_unit_list)):\n",
    "                with torch.no_grad():\n",
    "                    self.unique_unit_list[i].div_(torch.norm(self.unique_unit_list[i]))\n",
    "        else:\n",
    "            raise NotImplementedError(f\"we dont have init option:{method}\")\n",
    "\n",
    "    def get_batch_input(self,input_data):\n",
    "        #input data shape B,1,W,H\n",
    "        assert len(input_data.shape)==4\n",
    "        assert np.prod(input_data.shape[-2:])==len(self.info_per_point)\n",
    "        input_data  = input_data.squeeze(1)\n",
    "\n",
    "        _input = []\n",
    "        _units = []\n",
    "        #for i,((unit,symmetry_indices),point_idx) in enumerate(zip(unit_list,self.point_of_group)):\n",
    "        for i in range(len(self.info_per_group)):\n",
    "            #patch_idx  = self.info_per_group[i]['element_idx']\n",
    "            #print(f\"processing unit {i}: conclude point{patch_idx}\")\n",
    "            #batch_input= input_data[...,patch_idx] # B,P\n",
    "            pool = self.info_per_group[i]\n",
    "            point_idx = np.array(pool['element']).transpose()\n",
    "            #print(info_per_group[i][\"weight_idx\"])\n",
    "            unit             = self.unique_unit_list[pool[\"weight_idx\"]]\n",
    "            if pool['in_group_symmetry_permutation'] is not None:\n",
    "                unit = (unit + unit[pool['in_group_symmetry_permutation']])/2\n",
    "            unit_engine      = self.unique_groupwise_backend[pool[\"weight_idx\"]]\n",
    "            symmetry_indices = pool['symmetry_indices']\n",
    "\n",
    "            x,y = point_idx\n",
    "            batch_input= input_data[...,x,y] # B,P\n",
    "            if self.convertPeq1:\n",
    "                if self.convertPeq1 == 'all_convert':\n",
    "                    batch_input = torch.cat([batch_input,1-batch_input],-1)\n",
    "                elif batch_input.shape[-1]==1: #self.convertPeq1 == 'only convert for P=1':\n",
    "                    batch_input = torch.cat([batch_input,1-batch_input],-1)\n",
    "                else:\n",
    "                    pass\n",
    "            # the correct way to follow the sprit is symmetry the weight\n",
    "            # however, if there is no further processing, it is same to so contractrion than do symmetry.\n",
    "            # what's more, if we use further processing, it much more convience to do symmetriy later rather than\n",
    "            # create a symmetric further processing for symmetric weight.\n",
    "            if symmetry_indices is not None:\n",
    "                unit_sys = unit\n",
    "                for symmetry_indice in symmetry_indices[1:]:\n",
    "                    unit_sys = unit_sys + unit.transpose(*symmetry_indice)\n",
    "                unit = unit_sys/len(symmetry_indices)\n",
    "            #_units.append(unit)\n",
    "            #_input.append(batch_input)\n",
    "\n",
    "            batch_unit = torch.tensordot(batch_input,unit,dims=([-1], [0]))\n",
    "            if self.complex_number_mode:\n",
    "                batch_unit = batch_unit.abs()\n",
    "            batch_unit = self.pre_activate_layer(batch_unit)\n",
    "            #print(f\"{batch_unit.shape}->\",end=' ')\n",
    "            #print(unit_engine)\n",
    "            batch_unit = unit_engine(batch_unit)\n",
    "\n",
    "            #print(f\"{batch_unit.shape}\",end='\\n')\n",
    "            ##print(f\"{batch_input.norm()}-{unit.norm()}->{batch_unit.norm()}\")\n",
    "            ##print(batch_unit.shape)\n",
    "\n",
    "            if symmetry_indices is not None:\n",
    "                unit_sys = batch_unit\n",
    "                for symmetry_indice in symmetry_indices[1:]:\n",
    "                    unit_sys = unit_sys + batch_unit.transpose(*symmetry_indice)\n",
    "                batch_unit = unit_sys/len(symmetry_indices)\n",
    "            if self.debugQ:\n",
    "                std,mean = torch.std_mean(batch_unit)\n",
    "                print(f'patch_{i}: std:{std.item()} mean:{mean.item()}')\n",
    "            _input.append(batch_unit)\n",
    "        return _input\n",
    "\n",
    "    def forward(self,input_data, only_return_input=False):\n",
    "        _input = self.get_batch_input(input_data)\n",
    "        if only_return_input:\n",
    "            return _input\n",
    "        #return _input,_units\n",
    "        operands=[]\n",
    "        for tensor,sublist in zip(_input,self.sublist_list):\n",
    "            operand = [tensor,[...,*sublist]]\n",
    "            operands+=operand\n",
    "        operands+= [[...,*self.outlist]]\n",
    "        return self.einsum_engine(*operands,optimize=self.path)\n",
    "\n",
    "    def set_alpha(self,alpha):\n",
    "        for backend in self.unique_groupwise_backend:\n",
    "            #if hasattr(backend,'set_alpha'):\n",
    "            backend.set_alpha(alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 9])\n",
      "torch.Size([1, 1, 16, 9])\n",
      "tensor(49.2518, grad_fn=<CopyBackwards>)\n",
      "tensor(49.2518, grad_fn=<CopyBackwards>)\n",
      "tensor(7.6294e-06, grad_fn=<DistBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "                                            virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x9_2.json\",\n",
    "                                            label_position=(8,4),\n",
    "                                            symmetry=\"Z2_16x9\",complex_number_mode=True,\n",
    "                                            patch_engine=lambda *args:TensorNetConvND_Single(*args,alpha=200),\n",
    "                                            #fixed_virtual_dim=4\n",
    "                                           )\n",
    "model.weight_init(method=\"Expecatation_Normalization2\")\n",
    "a = torch.view_as_complex(torch.randn(1,1,16,9,2));print(a.shape)\n",
    "b = torch.zeros_like(a);print(b.shape)\n",
    "b[...,0,:]=a[...,0,:]\n",
    "b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "print(model(a).norm())\n",
    "print(model(b).norm())\n",
    "print(torch.dist(model(a),model(b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0019, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "                                            virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_1.json\",\n",
    "                                            label_position=(0,0),\n",
    "                                            #patch_engine=TensorNetConvND,\n",
    "                                            #fixed_virtual_dim=4\n",
    "                                           )\n",
    "model.weight_init(method=\"Expecatation_Normalization\")\n",
    "\n",
    "a = torch.randn(1,1,24,24)\n",
    "print(model(a).norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### complex number arbitary partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(32,2)\n",
    "a = torch.view_as_complex(a)\n",
    "b = torch.randn(32,2)\n",
    "b = Variable(torch.view_as_complex(b),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss = (a*b).abs().norm()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2354e-01+0.5003j, -1.1129e-04-0.0050j, -7.3215e-03-0.0206j,\n",
       "        -3.3028e-01-0.0266j, -1.6051e-02-0.1090j, -1.7346e-01-0.2475j,\n",
       "        -1.2109e-01+0.5542j, -1.2553e-01-0.5166j, -1.1763e-01+0.1733j,\n",
       "         1.8939e-02-0.0278j, -1.3130e-01-0.0777j, -2.4642e-03+0.0550j,\n",
       "        -3.2392e-02+0.2109j,  7.8137e-02+0.1199j, -5.9842e-03+0.0343j,\n",
       "        -6.2472e-03-0.0373j,  1.7025e-01-0.1420j, -1.0697e-01+0.3942j,\n",
       "        -1.7244e-01-0.0720j, -7.8269e-02+0.0634j, -2.7117e-02-0.4359j,\n",
       "         1.0190e-01-0.0621j,  1.1339e+00+0.7339j,  7.9907e-02+0.5528j,\n",
       "         1.6059e-03+0.0025j, -3.5469e-02-0.0204j, -1.0248e-01+0.1329j,\n",
       "         9.0897e-02+0.2352j, -1.2544e-01-0.2940j, -9.9931e-02+0.0218j,\n",
       "         1.6862e-01-0.1492j, -6.5128e-02-0.2442j])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Deep model (with nonlienar layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.tensornetwork_base import TN_Base\n",
    "import torch.nn as nn\n",
    "from models.model_utils import *\n",
    "from models.two_dim_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### the random set for tensornetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#         num_of_edge     = (W-1)*H+(H-1)*W\n",
    "#         num_of_unit     = W*H\n",
    "#         solved_var      = np.power(set_var,1/num_of_unit)*np.power(1/virtual_bond_dim,num_of_edge/(num_of_unit))/in_physics_bond\n",
    "#         solved_std      = np.sqrt(solved_var)\n",
    "#         print(solved_std)\n",
    "#         random_engine   = lambda shape:torch.normal(0,solved_std,shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "W=H=10\n",
    "LW = W//2\n",
    "LH = H//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index_matrix = torch.LongTensor([[[i,j] for j in range(W)] for i in range(H)])\n",
    "bulk_index,edge_index,corn_index=PEPS_uniform_shape_symmetry_base.flatten_image_input(index_matrix)\n",
    "\n",
    "flag_matrix = torch.zeros(W,H).long()\n",
    "position_matrix = torch.zeros(W,H).long()\n",
    "\n",
    "for n,(i,j) in enumerate(corn_index.numpy()):\n",
    "    flag_matrix[i,j]=0\n",
    "    position_matrix[i,j]=n\n",
    "for n,(i,j) in enumerate(edge_index.numpy()):\n",
    "    flag_matrix[i,j]=1\n",
    "    position_matrix[i,j]=n\n",
    "for n,(i,j) in enumerate(bulk_index.numpy()):\n",
    "    flag_matrix[i,j]=2\n",
    "    position_matrix[i,j]=n\n",
    "\n",
    "flag_matrix     = flag_matrix\n",
    "position_matrix = position_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "O=2;D=3;B=5\n",
    "corn_tensors = torch.randn(4,B,O,D,D)\n",
    "edge_tensors = torch.randn(2*(W-2)+2*(H-2),B,D,D,D)\n",
    "bulk_tensors = torch.randn((W-2)*(H-2),B,D,D,D,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "point_x = [0,0,1,1]\n",
    "point_y = [0,1,1,0]\n",
    "p       = part_idex[point_x,point_y]#(L,4,2)\n",
    "indexrule=position_matrix[p[...,0],p[...,1]]\n",
    "partrule =flag_matrix[p[...,0],p[...,1]][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "point_x = [j for j in range(L)]\n",
    "point_y = [L for j in range(L)]\n",
    "p       = part_idex[point_x,point_y]#(L,4,2)\n",
    "indexrule=position_matrix[p[...,0],p[...,1]]\n",
    "partrule =flag_matrix[p[...,0],p[...,1]][:,0]\n",
    "should = PEPS_uniform_shape_symmetry_base.pick_tensors(partrule,indexrule,corn_tensors,edge_tensors,bulk_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PEPS_uniform_shape_symmetry_deep_model(PEPS_uniform_shape_symmetry_base):\n",
    "    def __init__(self, nonlinear_layer=nn.Tanh(),\n",
    "                       normlized_layer_module=nn.InstanceNorm3d,\n",
    "                       init_std=1e-10,set_var=1,**kargs):\n",
    "        super().__init__(**kargs)\n",
    "        H = self.H\n",
    "        W = self.W\n",
    "        LW= self.LW\n",
    "        LH= self.LH\n",
    "        D = self.D\n",
    "        O = self.O\n",
    "        P = self.P\n",
    "        num_of_edge     = (W-1)*H+(H-1)*W\n",
    "        num_of_unit     = W*H\n",
    "        solved_var      = np.power(set_var,1/num_of_unit)*np.power(1/D,num_of_edge/(num_of_unit))/P\n",
    "        solved_std      = np.sqrt(solved_var)\n",
    "        print(solved_std)\n",
    "        random_engine   = lambda shape:torch.normal(0,solved_std,shape)\n",
    "        self.corn_tensors = torch.nn.init.normal_(self.corn_tensors,mean=0.0, std=solved_std)\n",
    "        self.edge_tensors = torch.nn.init.normal_(self.edge_tensors,mean=0.0, std=solved_std)\n",
    "        self.bulk_tensors = torch.nn.init.normal_(self.bulk_tensors,mean=0.0, std=solved_std)\n",
    "        \n",
    "        flag_matrix     = self.flag_matrix\n",
    "        position_matrix = self.position_matrix\n",
    "        index_matrix    = self.index_matrix\n",
    "        part_lu_idex = torch.rot90(index_matrix,k=0)[:LW,:LH]\n",
    "        part_ru_idex = torch.rot90(index_matrix,k=1)[:LW,:LH]\n",
    "        part_rd_idex = torch.rot90(index_matrix,k=2)[:LW,:LH]\n",
    "        part_ld_idex = torch.rot90(index_matrix,k=3)[:LW,:LH]\n",
    "        part_idex = torch.stack([part_lu_idex,\n",
    "                                 part_ru_idex,\n",
    "                                 part_rd_idex,\n",
    "                                 part_ld_idex],-2)\n",
    "        \n",
    "        indexrules = []\n",
    "        partrules  = []\n",
    "        point_x = [0,0,1,1]\n",
    "        point_y = [0,1,1,0]\n",
    "        p       = part_idex[point_x,point_y]#(L,4,2)\n",
    "        indexrule=position_matrix[p[...,0],p[...,1]]\n",
    "        partrule =flag_matrix[p[...,0],p[...,1]][:,0]\n",
    "\n",
    "        indexrules.append(indexrule)\n",
    "        partrules.append(partrule)\n",
    "        edge_contraction_path=[]\n",
    "        for L in range(2,LW):\n",
    "            indexrule={}\n",
    "            partrule={}\n",
    "            tn2D_shape_list = [[(D,D,D)]+[(D,D,D,D)]*(L-1)]\n",
    "            path,sublist_list,outlist = get_best_path(tn2D_shape_list,store=path_recorder,type='sub')\n",
    "            edge_contraction_path.append([path,sublist_list,outlist])\n",
    "            point_x = [[j for j in range(L)],[L for j in range(L)]]\n",
    "            point_y = [[L for j in range(L)],[j for j in range(L)]]\n",
    "            p       = part_idex[point_x,point_y]#(2,L,4,2)\n",
    "            indexrule['edge']= position_matrix[p[...,0],p[...,1]].transpose(0,1)\n",
    "            partrule['edge'] = flag_matrix[p[...,0],p[...,1]][0][:,0]\n",
    "            \n",
    "            point_x = [L]\n",
    "            point_y = [L]\n",
    "            p       = part_idex[point_x,point_y]#(L,4,2)\n",
    "            \n",
    "            indexrule['cent']= position_matrix[p[...,0],p[...,1]]\n",
    "            partrule['cent'] = flag_matrix[p[...,0],p[...,1]][:,0]\n",
    "            \n",
    "            indexrules.append(indexrule)\n",
    "            partrules.append(partrule)\n",
    "        self.indexrules = indexrules\n",
    "        self.partrules  = partrules\n",
    "        self.edge_contraction_path = edge_contraction_path\n",
    "        self.nonlinear_layer = nonlinear_layer\n",
    "        self.normlized_layers = nn.ModuleList([normlized_layer_module(O,affine=True) for _ in self.partrules])\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        LH = self.LH\n",
    "        LW = self.LW\n",
    "        D  = self.D\n",
    "        bulk_tensors,edge_tensors,corn_tensors = self.get_batch_contraction_network(input_data)\n",
    "        corn_tensors = self.pick_tensors(self.partrules[0],self.indexrules[0],corn_tensors,edge_tensors,bulk_tensors)\n",
    "        corn = self.einsum_engine(\"lkoab,lkcdb,lkefcg,lkgah->lkohedf\",*corn_tensors).flatten(-4,-3).flatten(-2,-1)\n",
    "        corn = self.nonlinear_layer(corn)# (4,B,O,D,D)\n",
    "        corn = self.normlized_layers[0](corn.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "        #corn = corn/D**2\n",
    "        for i,(partrule, indexrule) in enumerate(zip(self.partrules[1:],self.indexrules[1:])):\n",
    "            path,sublist_list,outlist = self.edge_contraction_path[i]\n",
    "            edge_tensors= self.pick_tensors(partrule['edge'],indexrule['edge'],corn_tensors,edge_tensors,bulk_tensors)\n",
    "            cent_tensor = self.pick_tensors(partrule['cent'],indexrule['cent'],corn_tensors,edge_tensors,bulk_tensors)[0]\n",
    "            L           = len(edge_tensors)\n",
    "            operands    = structure_operands(edge_tensors,sublist_list,outlist)\n",
    "            edge1,edge2 = self.einsum_engine(*operands,optimize=path).flatten(-L-L,-L-1).flatten(-L,-1)\n",
    "            corn = self.einsum_engine(\"lkoab,lkcdb,lkefcg,lkgah->lkohedf\",corn ,edge1,cent_tensor,edge2).flatten(-4,-3).flatten(-2,-1)\n",
    "            corn = self.nonlinear_layer(corn)\n",
    "            corn = self.normlized_layers[i+1](corn.permute(1,2,0,3,4)).permute(2,0,1,3,4)\n",
    "            #corn = corn/D**(L+1)\n",
    "        # corn now is a tensor (B,4,D^(L/2),D^(L/2))\n",
    "        corn   = corn/D**(LW/3)\n",
    "        corn   = self.einsum_engine(\"kvab,kxbc,kycd,kzda->kvxyz\",*corn).flatten(-4,-1)# -> (B,O^4)\n",
    "        \n",
    "        return corn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06538302430059152\n"
     ]
    }
   ],
   "source": [
    "model = PEPS_uniform_shape_symmetry_deep_model(W=6,H=6,virtual_bond_dim=5,in_physics_bond=16,\n",
    "                                               nonlinear_layer=nn.Identity(),\n",
    "                                               normlized_layer_module=nn.Identity)\n",
    "#model.weight_init(method=\"Expecatation_Normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.9999), tensor(-0.0018))\n",
      "(tensor(0.0016), tensor(-3.4246e-05))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x=torch.normal(mean=0,std=1,size=(1000,6,6,16));print(torch.std_mean(x))\n",
    "    x=model(x);print(torch.std_mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Symmetry Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  2  3  4  5  6  0]\n",
      " [ 6  0  1  2  3  4  5  0  0]\n",
      " [ 5  5  6  7  8  9  6  1  1]\n",
      " [ 4  4  9 10 11 10  7  2  2]\n",
      " [ 3  3  8 11 12 11  8  3  3]\n",
      " [ 2  2  7 10 11 10  9  4  4]\n",
      " [ 1  1  6  9  8  7  6  5  5]\n",
      " [ 0  0  5  4  3  2  1  0  6]\n",
      " [ 0  6  5  4  3  2  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(position_matrix.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 2, 3, 4, 5, 0],\n",
       "        [5, 0, 1, 2, 3, 4, 0, 0],\n",
       "        [4, 4, 5, 6, 7, 5, 1, 1],\n",
       "        [3, 3, 7, 8, 8, 6, 2, 2],\n",
       "        [2, 2, 6, 8, 8, 7, 3, 3],\n",
       "        [1, 1, 5, 7, 6, 5, 4, 4],\n",
       "        [0, 0, 4, 3, 2, 1, 0, 5],\n",
       "        [0, 5, 4, 3, 2, 1, 0, 0]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=1;b=1;c=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "code_folding": [
     0,
     9
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def flatten_image_input(batch_image_input):\n",
    "    if len(batch_image_input.shape)==5:batch_image_input=batch_image_input.squeeze(1)\n",
    "    bulk_input = batch_image_input[...,1:-1,1:-1,:].flatten(-3,-2)\n",
    "    edge_input = torch.cat([batch_image_input[...,0,1:-1,:],\n",
    "                            batch_image_input[...,1:-1,[0,-1],:].flatten(-3,-2),\n",
    "                            batch_image_input[...,-1,1:-1,:]\n",
    "                           ],-2)\n",
    "    corn_input = batch_image_input[...,[0,0,-1,-1],[0,-1,0,-1],:]\n",
    "    return bulk_input,edge_input,corn_input\n",
    "def generate_symmetry_map(W,H,symmetry=None):\n",
    "    if symmetry is None:return {}\n",
    "    LW =  int(np.ceil(1.0*W/2))\n",
    "    LH = int(np.floor(1.0*H/2))\n",
    "    index_matrix = torch.LongTensor([[[i,j] for j in range(W)] for i in range(H)])\n",
    "    if symmetry == 'R4':\n",
    "        part_lu_idex = torch.rot90(index_matrix,k=0)[:LW,:LH].flatten(0,1).numpy()\n",
    "        part_ru_idex = torch.rot90(index_matrix,k=1)[:LW,:LH].flatten(0,1).numpy()\n",
    "        part_rd_idex = torch.rot90(index_matrix,k=2)[:LW,:LH].flatten(0,1).numpy()\n",
    "        part_ld_idex = torch.rot90(index_matrix,k=3)[:LW,:LH].flatten(0,1).numpy()\n",
    "        symmetry_map = {}\n",
    "        for n,(a,b,c) in enumerate(zip(part_ru_idex,part_rd_idex,part_ld_idex)):\n",
    "            i = part_lu_idex[n][0]\n",
    "            j = part_lu_idex[n][1]\n",
    "            symmetry_map[a[0],a[1]] = (i,j)\n",
    "            symmetry_map[b[0],b[1]] = (i,j)\n",
    "            symmetry_map[c[0],c[1]] = (i,j)\n",
    "        return symmetry_map\n",
    "    elif symmetry == 'R4Z2':\n",
    "        symmetry_map = generate_symmetry_map(W,H,symmetry='R4')\n",
    "        part_lu_idex = torch.rot90(index_matrix,k=0)[:LW,:LH].flatten(0,1).numpy()\n",
    "        for i,j in part_lu_idex:\n",
    "            if (j,i) not in symmetry_map and j>i:symmetry_map[j,i] =(i,j)\n",
    "        \n",
    "        return symmetry_map\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==b==c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index_matrix = torch.LongTensor([[[i,j] for j in range(W)] for i in range(H)])\n",
    "bulk_index,edge_index,corn_index=flatten_image_input(index_matrix)\n",
    "flag_matrix     = torch.zeros(W,H).long()\n",
    "position_matrix = torch.zeros(W,H).long()\n",
    "symmetry_map    = generate_symmetry_map(W,H,symmetry='R4')\n",
    "tensor_needed   = []\n",
    "active_index    = []\n",
    "for flag, types in enumerate([corn_index,edge_index,bulk_index]):\n",
    "    res= 0\n",
    "    tensor_order_for_this_type=[]\n",
    "    for (i,j) in types.numpy():\n",
    "        flag_matrix[i,j]=flag\n",
    "        now_pos = (i,j)\n",
    "        while (now_pos in symmetry_map) and (isinstance(symmetry_map[now_pos],tuple )):\n",
    "            now_pos = symmetry_map[now_pos]\n",
    "        if now_pos not in symmetry_map:\n",
    "            symmetry_map[now_pos] = res\n",
    "            res += 1\n",
    "        position_matrix[i,j]= symmetry_map[now_pos]\n",
    "        tensor_order_for_this_type.append(position_matrix[i,j])\n",
    "    active_index.append(tensor_order_for_this_type)\n",
    "    tensor_needed.append(res)\n",
    "assert W*H == len(symmetry_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_symmetry_map(W,H,symmetry='R4Z2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-25660993343c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-25660993343c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    torch.cat([[torch.stack([position_matrix[i,j] for i,j in part if flag_matrix[i,j]==2]] for part in [part_lu_idex,part_ru_idex,part_rd_idex,part_ld_idex] )\u001b[0m\n\u001b[0m                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "torch.cat([[torch.stack([position_matrix[i,j] for i,j in part if flag_matrix[i,j]==2]] for part in [part_lu_idex,part_ru_idex,part_rd_idex,part_ld_idex] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TensorAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 9])\n",
      "torch.Size([2, 9, 9])\n",
      "torch.Size([2, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "B=2;D1=3;D2=4;D3=5;D=3;\n",
    "a=torch.randn(B,D,D,D,D)\n",
    "x = a.permute(0,1,2,3,4).flatten(-4,-3).flatten(-2,-1);print(x.shape)\n",
    "y = a.permute(0,4,1,2,3).flatten(-4,-3).flatten(-2,-1);print(y.shape)\n",
    "z = a.permute(0,3,4,1,2).flatten(-4,-3).flatten(-2,-1);print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D1,D2,D3,D4 = a.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attn_1 = torch.nn.MultiheadAttention(D3*D4,1,batch_first =True)\n",
    "attn_2 = torch.nn.MultiheadAttention(D2*D3,1,batch_first =True)\n",
    "attn_3 = torch.nn.MultiheadAttention(D1*D2,1,batch_first =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(a,a.permute(0,3,4,1,2).permute(0,3,4,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     21,
     42
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TensorAttentionU3D(torch.nn.Module):\n",
    "    def __init__(self,shape,**kargs):\n",
    "        super().__init__()\n",
    "        assert len(shape) == 3\n",
    "        D1,D2,D3 = shape\n",
    "        assert D1==D2==D3\n",
    "        self.shape  = shape\n",
    "        self.attn_1 = torch.nn.MultiheadAttention(D2*D3,1,batch_first =True)\n",
    "        self.attn_2 = torch.nn.MultiheadAttention(D1*D3,1,batch_first =True)\n",
    "        self.attn_3 = torch.nn.MultiheadAttention(D1*D2,1,batch_first =True)\n",
    "    def forward(self,a):\n",
    "        assert len(a.shape)==4\n",
    "        assert a.shape[1:]==self.shape\n",
    "        x = a.permute(0,1,2,3).flatten(-2,-1)\n",
    "        y = a.permute(0,2,3,1).flatten(-2,-1)\n",
    "        z = a.permute(0,3,1,2).flatten(-2,-1)\n",
    "        o1 =  attn_1(x,y,z)[0].reshape(-1,*self.shape).permute(0,1,2,3)\n",
    "        o2 =  attn_1(y,z,x)[0].reshape(-1,*self.shape).permute(0,3,1,2)\n",
    "        o3 =  attn_1(z,x,y)[0].reshape(-1,*self.shape).permute(0,2,3,1)\n",
    "        return (o1+o2+o3)/3\n",
    "\n",
    "class TensorAttentionU4D(torch.nn.Module):\n",
    "    def __init__(self,shape,**kargs):\n",
    "        super().__init__()\n",
    "        assert len(shape) == 4\n",
    "        D1,D2,D3,D4 = shape\n",
    "        assert D1==D2==D3==D4\n",
    "        self.shape  = shape\n",
    "        self.attn_1 = torch.nn.MultiheadAttention(D3*D4,1,batch_first =True)\n",
    "        self.attn_2 = torch.nn.MultiheadAttention(D2*D3,1,batch_first =True)\n",
    "        self.attn_3 = torch.nn.MultiheadAttention(D1*D2,1,batch_first =True)\n",
    "    def forward(self,a):\n",
    "        assert len(a.shape)==5\n",
    "        assert a.shape[1:]==self.shape\n",
    "        x = a.permute(0,1,2,3,4).flatten(-4,-3).flatten(-2,-1)\n",
    "        y = a.permute(0,4,1,2,3).flatten(-4,-3).flatten(-2,-1)\n",
    "        z = a.permute(0,3,4,1,2).flatten(-4,-3).flatten(-2,-1)\n",
    "        o1 =  attn_1(x,y,z)[0].reshape(*a.shape).permute(0,1,2,3,4)\n",
    "        o2 =  attn_1(y,z,x)[0].reshape(*a.shape).permute(0,2,3,4,1)\n",
    "        o3 =  attn_1(z,x,y)[0].reshape(*a.shape).permute(0,3,4,1,2)\n",
    "        return (o1+o2+o3)/3\n",
    "\n",
    "class TensorAttention(torch.nn.Module):\n",
    "    def __init__(self,shape,channels,alpha=4,**kargs):\n",
    "        super().__init__()\n",
    "        self.shape    = shape\n",
    "        self.channels = channels\n",
    "        self.num_dims = num_dims\n",
    "        self.alpha    = alpha\n",
    "        \n",
    "        if len(shape) == 3:\n",
    "            self.engine = TensorAttentionU3D(shape,**kargs)\n",
    "        elif len(shape) == 4:\n",
    "            self.engine = TensorAttentionU4D(shape,**kargs)\n",
    "            \n",
    "        self.dropout= nn.Dropout(p=0.1)\n",
    "        coef   = self.cal_scale(shape,alpha)\n",
    "        self.resize_layer=scaled_Tanh(coef)\n",
    "        self.reset_parameters()\n",
    "    def __repr__(self):\n",
    "        return f\"TensorAttention(shape={self.shape},channels={self.channels},alpha={self.alpha})\"\n",
    "    \n",
    "    def forward(self,x):\n",
    "        reshape = False\n",
    "        if len(x.shape) == len(self.shape) + 1:\n",
    "            pass\n",
    "        elif len(x.shape) == len(self.shape) + 2:\n",
    "            B,C = x.shape[:2]\n",
    "            assert C == self.channels\n",
    "            x = x.reshape(B*C,*self.shape)\n",
    "            reshape=True\n",
    "        res = x\n",
    "        out = self.engine(x)\n",
    "        out+= res\n",
    "        x = self.resize_layer(out)\n",
    "        x = self.dropout(x)\n",
    "        if reshape:\n",
    "            x = x.reshape(B,C,*self.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### aggregation  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     1,
     63
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.two_dim_model import *\n",
    "class PEPS_aggregation_model(TN_Base):\n",
    "    def __init__(self,out_features=None,virtual_bond_dim=None,label_position=None,#=(8,4),\n",
    "                      fixed_virtual_dim=None,\n",
    "                       patch_engine=torch.nn.Identity,\n",
    "                       symmetry=None,#\"Z2_16x9\",\n",
    "                       seted_variation=10,\n",
    "                       init_std=1e-10,\n",
    "                       solved_std=None,\n",
    "                       convertPeq1=False):\n",
    "        super().__init__()\n",
    "        assert out_features is not None\n",
    "        assert virtual_bond_dim is not None\n",
    "        assert label_position is not None\n",
    "        assert isinstance(fixed_virtual_dim,int)\n",
    "        if isinstance(virtual_bond_dim,str):\n",
    "            arbitary_shape_state_dict = torch.load(virtual_bond_dim)\n",
    "        else:\n",
    "            arbitary_shape_state_dict = virtual_bond_dim\n",
    "        assert isinstance(arbitary_shape_state_dict,list)\n",
    "        assert isinstance(arbitary_shape_state_dict[0],dict)\n",
    "        self.convertPeq1  = convertPeq1\n",
    "        self.out_features = out_features\n",
    "        self.modules_list = torch.nn.ModuleList()\n",
    "        # if you want to use aggregation model, then all the patch config should follow the \n",
    "        # same group order, for example, group should be arranged like  group0, group1,group2\n",
    "        #                                                               group3, group4,group5\n",
    "        #                                                               .....\n",
    "        # otherwise, they won't share the optim path and won't share same _input list\n",
    "        \n",
    "        ### reorder group\n",
    "        for patch_json in virtual_bond_dim:\n",
    "            info_per_point = patch_json['element']\n",
    "            info_per_group = patch_json['node']\n",
    "            \n",
    "            x_max,y_max = np.array(list(info_per_point.keys())).max(0)\n",
    "            info_per_group_new = {}\n",
    "            processed_group={}\n",
    "            for i in range(x_max + 1):\n",
    "                for j in range(y_max + 1):               \n",
    "                    real_group = info_per_point[i,j]['group']\n",
    "                    if real_group not in processed_group:\n",
    "                        ordered_group = len(info_per_group_new)\n",
    "                        info_per_group_new[ordered_group] = info_per_group[real_group]\n",
    "                        processed_group[real_group] = ordered_group\n",
    "                    else:\n",
    "                        ordered_group = processed_group[real_group]    \n",
    "                    info_per_point[i,j]['group'] = ordered_group\n",
    "            patch_json['node'] =  info_per_group_new\n",
    "                    \n",
    "        \n",
    "        for i,virtual_bond in enumerate(virtual_bond_dim):\n",
    "            #print(i)\n",
    "            self.modules_list.append(PEPS_einsum_arbitrary_partition_optim(out_features=out_features,\n",
    "                                                                           fixed_virtual_dim=fixed_virtual_dim,\n",
    "                                                                           virtual_bond_dim = copy.deepcopy(virtual_bond),\n",
    "                                                                           label_position = label_position,\n",
    "                                                                           patch_engine   = patch_engine,\n",
    "                                                                           symmetry=symmetry,\n",
    "                                                                           seted_variation=seted_variation,\n",
    "                                                                           init_std=init_std,\n",
    "                                                                           solved_std=solved_std,\n",
    "                                                                           convertPeq1=convertPeq1))\n",
    "    def forward(self,input_data):\n",
    "        assert len(input_data.shape)==4\n",
    "        assert np.prod(input_data.shape[-2:])==len(self.modules_list[0].info_per_point)\n",
    "        all_inputs  = [module(input_data,only_return_input=True) for module in self.modules_list]\n",
    "        all_inputs = [torch.stack([t[i] for t in all_inputs],1) for i in range(len(all_inputs[0]))]\n",
    "        \n",
    "        operands=[]\n",
    "        for tensor,sublist in zip(all_inputs,self.modules_list[0].sublist_list):\n",
    "            operand = [tensor,[...,*sublist]]\n",
    "            operands+=operand\n",
    "        operands+= [[...,*self.modules_list[0].outlist]]\n",
    "        return self.einsum_engine(*operands,optimize=self.modules_list[0].path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "patch_partions_3colum_max45raw_json_list = torch.load(\"models/arbitary_shape/patch_partions_3colum_max45raw_json_list.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "                               virtual_bond_dim=copy.deepcopy(patch_partions_3colum_max45raw_json_list[5]),\n",
    "                               label_position=(8,4),\n",
    "                               symmetry=\"Z2_16x9\",\n",
    "                               #patch_engine=None,\n",
    "                               fixed_virtual_dim=4,\n",
    "                               convertPeq1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = PEPS_aggregation_model(out_features=1,\n",
    "                               virtual_bond_dim=copy.deepcopy(patch_partions_3colum_max45raw_json_list),\n",
    "                               label_position=(8,4),\n",
    "                               symmetry=\"Z2_16x9\",\n",
    "                               #patch_engine=None,\n",
    "                               fixed_virtual_dim=4,\n",
    "                               #convertPeq1=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=torch.randn(1,1,16,9)\n",
    "out = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9854e-13, grad_fn=<CopyBackwards>)\n",
      "tensor(3.9854e-13, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros_like(a)\n",
    "b[...,0,:]=a[...,0,:]\n",
    "b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "print(model(a).norm())\n",
    "print(model(b).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from mltool.dataaccelerate import DataSimfetcher\n",
    "from mltool.loggingsystem import LoggingSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b73e7f598b5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=3000, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=3000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(dataset=dataset_train, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_valid , batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model = MPSLinear(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=100,\n",
    "#                   bias=False,label_position='center',init_std=0)\n",
    "#model = PEPS_einsum_uniform_shape_6x6_fast(in_physics_bond=16,virtual_bond_dim=3,init_std=1)\n",
    "inp_channel = 1 \n",
    "mid_channel = 16 \n",
    "out_channel = 32\n",
    "model = nn.Sequential(\n",
    "#                       nn.Conv2d(inp_channel, mid_channel, kernel_size=3, bias=False),\n",
    "#                       nn.BatchNorm2d(mid_channel),\n",
    "#                       nn.ReLU(inplace=True),\n",
    "#                       nn.Conv2d(mid_channel, out_channel, kernel_size=3, bias=False),\n",
    "#                       nn.BatchNorm2d(out_channel),\n",
    "#                       nn.ReLU(inplace=True),\n",
    "                      Patch2NetworkInput(1),\n",
    "                      PEPS_uniform_shape_symmetry_any(W=4,H=4,in_physics_bond=16,init_std=1e-5,out_features=16,virtual_bond_dim=2,symmetry='P4Z2'),\n",
    "                      nn.Linear(16,32)\n",
    "                     )\n",
    "#model = PEPS_einsum_arbitrary_partition_optim(virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_3.json\",init_std=1e-2,solved_std=0.024)\n",
    "#model  = AMPSShare(n=28*28, bond_dim=10, phys_dim=2)\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PEPS_einsum_arbitrary_partition_optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6aa978c478b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPEPS_einsum_arbitrary_partition_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvirtual_bond_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/arbitary_shape/arbitary_shape_16x9_2.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymmetry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Z2_16x9\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PEPS_einsum_arbitrary_partition_optim' is not defined"
     ]
    }
   ],
   "source": [
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=32,virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x9_2.json\",symmetry=\"Z2_16x9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "infiniter = DataSimfetcher(train_loader, device=device)\n",
    "flip = lambda x:torch.flip(x,dims=(-2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#image=torch.randn(1,1,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bcd052184c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minfiniter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_images' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "curve,image= infiniter.next()\n",
    "aa = preprocess_images(image)\n",
    "cc = preprocess_images(flip(image))\n",
    "\n",
    "a = aa[:1]\n",
    "c = cc[:1]\n",
    "b = torch.zeros_like(a)\n",
    "b[...,0,:]=a[...,0,:]\n",
    "b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "print(model(a).norm())\n",
    "print(model(b).norm())\n",
    "print(model(c).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5268e-09, grad_fn=<CopyBackwards>)\n",
      "tensor(8.5268e-09, grad_fn=<CopyBackwards>)\n",
      "tensor(8.5268e-09, grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    # the rfft result of a 16x9 image.\n",
    "    # the first line FM[0] is the unque line\n",
    "    # if we flip the images,\n",
    "    # the returned FM would flip in FM[1:] and FM[0] remain\n",
    "    return abs(torch.fft.fftshift(torch.fft.rfft2(images,norm='ortho')))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.3264), tensor(0.9903)) torch.Size([100, 1, 16, 9])\n",
      "(tensor(1.1805), tensor(0.0188)) torch.Size([100, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "infiniter = DataSimfetcher(train_loader, device=device)\n",
    "curve,image= infiniter.next()\n",
    "bs,c,w,h = image.shape\n",
    "with torch.no_grad():\n",
    "    x     = preprocess_images(image);print(torch.std_mean(x),x.shape)\n",
    "    x     = model(x);print(torch.std_mean(x),x.shape)\n",
    "    #label = image.flatten().long()\n",
    "    #logits     = model(binary).reshape(bs*w*h,2)\n",
    "    #print(model(binary).squeeze().norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from mltool.dataaccelerate import DataSimfetcher\n",
    "from mltool.loggingsystem import LoggingSystem\n",
    "\n",
    "# train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=3000, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=3000, shuffle=False)\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(dataset=dataset_train, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_valid , batch_size=10,shuffle=False)\n",
    "\n",
    "# model = MPSLinear(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=100,\n",
    "#                   bias=False,label_position='center',init_std=0)\n",
    "#model = PEPS_einsum_uniform_shape_6x6_fast(in_physics_bond=16,virtual_bond_dim=3,init_std=1)\n",
    "inp_channel = 1 \n",
    "mid_channel = 16 \n",
    "out_channel = 32\n",
    "model = nn.Sequential(\n",
    "#                       nn.Conv2d(inp_channel, mid_channel, kernel_size=3, bias=False),\n",
    "#                       nn.BatchNorm2d(mid_channel),\n",
    "#                       nn.ReLU(inplace=True),\n",
    "#                       nn.Conv2d(mid_channel, out_channel, kernel_size=3, bias=False),\n",
    "#                       nn.BatchNorm2d(out_channel),\n",
    "#                       nn.ReLU(inplace=True),\n",
    "                      Patch2NetworkInput(1),\n",
    "                      PEPS_uniform_shape_symmetry_any(W=4,H=4,in_physics_bond=16,init_std=1e-5,out_features=16,virtual_bond_dim=2,symmetry='P4Z2'),\n",
    "                      nn.Linear(16,32)\n",
    "                     )\n",
    "#model = PEPS_einsum_arbitrary_partition_optim(virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_3.json\",init_std=1e-2,solved_std=0.024)\n",
    "#model  = AMPSShare(n=28*28, bond_dim=10, phys_dim=2)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=32,virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x9_2.json\",symmetry=\"Z2_16x9\")\n",
    "\n",
    "device = 'cpu'\n",
    "infiniter = DataSimfetcher(train_loader, device=device)\n",
    "flip = lambda x:torch.flip(x,dims=(-2,))\n",
    "\n",
    "#image=torch.randn(1,1,16,16)\n",
    "\n",
    "\n",
    "curve,image= infiniter.next()\n",
    "aa = preprocess_images(image)\n",
    "cc = preprocess_images(flip(image))\n",
    "\n",
    "a = aa[:1]\n",
    "c = cc[:1]\n",
    "b = torch.zeros_like(a)\n",
    "b[...,0,:]=a[...,0,:]\n",
    "b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "print(model(a).norm())\n",
    "print(model(b).norm())\n",
    "print(model(c).norm())\n",
    "\n",
    "def preprocess_images(images):\n",
    "    # the rfft result of a 16x9 image.\n",
    "    # the first line FM[0] is the unque line\n",
    "    # if we flip the images,\n",
    "    # the returned FM would flip in FM[1:] and FM[0] remain\n",
    "    return abs(torch.fft.fftshift(torch.fft.rfft2(images,norm='ortho')))*2\n",
    "\n",
    "\n",
    "\n",
    "infiniter = DataSimfetcher(train_loader, device=device)\n",
    "curve,image= infiniter.next()\n",
    "bs,c,w,h = image.shape\n",
    "with torch.no_grad():\n",
    "    x     = preprocess_images(image);print(torch.std_mean(x),x.shape)\n",
    "    x     = model(x);print(torch.std_mean(x),x.shape)\n",
    "    #label = image.flatten().long()\n",
    "    #logits     = model(binary).reshape(bs*w*h,2)\n",
    "    #print(model(binary).squeeze().norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.extend_model import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "arbitary_shape_state_dict = torch.load(\"models/arbitary_shape/arbitary_shape_16x16_6x6.json\")\n",
    "info_per_group = arbitary_shape_state_dict['node']\n",
    "info_per_line  = arbitary_shape_state_dict['line']\n",
    "info_per_point = arbitary_shape_state_dict['element']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "                                            virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x16_6x6.json\",\n",
    "                                            label_position=\"no_center\",\n",
    "                                                symmetry=\"Z2_16x16\",\n",
    "                                                patch_engine=lambda *arg:TensorNetConvND_Single(*arg,alpha=0.6),\n",
    "                                                fixed_virtual_dim=2,\n",
    "                                                #convertPeq1=True\n",
    "                                               )\n",
    "model.weight_init(method=\"Expecatation_Normalization2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3016, device='cuda:0')\n",
      "tensor(0.3016, device='cuda:0')\n",
      "tensor(0.3016, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    a = torch.randn(1,1,16,16).cuda()\n",
    "    b = torch.flip(a,dims=(-1,))\n",
    "    c = torch.flip(a,dims=(-2,))\n",
    "    print(model(a).norm())\n",
    "    print(model(b).norm())\n",
    "    print(model(c).norm())\n",
    "#     a_input,a_units = model(a,only_return_input=True)\n",
    "#     b_input,b_units = model(b,only_return_input=True)\n",
    "#     c_input,c_units = model(c,only_return_input=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### archive script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from train_base import *\n",
    "# statisticinfo= torch.load(\"/media/tianning/DATA/DATASET/MNIST/MNIST/statisitc_stdmean.pt\")\n",
    "# statistic_std= statisticinfo['statisitc_std']\n",
    "# statistic_mean=statisticinfo['statisitc_mean']\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,)),\n",
    "#     #lambda x:(x-statistic_mean)/(statistic_std+1e-8),\n",
    "#     transforms.CenterCrop(16)\n",
    "# ])\n",
    "# DATAPATH    = '/media/tianning/DATA/DATASET/MNIST/'\n",
    "# mnist_train = datasets.MNIST(DATAPATH, train=True, download=False, transform=transform)\n",
    "# mnist_test  = datasets.MNIST(DATAPATH, train=False,download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=100, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=mnist_test , batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### data processing ############\n",
    "# import json\n",
    "# with open(f\"dataset/fft16x9_statistic_info_{flag}.json\",\"r\") as f:\n",
    "#     fft16x9_statistic_info = json.load(f)\n",
    "#     mean_info = torch.Tensor(fft16x9_statistic_info['mean'])\n",
    "#     std_info  = torch.Tensor(fft16x9_statistic_info['std'])\n",
    "\n",
    "# fftdata=abs(torch.fft.fftshift(torch.fft.rfft2(dataset_train.imagedata,norm='ortho')))\n",
    "# with open(\"dataset/fft16x9_statistic_info_PLR.json\",\"w\") as f:\n",
    "#     fft16x9_statistic_info = {'mean':torch.mean(fftdata,0)[0].numpy().tolist(),\n",
    "#                               'std':torch.std(fftdata,0)[0].numpy().tolist()}\n",
    "#     json.dump(fft16x9_statistic_info,f)\n",
    "# def preprocess_images(image):\n",
    "#     return image\n",
    "#     return (image-0.5)/0.5\n",
    "\n",
    "# import numpy as np\n",
    "# def preprocess_images(images):\n",
    "#     return images\n",
    "#     # the rfft result of a 16x9 image.\n",
    "#     # the first line FM[0] is the unque line\n",
    "#     # if we flip the images,\n",
    "#     # the returned FM would flip in FM[1:] and FM[0] remain\n",
    "#     fM = abs(torch.fft.fftshift(torch.fft.rfft2(images,norm='ortho')))\n",
    "#     fM = (fM - mean_info.to(images.device))/(std_info.to(images.device))\n",
    "#     return fM\n",
    "# #     #return fM*1.6/np.sqrt(2)\n",
    "# #     return (fM+0.5).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### for the overfitting model\n",
    "# from dataset import get_balance_2_classifier_dataset\n",
    "# flag = 'RDN'\n",
    "# dataset_train,dataset_valid = get_balance_2_classifier_dataset(curve_branch='T',dataset=flag,image_transformer='fft16x9_norm')\n",
    "## Epoch: 005 \t Loss: 0.3377 \t Acct: 0.8500 \t Accu: 0.1827 \t Time: 718.98 s\n",
    "# model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "#                                                 virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x9_2.json\",\n",
    "#                                                 label_position=(8,4),\n",
    "#                                                 symmetry=\"Z2_16x9\",\n",
    "#                                                 patch_engine=lambda *arg:TensorNetConvND_Single(*arg,alpha=4),\n",
    "#                                                 #fixed_virtual_dim=4,\n",
    "#                                                 convertPeq1=True\n",
    "#                                                )\n",
    "# model.weight_init(method=\"Expecatation_Normalization2\")\n",
    "# state_dict = torch.load(\"checkpoints/SMSDatasetB1NES128/PEPS_16x9_Z2_Binary_CNN_0_with_convertPeq1/overfitting_model_20220513.pt\")\n",
    "# model.load_state_dict(state_dict['state_dict'])\n",
    "#model.set_drop_prob(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:0:best alpha 3.0180547035271985\n",
      "id:1:best alpha 2.2516877882991997\n",
      "id:2:best alpha 2.3530810912002273\n",
      "id:3:best alpha 2.244412880820901\n",
      "id:4:best alpha 2.317596416950119\n",
      "id:5:best alpha 2.479475871624854\n",
      "id:6:best alpha 2.327781434686427\n",
      "id:7:best alpha 2.4212193717479957\n",
      "id:8:best alpha 2.376767072840816\n",
      "id:9:best alpha 2.9012983185212207\n",
      "id:10:best alpha 2.52467272741226\n",
      "id:11:best alpha 2.4784325989626934\n",
      "id:12:best alpha 2.3225382279981925\n",
      "id:13:best alpha 2.388135933138411\n",
      "id:14:best alpha 2.308091791422882\n",
      "id:15:best alpha 2.8206981551680146\n",
      "id:16:best alpha 2.3541422112015464\n",
      "id:17:best alpha 2.2443119183832803\n",
      "id:18:best alpha 2.362978985498086\n"
     ]
    }
   ],
   "source": [
    "########## get a suitabel alpha ##################\n",
    "##### to get right alpha number\n",
    "# virtual_bond_dim=torch.load(\"models/arbitary_shape/patch_partions_3colum_max45raw_json_list.pt\")\n",
    "# for patch_json in virtual_bond_dim:\n",
    "#     info_per_point = patch_json['element']\n",
    "#     info_per_group = patch_json['node']\n",
    "\n",
    "#     x_max,y_max = np.array(list(info_per_point.keys())).max(0)\n",
    "#     info_per_group_new = {}\n",
    "#     processed_group={}\n",
    "#     for i in range(x_max + 1):\n",
    "#         for j in range(y_max + 1):\n",
    "#             real_group = info_per_point[i,j]['group']\n",
    "#             if real_group not in processed_group:\n",
    "#                 ordered_group = len(info_per_group_new)\n",
    "#                 info_per_group_new[ordered_group] = info_per_group[real_group]\n",
    "#                 processed_group[real_group] = ordered_group\n",
    "#             else:\n",
    "#                 ordered_group = processed_group[real_group]\n",
    "#             info_per_point[i,j]['group'] = ordered_group\n",
    "#     patch_json['node'] =  info_per_group_new\n",
    "# alpha_list = np.linspace(0.2,2,20)\n",
    "# for config_id,structure_config in enumerate(copy.deepcopy(virtual_bond_dim)):\n",
    "#     std_record=[]\n",
    "#     for alpha in alpha_list:\n",
    "#         model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "#                                                     virtual_bond_dim=copy.deepcopy(structure_config),\n",
    "#                                                     label_position=(8,4),\n",
    "#                                                     symmetry=\"Z2_16x9\",\n",
    "#                                                     patch_engine=lambda *arg:TensorNetConvND_Single(*arg,alpha=alpha),\n",
    "#                                                     fixed_virtual_dim=5,\n",
    "#                                                     convertPeq1=True\n",
    "#                                                   )\n",
    "#         model.weight_init(method=\"Expecatation_Normalization2\")\n",
    "#         device = 'cuda'\n",
    "#         model  = model.to(device).eval()\n",
    "#         with torch.no_grad():\n",
    "#             std_list = []\n",
    "#             for _ in range(10):\n",
    "#                  std_list.append(torch.std(model(torch.randn(100,1,16,9).cuda())).item())\n",
    "#         std_record.append(np.mean(std_list))            \n",
    "#     headers_str = [str(b) for b in alpha_list]\n",
    "#     data = np.array([std_record])\n",
    "#    #tp.table(data, headers_str)\n",
    "#     x = alpha_list\n",
    "#     y = std_record\n",
    "#     z1 = np.polyfit(x, y, 6) #用3次多项式拟合，输出系数从高到0\n",
    "#     p1 = np.poly1d(z1) #使用次数合成多项式\n",
    "#     y_pre = p1(x)\n",
    "#     #print(\"拟合结果: y = %10.5f x + %10.5f \" % (a,b) )\n",
    "#     print(f\"id:{config_id}:best alpha {[t.real for t in np.roots(z1-2) if np.isreal(t) and t>1][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.extend_model import *\n",
    "model = PEPS_uniform_shape_symmetry_6x6(out_features=16,in_physics_bond=16,**kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.extend_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "#                                                 virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x9_2.json\",\n",
    "#                                                 label_position=(8,4),\n",
    "#                                                 symmetry=\"Z2_16x9\",\n",
    "#                                                 patch_engine=lambda *arg:TensorNetConvND_Single(*arg,alpha=1.1),\n",
    "#                                                 fixed_virtual_dim=4,\n",
    "#                                                 convertPeq1=True\n",
    "#                                                )\n",
    "# model.weight_init(method=\"Expecatation_Normalization2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from models.extend_model import *\n",
    "# model = PEPS_aggregation_model(out_features=1,\n",
    "#                                virtual_bond_dim=\"models/arbitary_shape/patch_partions_3column_12units.pt\",\n",
    "#                                label_position=(8,4),\n",
    "#                                symmetry=\"Z2_16x9\",\n",
    "#                                patch_engine=TensorNetConvND_Single,\n",
    "#                                alpha_list=1,\n",
    "#                                fixed_virtual_dim=5,\n",
    "#                                convertPeq1=True\n",
    "#                               )\n",
    "# model.weight_init(method=\"Expecatation_Normalization2\")\n",
    "# #model.get_alpha_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "                                            virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x16_6x6.json\",\n",
    "                                            label_position=\"no_center\",\n",
    "                                                symmetry=\"Z2_16x16\",\n",
    "                                                patch_engine=lambda *arg:TensorNetConvND_Single(*arg,alpha=1.5),\n",
    "                                                fixed_virtual_dim=4,\n",
    "                                                #convertPeq1=True\n",
    "                                               )\n",
    "model.weight_init(method=\"Expecatation_Normalization2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_num,get_model_para_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     8
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n",
      "tensor([7.1312, 4.5188, 0.8427, 1.5331, 1.8426, 3.1857, 1.9725, 1.0028, 2.4620,\n",
      "        3.0645, 2.0335, 2.2934, 0.4913, 1.4893, 2.0801, 2.0322, 0.6756, 2.0323,\n",
      "        1.6720, 9.4965, 7.0030, 7.5361, 7.0705, 2.0254, 1.8864, 2.1027, 4.1485,\n",
      "        5.7367], device='cuda:0') torch.Size([100, 28])\n",
      "Totally 1232 parameters, totally 301420 numbers\n",
      "ops=1232,paras=301420\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "model = PEPS_16x16_Z2_Binary_Aggregation_Wrapper(TensorNetConvND_Single,\n",
    "\"models/arbitary_shape/patch_partions_28_6x6_Z2_json_list.pt\",fixed_virtual_dim=4, alpha_list = 1)(out_features=1)\n",
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.randint(0,2,(100,1,16,16)).float().to(device)-0.5\n",
    "    print(x.max())\n",
    "    try:\n",
    "        x = model(x,do_average=False);\n",
    "    except:\n",
    "        x = model(x);\n",
    "    print(torch.std(x,0),x.shape)\n",
    "ops,paras=get_model_para_num(model)\n",
    "print(f\"ops={ops},paras={paras}\")\n",
    "# # [p.shape[1:] for p in model.unique_unit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from models import *\n",
    "model = PEPS_16x9_Z2_Binary_Aggregation_Wrapper(TensorAttention,#ops=3120,paras=71456\n",
    "\"models/arbitary_shape/patch_partions_5column_12units.pt\", alpha_list = 30,fixed_virtual_dim=2,complex_number_mode=True)(out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6929],\n",
      "        [ 3.2353],\n",
      "        [ 9.7131],\n",
      "        [ 5.6308],\n",
      "        [ 4.7460],\n",
      "        [ 1.8464],\n",
      "        [16.9250],\n",
      "        [ 5.6695],\n",
      "        [14.7725],\n",
      "        [ 4.9229],\n",
      "        [ 3.2875],\n",
      "        [ 1.1423]], device='cuda:0') torch.Size([20, 12, 1])\n",
      "Totally 3120 parameters, totally 71456 numbers\n",
      "ops=3120,paras=71456\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.view_as_complex(torch.randn(20,1,16,9,2)).to(device)\n",
    "    try:\n",
    "        x = model(x,do_average=False);\n",
    "    except:\n",
    "        x = model(x);\n",
    "    print(torch.std(x,0),x.shape)\n",
    "ops,paras=get_model_para_num(model)\n",
    "print(f\"ops={ops},paras={paras}\")\n",
    "# # [p.shape[1:] for p in model.unique_unit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0394, device='cuda:0')\n",
      "tensor(0.0394, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    a = torch.randn(1,1,16,9).cuda()\n",
    "    b = torch.zeros_like(a)\n",
    "    b[...,0,:]=a[...,0,:]\n",
    "    b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "    print(model(a).norm())\n",
    "    print(model(b).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cca87e43979a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model =  PEPS_16x9_Z2_Binary_Aggregation_Wrapper(TensorAttention,\n\u001b[0;32m----> 3\u001b[0;31m                                     \"models/arbitary_shape/patch_partions_3column_12units.pt\", alpha_list = 0.2,fixed_virtual_dim=None)(out_features=1)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = PEPS_16x9_Z2_Binary_Wrapper(TensorAttention,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                                     \"models/arbitary_shape/arbitary_shape_16x9_13.json\",fixed_virtual_dim=4,alpha=0.15)(out_features=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/models/extend_model_16x9.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, alpha_list, **kargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                    \u001b[0malpha_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                    \u001b[0mfixed_virtual_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_virtual_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                    \u001b[0mconvertPeq1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertPeq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                                   )\n\u001b[1;32m     40\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Expecatation_Normalization2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/models/two_dim_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, out_features, virtual_bond_dim, label_position, fixed_virtual_dim, patch_engine, alpha_list, symmetry, seted_variation, init_std, solved_std, convertPeq1)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvirtual_bond_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlabel_position\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_virtual_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvirtual_bond_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mvirtual_bond_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvirtual_bond_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.extend_model import *\n",
    "model =  PEPS_16x9_Z2_Binary_Aggregation_Wrapper(TensorAttention,\n",
    "                                    \"models/arbitary_shape/patch_partions_3column_12units.pt\", alpha_list = 0.2,fixed_virtual_dim=2)(out_features=1)\n",
    "# model = PEPS_16x9_Z2_Binary_Wrapper(TensorAttention,\n",
    "#                                     \"models/arbitary_shape/arbitary_shape_16x9_13.json\",fixed_virtual_dim=4,alpha=0.15)(out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import *\n",
    "model =PEPS_16x9_Z2_Binary_Aggregation_Wrapper(TensorAttention,\n",
    "               \"models/arbitary_shape/patch_partions_5column_12units.pt\", alpha_list = 0.1,fixed_virtual_dim=2)(out_features=1)\n",
    "#model.debugQ=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PEPS_16x9_Z2_Binary_TAT_Aggregation_19_5_v3(out_features=1,alpha_list=0.06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4342],\n",
      "        [ 2.4656],\n",
      "        [15.7809],\n",
      "        [ 0.8383],\n",
      "        [ 2.2443],\n",
      "        [ 1.2604],\n",
      "        [ 1.4427],\n",
      "        [ 0.3181],\n",
      "        [ 7.3057],\n",
      "        [ 5.1960],\n",
      "        [ 2.9489],\n",
      "        [ 3.8118]], device='cuda:0') torch.Size([10, 12, 1])\n",
      "Totally 3120 parameters, totally 71456 numbers\n",
      "ops=3120,paras=71456\n"
     ]
    }
   ],
   "source": [
    "from mltool.universal_model_util import get_model_para_num\n",
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(10,1,16,9).to(device)\n",
    "    try:\n",
    "        x = model(x,do_average=False);\n",
    "    except:\n",
    "        x = model(x);\n",
    "    print(torch.std(x,0),x.shape)\n",
    "ops,paras=get_model_para_num(model)\n",
    "print(f\"ops={ops},paras={paras}\")\n",
    "# # [p.shape[1:] for p in model.unique_unit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0593, device='cuda:0')\n",
      "tensor(0.0593, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    a = torch.randn(1,1,16,9).cuda()\n",
    "    b = torch.zeros_like(a)\n",
    "    b[...,0,:]=a[...,0,:]\n",
    "    b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "    print(model(a).norm())\n",
    "    print(model(b).norm())\n",
    "#     x = torch.randn(100,1,16,9).to(device)\n",
    "#     x = model(x,do_average=False);print(torch.std(x,0),x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataSimfetcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2027923a1ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minfiniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSimfetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minfiniter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataSimfetcher' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "infiniter = DataSimfetcher(train_loader, device=device)\n",
    "curve,image= infiniter.next()\n",
    "bs,c,w,h = image.shape\n",
    "x     = image\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(torch.std_mean(x),x.shape)\n",
    "    #x     = preprocess_images(x);\n",
    "    x     = model(x);print(torch.std(x,0),x.shape)\n",
    "    #label = image.flatten().long()\n",
    "    #logits     = model(binary).reshape(bs*w*h,2)\n",
    "    #print(model(binary).squeeze().norm())\n",
    "    #a = image\n",
    "    #b = torch.zeros_like(a)\n",
    "    #b[...,0,:]=a[...,0,:]\n",
    "    #b[...,1:,:]=torch.flip(a[...,1:,:],dims=(-2,))\n",
    "    #print(model(a).norm())\n",
    "    #print(model(b).norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from mltool.visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will use 【none】 norm on vector data\n",
      "/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128\n",
      "find offline curve data at --->|/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128/train_curvedata_108.npy|<-----\n",
      "find offline image data at --->|/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128/train_imagedata_108.npy|<-----\n",
      "find offline feature data at --->|/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128/train.feat.onehot.balance_leftorright_108.npy|<-----\n",
      "use fft16x9_complex of RDN DATASET\n",
      "this dataset use 【none】 norm\n",
      ">>>> train dataset size 108000\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|             type|         fea_type| curve data shape| image data shape|vector data shape|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|            train|              all|   (108000,1,128)|  (108000,1,16,9)|         (108000)|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "Inherit norm method from last dataset\n",
      "/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128\n",
      "find offline curve data at --->|/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128/test_curvedata_3.npy|<-----\n",
      "find offline image data at --->|/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128/test_imagedata_3.npy|<-----\n",
      "find offline feature data at --->|/media/tianning/DATA/metasurface/Compressed/RDNDATASET/B1NES128/test.feat.onehot.balance_leftorright_3.npy|<-----\n",
      "use fft16x9_complex of RDN DATASET\n",
      ">>>> test dataset size 3000\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|             type|         fea_type| curve data shape| image data shape|vector data shape|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|             test|              all|     (3000,1,128)|    (3000,1,16,9)|           (3000)|\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "from dataset import get_balance_2_classifier_dataset\n",
    "flag = 'RDN'\n",
    "dataset_train,dataset_valid = get_balance_2_classifier_dataset(curve_branch='T',dataset=flag,image_transformer='fft16x9_complex'\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltool.visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mltool.dataaccelerate import DataSimfetcher\n",
    "from mltool.loggingsystem import LoggingSystem\n",
    "train_loader= torch.utils.data.DataLoader(dataset=dataset_train , batch_size=200, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_valid , batch_size=200,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(dataset=dataset_train , batch_size=200, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_valid , batch_size=200,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from models.extend_model import *\n",
    "model=PEPS_einsum_arbitrary_partition_optim(out_features=1,\n",
    "                                            virtual_bond_dim=\"models/arbitary_shape/arbitary_shape_16x9_13.json\",\n",
    "                                            label_position=(8,4),#\"no_center\",\n",
    "                                                symmetry=\"Z2_16x9\",\n",
    "                                                patch_engine=lambda *arg:TensorNetConvND_Single(*arg,alpha=50),\n",
    "                                                fixed_virtual_dim=3,complex_number_mode=True,\n",
    "                                                #convertPeq1=True\n",
    "                                               )\n",
    "model.weight_init(method=\"Expecatation_Normalization2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0634], device='cuda:0') torch.Size([10, 1])\n",
      "Totally 70 parameters, totally 5670 numbers\n",
      "ops=70,paras=5670\n"
     ]
    }
   ],
   "source": [
    "from mltool.universal_model_util import get_model_para_num\n",
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.view_as_complex(torch.randn(10,1,16,9,2)).to(device)\n",
    "    #x = torch.randn(10,1,16,9).to(device)\n",
    "    try:\n",
    "        x = model(x,do_average=False);\n",
    "    except:\n",
    "        x = model(x);\n",
    "    print(torch.std(x,0),x.shape)\n",
    "ops,paras=get_model_para_num(model)\n",
    "print(f\"ops={ops},paras={paras}\")\n",
    "# # [p.shape[1:] for p in model.unique_unit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "model = PEPS_16x9_Z2_Binary_Wrapper(TensorNetConvND_Single,\n",
    "                               \"models/arbitary_shape/arbitary_shape_16x9_3.json\",fixed_virtual_dim=4,alpha=1.5)(out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "loss_fn   = torch.nn.BCEWithLogitsLoss()\n",
    "#loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "model  = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########## LBFGS train #################\n",
    "from models.two_dim_model import PEPS_arbitary_shape_16x9_2_Z2_symmetry\n",
    "model = PEPS_arbitary_shape_16x9_2_Z2_symmetry()\n",
    "loss_fn   = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "device = 'cuda'\n",
    "model  = model.to(device)\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.01)\n",
    "\n",
    "dataset_train.length=1200;len(dataset_train.imagedata)\n",
    "train_loader= torch.utils.data.DataLoader(dataset=dataset_train, batch_size=400, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_valid , batch_size=400,shuffle=False)\n",
    "\n",
    "from train_base import *\n",
    "logsys            = LoggingSystem(True,\"log/test\")\n",
    "metric_list       = ['loss','g_norm']\n",
    "losses=[]\n",
    "accues=[]\n",
    "\n",
    "metric_dict       = logsys.initial_metric_dict(metric_list)\n",
    "master_bar        = logsys.create_master_bar(300)\n",
    "master_bar.set_multiply_graph(figsize=(9,3),engine=[['plot','plot']],labels=[metric_list])\n",
    "accu = loss = -1\n",
    "accu_train = accu_valid = loss = -1\n",
    "global global_count\n",
    "global_count= 1\n",
    "total_count  = optimizer.param_groups[0]['max_iter']\n",
    "def closure():\n",
    "    global global_count\n",
    "    model.train()\n",
    "    infiniter = DataSimfetcher(train_loader, device=device)\n",
    "    inter_b   = logsys.create_progress_bar(len(train_loader))\n",
    "    optimizer.zero_grad()\n",
    "    loss_all = 0\n",
    "    while inter_b.update_step():\n",
    "        label,image= infiniter.next()\n",
    "        bs,c,w,h   = image.shape\n",
    "        loss       = loss_fn(model(image).squeeze(),label.float().squeeze())\n",
    "        loss       = loss/len(train_loader.dataset)\n",
    "        loss.backward()\n",
    "        loss_all+=loss.item()\n",
    "        #print(loss_all)\n",
    "    master_bar.lwrite('Epoch: %.3i:{%.2i}/{%.2i} \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, global_count,total_count,loss_all, accu_train,accu_valid,time.time() - start_time),end='\\r')\n",
    "    global_count+=1\n",
    "    return loss_all\n",
    "\n",
    "def test_epoch(model,data_loader):\n",
    "    model.eval()\n",
    "    prefetcher = DataSimfetcher(data_loader, device=device)\n",
    "    inter_b    = logsys.create_progress_bar(len(test_loader))\n",
    "    labels     = []\n",
    "    logits     = []\n",
    "    with torch.no_grad():\n",
    "        while inter_b.update_step():\n",
    "            label,image = prefetcher.next()[:2]\n",
    "            binary     = image\n",
    "            logit      = torch.sigmoid(model(binary).squeeze())\n",
    "            labels.append(label)\n",
    "            logits.append(logit)\n",
    "    labels  = torch.cat(labels)\n",
    "    logits  = torch.cat(logits)\n",
    "   \n",
    "    if len(logits.shape)==2:\n",
    "        pred_labels  = torch.argmax(logits,-1)\n",
    "        accu =  torch.sum(pred_labels == labels)/len(pred_labels)\n",
    "    elif len(logits.shape)==1:\n",
    "        accu =  torch.sum(torch.round(logits) == labels)/len(labels)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    accu = accu.item()\n",
    "    return accu\n",
    "\n",
    "   #master_bar.lwrite('Epoch: %.3i \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accut,accu,time.time() - start_time),end='\\r')\n",
    "\n",
    "for epoch in master_bar:\n",
    "    start_time = time.time()\n",
    "#     accu_valid = test_epoch(model,test_loader)\n",
    "#     master_bar.lwrite('Epoch: %.3i/accu_v \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accu_train,accu_valid,time.time() - start_time),end='\\r')\n",
    "#     accu_train = test_epoch(model,train_loader)\n",
    "#     master_bar.lwrite('Epoch: %.3i/accu_t \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accu_train,accu_valid,time.time() - start_time),end='\\r')\n",
    "    global_count = 0\n",
    "    loss       = optimizer.step(closure)\n",
    "    master_bar.lwrite('Epoch: %.3i/loss_t \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accu_train,accu_valid,time.time() - start_time),end='\\r')\n",
    "        \n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=0.001)\n",
    "#loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "#loss_fn   = torch.nn.MSELoss()\n",
    "loss_fn   = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     14
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log at log/test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='300', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/300 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='464' class='' max='540', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      85.93% [464/540 02:16<00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 \t Loss: 0.6938 \t Acct: 0.5550 \t Accu: 0.5073 \t Time: 0.06 ss\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-50d8e30af2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mbinary\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mlogit\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;31m#loss       = loss_fn(logit ,label.squeeze())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/models/two_dim_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data, only_return_input)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0moperands\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0moperands\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/models/tensornetwork_base.py\u001b[0m in \u001b[0;36meinsum_engine\u001b[0;34m(self, optimize, *operands)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# for now, we will compute path at outside since we need build the contraction map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# in __init__ phase. So build map at that time make sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'path_record'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_contracting_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/contract.py\u001b[0m in \u001b[0;36mcontract\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mContractExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meinsum_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_core_contract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meinsum_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;31m# Do the contraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mnew_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinsum_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtmp_operands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meinsum_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;31m# Append new items and dereference what we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/sharing.py\u001b[0m in \u001b[0;36mcached_einsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached_einsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrently_sharing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# hash modulo commutativity by computing a canonical ordering and names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_einsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0meinsum_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_valid_einsum_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinsum_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinsum_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/backends/torch.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# rename symbols to support PyTorch 0.4.1 and earlier,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# which allow only symbols a-z.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mequation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_valid_einsum_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_torch_and_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/parser.py\u001b[0m in \u001b[0;36mconvert_to_valid_einsum_chars\u001b[0;34m(einsum_str)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinsum_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',->'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mreplacer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplacer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meinsum_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MachineLearning/Tensor_Network_Project/opt_einsum/parser.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meinsum_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',->'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mreplacer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplacer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meinsum_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########## Normal train ##############\n",
    "from train_base import *\n",
    "logsys            = LoggingSystem(True,\"log/test\")\n",
    "metric_list       = ['loss','g_norm']\n",
    "losses=[]\n",
    "accues=[]\n",
    "\n",
    "metric_dict       = logsys.initial_metric_dict(metric_list)\n",
    "master_bar        = logsys.create_master_bar(300)\n",
    "#master_bar.set_multiply_graph(figsize=(9,3),engine=[['plot','plot']],labels=[metric_list])\n",
    "accu = loss = accuv = accut = -1\n",
    "losses = []\n",
    "accuvs = []\n",
    "accuts = []\n",
    "for epoch in master_bar:\n",
    "    infiniter = DataSimfetcher(train_loader, device=device)\n",
    "    inter_b   = logsys.create_progress_bar(len(train_loader))\n",
    "    while inter_b.update_step():\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        label,image= infiniter.next()\n",
    "        bs,c,w,h = image.shape\n",
    "        optimizer.zero_grad()\n",
    "        binary     = preprocess_images(image)\n",
    "        logits     = model(binary).squeeze()\n",
    "        if isinstance(loss_fn,torch.nn.BCEWithLogitsLoss):label=label.float()\n",
    "        loss       = loss_fn(logits,label.squeeze())\n",
    "        loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        if torch.isnan(loss):raise \n",
    "        optimizer.step()\n",
    "        #g_norm=[(p.grad.norm()/p.norm()).item() for name,p in model.named_parameters() ] \n",
    "        losses.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            if len(logits.shape)==2:\n",
    "                pred_labels  = torch.argmax(logits,-1)\n",
    "                accu =  torch.sum(pred_labels == label)/len(pred_labels)\n",
    "            elif len(logits.shape)==1:\n",
    "                logits = torch.sigmoid(logits)\n",
    "                accu =  torch.sum(torch.round(logits) == label)/len(label)\n",
    "            else:raise NotImplementedError\n",
    "\n",
    "        loss = loss.item()\n",
    "        accut= accu.item()\n",
    "        losses.append(loss)\n",
    "        accuts.append(accut)\n",
    "        master_bar.lwrite('Epoch: %.3i \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accut,accuv,time.time() - start_time),end='\\r')\n",
    "\n",
    "        #master_bar.update_graph_multiply([[losses[-100:],\n",
    "        #                                  #g_norm,\n",
    "        #                                    accues[-100:]\n",
    "        #                                    stdes[-100:],meanes[-100:]\n",
    "        #                                      ]])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        data_loader = test_loader\n",
    "        model.eval()\n",
    "        prefetcher = DataSimfetcher(data_loader, device=device)\n",
    "        #inter_c    = logsys.create_progress_bar(len(data_loader))\n",
    "        labels     = []\n",
    "        logits     = []\n",
    "        with torch.no_grad():\n",
    "            for label,image,_ in data_loader:\n",
    "                #image,label,_=  prefetcher.next()\n",
    "                label,image = prefetcher.next()[:2]\n",
    "                binary     = preprocess_images(image)\n",
    "                logit      = model(binary).squeeze()\n",
    "                #loss       = loss_fn(logit ,label.squeeze())\n",
    "                labels.append(label)\n",
    "                logits.append(logit)\n",
    "        labels  = torch.cat(labels)\n",
    "        logits  = torch.cat(logits)\n",
    "        if len(logits.shape)==2:\n",
    "            pred_labels  = torch.argmax(logits,-1)\n",
    "            accu =  torch.sum(pred_labels == labels)/len(pred_labels)\n",
    "        elif len(logits.shape)==1:\n",
    "            accu =  torch.sum(torch.round(torch.sigmoid(logits)) == labels)/len(labels)\n",
    "        else:raise NotImplementedError\n",
    "        accuv = accu.item()\n",
    "        accuvs.append(accuv)\n",
    "        master_bar.lwrite('Epoch: %.3i \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accut,accuv,time.time() - start_time),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9777496d30>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABFqklEQVR4nO2dd5gW1fXHv4dld2lLXxBpCwICgiCsqAiKIopoAGssseSnkmjURJMYjLHHaJomGmNiUGNHYwsIgggSsSEgvS9FiiBL7yy7e35/vDO788475c68M2+b83meffZ978zcufPOzD33nnPuOcTMEARBEKJHnXQ3QBAEQUgPIgAEQRAiiggAQRCEiCICQBAEIaKIABAEQYgoIgAEQRAiipIAIKLhRLSSiMqIaKzF9uuJqJyIFmh/N2rlfYnoCyJaSkSLiOj7hmM6EdFsrc43iKgguMsSBEEQ3CC3dQBElAdgFYBhADYBmAPgSmZeZtjnegClzHyr6dhuAJiZVxPRsQDmAejBzLuJ6E0A7zDzeCL6B4CFzPxMgNcmCIIgOKAyAxgAoIyZ1zJzBYDxAEapVM7Mq5h5tfb5WwDbABQTEQE4G8Bb2q4vAhjtse2CIAhCEtRV2KctgI2G75sAnGKx3yVEdAZis4U7mNl4DIhoAIACAGsAtACwm5krDXW2dWtIy5YtuaSkRKHJgiAIgs68efO2M3OxuVxFAKgwEcDrzHyEiH6E2Ij+bH0jEbUB8DKA65i5OjYBUIOIxgAYAwAdOnTA3LlzA2qyIAhCNCCib6zKVVRAmwG0N3xvp5XVwMw7mPmI9nUcgP6GEzcGMAnAPcz8pVa8A0BTItIFUEKdhrqfZeZSZi4tLk4QYIIgCIJPVATAHABdNa+dAgBXAJhg3EEb4euMBLBcKy8A8C6Al5hZ1/eDY5bnjwFcqhVdB+C/fi9CEARB8I6rAND09LcCmIpYx/4mMy8looeIaKS22+2aq+dCALcDuF4rvxzAGQCuN7iI9tW2/QrAnURUhphN4LmgLkoQBEFwx9UNNJMoLS1lsQEIgiB4g4jmMXOpuVxWAguCIEQUEQCCIAgRRQSAIAhCRBEBkAF8sqocG3ceTHczBEGIGEEtBBOS4NrnvwIRsO7RC9LdFEEQIoTMADKELHLGEgQhRxABIAiCEFFEAAiCIEQUEQCCIAgRRQSAIAhCRBEBIAiCEFFEAAiCIEQUEQBJsufgUew8UJHuZgiCIHhGFoIlSZ+HPgQArH9MFnEJgpBdyAxAEAQhoogAEARBiCgiAARBECKKCABBEISIIgJAEAQhoigJACIaTkQriaiMiMZabL+eiMoNid9vNGybQkS7ieh90zH/JqJ1FsniBUEQhBTgKgCIKA/A0wDOB9ATwJVE1NNi1zeYua/2N85Q/kcA19hU/0vDMQs8tj1r2bjzIAY+Oh2bdx9Kd1MEQYgwKjOAAQDKmHktM1cAGA9glOoJmHk6gH0+25eTjJ+zAd/uOYx35m1Kd1MEQYgwKgKgLYCNhu+btDIzlxDRIiJ6i4jaK57/Ee2YJ4ioUPGYjGbW6nKUjJ2EDTviUzweqaxCydhJeP7TdSAQACDIHDCvfPkNSsZOwsGKygBrFQRBlSemrULJ2EnpboYngjICTwRQwswnApgG4EWFY+4G0B3AyQCaA/iV1U5ENIaI5hLR3PLy8oCa649DFVU4fLTKcZ93vt4MAJizfmdc+Z5DRwEAf5+5BkTBt+2ZmWsAADv2S1gKQUgHf52+Ot1N8IyKANgMwDiib6eV1cDMO5j5iPZ1HID+bpUy8xaOcQTAC4ipmqz2e5aZS5m5tLi4WKG54dHjvik4+ZGPHPfR+/aE0b3FcD/INJCsVRaGcBEEQR3OovyuKgJgDoCuRNSJiAoAXAFggnEHImpj+DoSwHK3SvVjiIgAjAawRLHNaWXfYRcVi00HrD8SRLa7JIVefx2RAIKQVqoV+//12w9g98H0zthdg8ExcyUR3QpgKoA8AM8z81IiegjAXGaeAOB2IhoJoBLATgDX68cT0SzEVD2NiGgTgBuYeSqAV4moGLH+cAGAHwd6ZWnGPArQvxq7Zw7QClCtnUAEgCCkl6pqRl4d9/dwyJ9momWjQsz9zTkpaJU1StFAmXkygMmmsvsMn+9GTKdvdexgm/Kz1ZuZPbgZeIlgq6cZ/9UGHNeqEU4uaZ6wbd/ho5i1ejtG9G5jcWTtqEP6f0FIL1WqUwAA2/cfSSj7eMU2nNC2MVoV1QuyWZbISuCAIRsjgD7aJ8McwKwqHPvOYlz2jy8s673rrUW45dWvseo78agVhEymsrra97HMjB/+ew6+/88vA2yRPSIAAqamUyfrciLg4JGYHcHLg7JpV2zR2KEKZy8kQRDSS2VV8qrdddsPBNASd0QABEztSN9cXsu4T9cBAN6b/63n+u1UPDUTj+xxQBCEnORoUjOAABuigAiAkCBTT13jpmkoO1Kp/qC4GYxF9y8ImUEQM4BUIQIgRdSqgAjXndYRAHDFyaoLpmshGyfSWuNz9jx8gpCLJCMAUv32igBIAwV1Yz/7R8u/C2zRiD4D8OCAIAhCCGw/kOjZk6mIAAgalw6YCNi2L/aArNi6T1nn57afPi+oFgkgCKHBzK6DtgUbdqemMQEgAiAkEozABi+gZNT1Zl1/ydhJuPONBTXfq8UKLAih0enuyRj9988d90nWDTSViABIEbpu/uCRqjgDsertdnou3pm/uaZOfQJwtKoa30q+AUEInIUbdztuPypG4OhijPkTV65t2HGgApUGNU1QEl9PLvNp2XYAwAMTlmLgYzOw5+DRQOoXBEGNCg/efWaMvUEqQruLAAgIvSO3i8ppvLF5hm2HHMJLb91zGMu+3QsAWLZlr1I7Vm6N7TdzZSx09r4jIgD8snn3IVl5LXjmaJU3AWAMMW8cDx456l+QqCICICA+XPad8r7vLahdAPb0x2ts9zv10ekY8eQsT+0QE0BwnP7YDJz7xCfpboaQZbRr1sDT/g9OXBZSS9wRAWDBm3M2YodFkCad8n1H8J+5G+PKXvnyG8c67VQ98zfs8tQ2tw5e+v/s58OlW1G2bX+6myH4pGWjAk/7v/7VhprPqV7HIwLAxDc7DuCutxfh5le/tt3nxpfm4pdvLcLWPYdrymatjune7W6fbbnH++32gAzu0jKp+oX0M+bleTjn8f+luxmCT45tWt/3sYs37QmwJe6IADCh6+++WrcTTxpSvE1evKXms+4F4OTuZV6xa9cRL7DxKLAzJM1avR0vf/kNXvhsHb5cuyNhe+sm6iFk52/YhQcnLsUDE5ai0qPeMpeoqmY8NHFZnEAXBDcWbNxdk4rVSL38PADAlj2H8NDEZZbhoe00AsbwMKkI76KUDyCqPD5tFW4f2hUAcIvDjECN2ht+UoemmK8tFqmw6XgnLbYOFPfHqSuTbEctFxn8mU/v0hLDerYOrO5sYvbaHXj+s3VYvW0fXr7hlHQ3R8gSRj/9GQDg5iHHWW7/5X8W4dOy7TinZysMPC5+Zl5uo2JOdUwvmQGY0L1ndJgZK7dae4JYJWD/7wLrjtso8Nu6TBH/9cla+F1LYh5YDP3z/7Bx50HX47wkscgWNu48iLP+NBPf7XUe2euXLovohGBg/PzNhTUu2Zbxu2weNbtYX2EhAsDEbyfFpzNmRpwqyIhRLWTGyQ3UjUcmu6ZUVqaiqhovuxiogdzs/F7+8hus234A783fnO6mCBGCGXj76001361G9XZvm0ImyUARAeBCNbOt4XXq0q1x30vGTrKt5/1F9sIibHYdqEDJ2EmY5NCGXJoBnP7YDNz2+vyauEiqeZJzUAYKacD8GF3x7JfKi8OMUQKmLNmKkrGTHD0Sk0VJABDRcCJaSURlRDTWYvv1RFRORAu0vxsN26YQ0W4iet90TCcimq3V+QYRefOdShFOfcL6He6qlf1HKlFdzRg3a61SncmTWPtqzaXwxc/Xh3rmTGHz7kOYuPBbz3mSVffbd1gW1wnxHDjivGrXvN1usGGcATynJY4K0yXYVQAQUR6ApwGcD6AngCuJqKfFrm8wc1/tb5yh/I8ArrHY//cAnmDmLgB2AbjBc+tTQDWz75HhnoNH0ev+qXjio1VJteFIZXJpIO1WJ+c6ulrLbQZgla/ZjqXf7kHvBz7EfxeEo1baK8IlKznh/qk1n636C9V3b4XB3qgP3PJC1AupzAAGAChj5rXMXAFgPIBRqidg5ukA4qyoFJvnnA3gLa3oRQCjVesMkwElzeO+M/vTjxMRdh6MGYknLow3DKvczhVba0M/eMocZtFUvcipI8w24bBk8x7XOEq1AsC5LvYwU9BDc/xvVbnLnv7YfUAEQLZj5R5OIGzdcxjb9sUcEuzUylYOJ3XSLADaAjAue92klZm5hIgWEdFbROSW6qoFgN3MrM+L7OoEEY0horlENLe8PJyXzshX63fGfQ/COMqI75hbN3b31f/XrHW+zqVPG43UdISmu51tnb7O1KVbceFTn+Ltr51H4fpvHuQLVBPsLyRvjWRne0L6ueDJTy3LT310OgY8Mt3xWKvRvqoNyw9BGYEnAihh5hMBTENsRB8IzPwsM5cyc2lxcXFQ1Spz4EhV4MbBlo0Kg63QwBfa4rDNhlDQ+lID84OUif3/lCVbXfWpa8pjU+PV25wDtVXXqL6Su9K4gHAe7QpeySFbfEr4YPEWX1EzmRlPTl+N2RaLKXU+XrktzgD7uebWGSZWAiDM91RFAGwGYBzRt9PKamDmHcys/1LjAPR3qXMHgKZEpC9ES6gzU7j7nUW+jLZk+myc8h0KMczrbofwz+aOMNmOMWhWfbcPP35lHn719qJA6lNWAbnUYxUQLqxfLsNuSUazeNMe3Pzq17j3vaWej528eCsen7YK33/2S8vthyqq8MMX5uD6F+bUlF01bnbNZ+Mq/E273J1BzA+MnaBv3Ti8waEVKgJgDoCumtdOAYArAEww7kBEbQxfRwJwdGTnmPL2YwCXakXXAfivaqNTycadh5KeAazfcRCHDaFdV9gsLLNj297k3MCWa6GkzX1LpvU1+w7HBONmm0Q2U5ZscRyxmdEX5eURYd43uxzdYFXYvPsQ7tKEk95RMzOembmmRrcrpA7dG8trQEUANfY5O/QV+uu3H7DcvvdQ7UDrUIV3tZ2d/aphYWJwhjAnha4CQNPT3wpgKmId+5vMvJSIHiKikdputxPRUiJaCOB2ANfrxxPRLAD/ATCUiDYR0Xnapl8BuJOIyhCzCTwX1EUFSRUzgr4FXkJHA8Atr84L5LzmuENGlVCqVyBawS5eOz9+5eu4EZtbmw9qL2YdIlzyzOf4yWvW4TxUk/Lc+OLchLLlW/bh91NW4LbX5ivVIQRHmaYKXGvTSTvics/dPOeMR1cpPD92CaLMdGrR0LWuIFGyATDzZGbuxszHMfMjWtl9zDxB+3w3M5/AzH2Y+SxmXmE4djAzFzNzfWZux8xTtfK1zDyAmbsw82UGFVJGUV3tzw30ttfnY5tLCAJV9hwKxjMkIWF8Cvv8w0ercMur8xyny3qmNLeIiF7vh/Hlu+XVeXEJOOL3c/5BlhuS8ujCR19Ad8BGrfevT9birXmbLLcJyeEW4sOJvYed1bBeHAhUwraYa9lgE57FbvYbFrIS2MSATvFuoLGVwP4Y+87i5BuEAFfpkuPXUJm+fBsmL96K3zmEudAFnV2APL8YZxSTF29NiPfkB9UR3SOTl+MX/1moXm8SbYoaTsmU3HhjzkbH7aprSIz7euFqgz3ByN0B9RmqiAAw0cikg1u/46DvvL3r/ExNLagMSADsM416jM92qhJRpCPcgtmz4sevzMOUJVtQMnYSSsZO8nXl47UOxK5/OHy0Cic+ULs4aOm3tbOaD5duxai/fWoZgluMwOoU1PXWfU1ZsgUnPjAVh49WJbznZvQO2m4CYPQ8UnmmVR0uUh2TSwSAAun2zKuqCqcFqdT7qzz/XluTTIiH301ekbifx/MbMb+33+09HKdmMGaM+817S7Bw0x4c8GE8FGo54djGnvZ/+P3l2Hu4EuX7juDifpbLjmrQHTXsOu515bWDu7A7bb8DUBVEAJiYsWJbQlm6g4SpGJn8YHy2UyUM0vFb2iXdsUOfGQTJ61/VqhycvEbCXPSTa6j8Uk9OX42SsZMSFtip/s7l+45g1wFnjyEVAeDWid/z7mKUjJ2U8nUgIgAUSHd8Fq8qILcOr3zfERyprMqKzmbPwaPY77IwzI3PyxJdR+NUXgG8dMYqvt1t7Tq8ZU+8gc/q569rXq6dIzAzvk2xgRMAXvgstjL+wBH/sy0rw6xx9haE4fbV2Rtsty3fsi80N+PcfNoCRs/elS68pmvUMxXZcfIjH+HmV77OCoNjn4c+xMm//SipOlSDafmRh+ZjVmzdi4GPzcC/LSKvnvboDNfOItVJwVPFS198g4GPzYizhaQCXYVjHqUn+ysbQ67cGrIL8K/fXewaQsIvIgCygDCmhVaqrjDx2rfuMaxoPmRy29Rz96rWab28vrbMrw73M0NoAH2Kry8cssrXDMDVNVjlXjMzPivbHqpuOGj04HnfKIRQV0XFsKrvYf6pvPx2QUyUM/VOiQCIMunwAnI4z6ZdtaPjq5+zXqIPQCnDmREr1YOxHWNeji20m7my3FPHcPW42fhgcXxSIP1wpwQgTmdQOf/ERVtw9bjZeO0re7VBpqEPONaWBxfbfr+LLz9gWLFt+tV3uawEjqsjK+bK/hABEDGMi6Di4xX5f8hXbt3nGh65NnSC/T5GXf+SzXvtdzTw6ertcQu0rLBbpKVjXGcxdam3VdobtYVtR01quqM+kzqrzAA2a4LSbjFRJuO2AMsNZsZrszfgwJHKhN/cmhoJEMfBFHtgfbcnM0OFODvDCjnHz20WJSUzzT3vL7Fgaesfu8B/JT75wXOzXc8dM6yqdcg/fsVb2A093tAazS0w+XDRCh4lGatQcCdZdcoXa3bg1+8uVo7/Q9b9vyeCUAEZA8mV78ucoAcyA9DYtvdw4K5/mcgaQ3o5ow7VamT+95llNQlQkqV8f2zK/eGy7zDvm52W+yQT4sEJKxvwxp3heKS4JZeZvHhLzUzHahcvv0EQqom/zVhtmYQkLJL1PNNH7jsOVNgagcbNWouFmiecfu837z7kaoBf9u1e/H1mWUJ5EBm5jJ3+yY8k59QQJCIANAb8Lhwre6ZhjETq9i7+YcpKfO9v1sktvHLve0tqPl/yzBeW+4S1oCbMlHpmatNLWuOW6EfNCOyxUTYcqazCnz5chUue+TyYCh1oVRQLc3xS+6aB1Wn3G/920nKM0jzhdCH50/HxnjpWwvOCp2bhD1NWBta+bEAEQIRxyh2gk2wcoqemr3aN/zPir7NQtm2/Z6+WGStq7Q7jZq2NEzJG6ual7jHXL2G9T28XL+qdoJZxOBmsg+L4Y4oAAPXy8wKpb8aKbTVqNwBYtGk3SsZOwtcm1dBWzetqlyHV5oVPfRrnV3/1uC8dgz7mrglYBIAQMn+etgrPfrLWdvuMFd9h2Za9eGrGas96WqPx97eTltt6B/Xr0NRjzanB6nq92I6D6phSYVMIatZiJ/Qu/vvncf/NGB0M9hw6ivcNuSE+K9sReADCbEEEgBAYv3nPeyTDv3y0GoCWNS3BU6P2pfUSUdNMKlVAazy6Oe45eBQlYydh8uJYh6TSGQfl/69iQ3hi2ioc9+vJgZwPCC/YndUv4sWmV7eO0R4WX9vew0dz1j4oAkCwxE8n88qX6n7putpBXxhERAmd33eGTGiqMfWt2p3K1Jcfe1xgpyc10WdJnozAAV2W0zn/On11IOHIw55lJCvknRwi1pYHE9U3ExEBoNGiYUG6m5BW3pzrHB89aLr95oO471YzAC+LdXSsOrPiRqnLs7rQJZmNEWb1vAJhkEp30s+0eEwLkgyrMme9tftnsrLQePw8kx3BV8axLEEEgMaovs7hYXMd80KulEcZIGCLabHMzv0+BIBFWdfWjXw2KvWodMq6ulrlHu0+WJEQhM5MUPkmVFCJzLq2fD8OH61C+b4jCUHQvlpnHWIj2SvYb1A3zvsmXgAYXadzDSUBQETDiWglEZUR0ViL7dcTUTkRLdD+bjRsu46IVmt/1xnKZ2p16se0CuaS/JFCNbFgAYHw7vzNSddjfnmB9IfztoUNsWq0/yp98edrYjGI/j7TPSPWqY9Ox2mPzrA+fQb+LgcrKnH2n/+HO99cgJMf+SghCFpg2fFMjHyq1t3Z/Lu0b94glHNmAq4CgIjyADwN4HwAPQFcSUQ9LXZ9g5n7an/jtGObA7gfwCkABgC4n4iaGY652nBMaqOTCcos2Rx+BEcrfbYx4qIqGy3CI0xdutViz+DxmqOWwbW6Z63XsbJhTFmyNS6Eh5fotIePZpZ3i1to76OVsev/YEntPSvbtr8miqhtbowk5YLRbdccfbdNk3rJVZ7BqMwABgAo05K4VwAYD2CUYv3nAZjGzDuZeReAaQCG+2tquGTgYCitGH+PC5+yXwxm1eH6wWoC9oVNRE0nrDqIWau3W+wZPOc+8Ymn/T+xaJd5gPvVup348Svz8NgHtRnMstllcfY661XgOrrKx3gbz3n8f7jgydgzmIpLf+KjVeGfJENQEQBtARgthJu0MjOXENEiInqLiNorHvuCpv65l1LpqmFBJk6Hs4GJi771faxx0c6KgMIR6O6U6UBPaq/Kbksjt3XUys27D2Hxpj2YuND/7+3Gtn2HMW7W2ppZiFvo6jA44rAo7Z2vN7kG/gsCsxDO5b4hqGBwEwG8zsxHiOhHAF4EcLbLMVcz82YiKgLwNoBrALxk3omIxgAYAwAdOnQIqLmJ6HpVIYaqG2gyS+eNi3YWB6RmmrnSOSppJmEc8djZAKq1gjqEwMJy6JhVVj959WvMWb8LQ44vRpdWRTVhslOJ0zDwzjft14KEOSvK5uB7bqjMADYDaG/43k4rq4GZdzCz7rQ9DkB/t2OZWf+/D8BriKmaEmDmZ5m5lJlLi4uLFZrrj6BGoLnKzgMVuP31+TU63PkbduHh95eluVW5Q1U145f/WYh1JpfDHVo+2kMOuvwnpq1yDcdthVnY6C6WuleQyozm4xXb8NT02GK+Z2auwYdJ2lv8BrgrrBueQ2PUZwBzAHQlok6Idd5XALjKuAMRtWFmfe49EoAe/GUqgN8ZDL/nAribiOoCaMrM24koH8CFADInRF4aKCqsi31J5r4NEvMz/+T01Ziw8Fv0ad8UNwzqhItsltwL/lj67V4s/XYv/mNa8Pb0x7HolJ84dPB/1Tpgr+G4m7usfVEJzvfDf88BANw2tCt+P2WFbTvy6pCSB0/QKZGJku/AU52o3Q5mDnxRo+vPzcyVAG5FrDNfDuBNZl5KRA8R0Uhtt9uJaCkRLQRwO4DrtWN3AngYMSEyB8BDWlkhgKlEtAjAAsQEy7+CvLBM57jihnHfL+7XFi0bZe5itCzIHx8q/R6elvJzloyd5Nmu4IW9hrpvfHFuwvYgXS4b11PTNvudAdg9n0GM3t8LwD05CMKYiSjdFWaeDGCyqew+w+e7Adxtc+zzAJ43lR1ArZookpgl+QdLtmK7j4VPYWHrbadtCGJklU3sPJCee+OUuWp032OTqtu4ZuKj5YmZ0IK8v6qLzfwONMJ8Fld+lxnq4Wpm1Ak4NqmsBE4TZiPrtgzKEmSFeWQWpc4/LBjJBXZ7b0E4HkF6k6wSqNz11kJfgdFULzMot+JcJIxXTgRAmsi2DtSPJ8SBDLJpCLFFWMaFWHb3tJo5Id+uLqjenKsWlM+MWdAxMxZbxE2ymomoEAUVZRgJk0QApIlvXeKzpBtz5/DSF9ax9p1Ih948m0h1n9Xr/qnodf9U1/2qqxNdLpPtew6YVFnXPPcVvve3TzFuVm2uiI07D+L1r1IblDCbCGPQKAIgTWTaEn03/BgEnRb1CJlLNXPCgrNXZ38TaDLzT8ti624mGhKzmIMBCvGIABBSRrapqLKVdP7Mdve4YWFi2sZ7/7s0o5KZm4nC8xqGCiiolcBCkhTk1Ul7jJeqasb4ORtQUVlt630ShRctKth55hTWDSZvrwoRUN37ouyR89HlnvicGSIAhMAxunO+OXcj7nnXOrG6kN1YeRu9+Pn6wOrfczC89QoqDOvZOi7Pb7ZTNy9ROTN/w26c0S3YaAiiAsoU0jQUyjO4T+xWeIkZjH2H3ff7II1B2aLO3sNH8dv3l+FIZe0szsqGsyvAtQ3GhCpeCCoPR+fi7En645eDPn9jJ2QGEHGObVofGzz4XjMDT0xb7brfza9+nUyzIsFxrcLptB7/cBX+/fn6uE5x4abdysf70TTMXOkvnUd8Lt4kVBwR0E2GoSGWGUAWMahLy8DrNHb+qr7+bikGBTXy8+qE0m/pPvzG3AhWic3t4sr4WfNxxKdXW9Qz8bVvXl95X1kHkEM0bZAf913lPbh9aFcM7hq8ENBReb4Y8dmaBP+kctCqB4xTwU+7erRp7P0gxK8wT3NKkLRQx+GafzW8e9x3EQARp2FhHsae3919RyEruPyfX2DDzsSReTJs3n0Ir87ekFC+aVfirM1O5eKnm7GaNYx6+jP8dPz8hPKfGcpUksSrkCkRO71S3KjQdtvNQ46L+y4CIIfwO9YJc9SoooOV8A7BEnQCmy/WqKfRtFuo50cXb2VkXrhxN/6rxSsqKqw1NxpjGAXl+mw0eGcTY87orLxvtdgAwmHH/tQHYttl8rhRnf02qZ/vvpNPVN77p2aUhXb+KPLfgAO6qQ4sDh+tqkk245ephuQvBRZui15Jxgj8r1nrkj5/Oqibpz4UlBmATyoqqxMyLRnJhCXoqve2ffMGePeWgQk2BCGey0vbpbsJKWHTrngPLtUu4pBDmGnVOj5eUev5k+fTmpvvoQPMRbzkP2hZZK8u8kskBMBv3luMs/4009bvORNsT6crePjoQuKkDs1CTYGXDNUZoozt2qoo3U1ICYN+/3HgdfoZaF76jy+c67QpP1pVuyWKRmAvHlddQ3AbzsxeJGA+XR0LPHXAYiHFzgMV+GrdTqV6/vL9vlh437lYeN+5tvsU+5TSV5+SmPD+ozvPjPtufD/ML+mMn8fv64cgum4v/uZhEsG+JEDcnwTVd6amRgWpktQ6gAyhSDHzmR/aNWsQeJ2REAC6P3Rdi4SjVzz7BR6cqJbcvLioEE0a5KOJg/rlF+d289VGK3ewLh4kftMGyaeTDOL9y5RcwVEcTQaFynNw+T+dR/wJdfpsixAukRAA3+2NGXmtEk6v+m6/cj0qfcpZ3Vsp1xdfOdCoUH30YH6hguju/CwAsmK6z6QeQRLV7p9sPpv5xydrbLeF0VnnwOBeiWy7TiUBQETDiWglEZUR0ViL7dcTUTkRLdD+bjRsu46IVmt/1xnK+xPRYq3OJykFQ7aNO5NbwRqmXpkAXOZiuDQajAaUNI/fZvPr9W7bJNmmeeYGiwTjqYYoPdeeDRw+WoV//m+t7fYwOrGgBheZzrCerT3t379Dc/edQsRVABBRHoCnAZwPoCeAK4mop8WubzBzX+1vnHZscwD3AzgFwAAA9xNRM23/ZwDcBKCr9jc82Ytx45JnPo9LieeVBgXhhcklIpS0aKi8f+P65pXE1hJg4m2DlOt8zWIBUbZCAP542YnpbkZaCTvPtJNbonmBl4pQyQW1nVcboJM6ORWozAAGAChj5rXMXAFgPIBRivWfB2AaM+9k5l0ApgEYTkRtADRm5i85Zvl5CcBo7833TlAhcK89rWPc9z9d1gd/vPTEhPnzjxQXehCAy0vb45J+7XD1KR3w9s0DXY6IP1Hj+vbqo/u/1xP/vKa/axsyPTG9F4gI3SLiCWTHkx7CPxhRHa07OXyNfvqzuO9XDkh0ckg4b7bpTyww51LOdFSUzm0BGBN1bkJsRG/mEiI6A8AqAHcw80abY9tqf5ssykMnqEGG2UB7af+Y+mbb3vg1BXeP6IFdBytck2nXIUL9gjz8+fI+tvsYF400LIi/dU6jpx+e3snx3LkIEVAnYpHGZqz4Tun57n7vFMftqv3wW/PUE8TXV5g9//KtRcr1ZSqZ4gatSlBG4IkASpj5RMRG+S8GVC+IaAwRzSWiueXlyS+bdwq+5IZxymu80e8b1CxWt//eC600ZvHYNeucHrVGZaMf8J3ndsNdw4/H+7cNwlNXnmR5bN2IdYBR54EJat5sboRiA1Co00tY8kylKstmMSoCYDOA9obv7bSyGph5BzPr+oNxAPq7HLtZ+2xbp6HuZ5m5lJlLi4uTz4ZDAN6etwklYyd5DgFhFO5GHXwvg7HR6v4X1XPX89mFeOh+TG2UReMov0FBXdwypAt6tW2C7/U51vLYts3UQ83mGn6S2Gc7QXWgI56cFUg9RpzUSqc9Oj3w86WLbHvuVATAHABdiagTERUAuALABOMOmk5fZySA5drnqQDOJaJmmvH3XABTmXkLgL1EdKrm/XMtgP8meS1KEAGvzP4GALB+h7cXxjjq1z1MvMTztuOu4cfHCREjW/f6D1MR5fF/tr2IQZFlA1AAmRGKJSicnru2TTNvQOYqAJi5EsCtiHXmywG8ycxLieghIhqp7XY7ES0looUAbgdwvXbsTgAPIyZE5gB4SCsDgFsQmy2UAVgDID4Dcki8NntDzUviVRtUYAi/oN9mcwLtBoXWuk4n74AfnXGc7baodmTJYpfwXEgTOXQ7jm9t71xQVQ2caZO3t02TemjpEP45HSjZAJh5MjN3Y+bjmPkRrew+Zp6gfb6bmU9g5j7MfBYzrzAc+zwzd9H+XjCUz2XmXlqdt3KKXADW7ziIpd/uAeBthHxB7zZoaLFQy1xHY4O6Z9ZdZ9V8/ujOM/HF3Wd7aSoAEQB+qecjVlLnYnU33EwlE3XQ877Z6focz1nvLbREOnFyL66qrsZ5Jxxju316ACFbgiQSK4HNGANQOXFJv1ozhTlWj57IYfRJ9s5L7ZvXxu5oUj8fbZpYTwGdBFEyL/SATuldZJJOdGF91vHqdiOnkV228MepK9PdhAQueeYLPPeZc7jmy1yCyWUK/Ts2S5j1G6liZ3tHmOHc/RBJAaDKHy+tlfQDTdE6mzUswIqHh+OWIfbqGyu8ZvSqUhRWVvTv2Mx9pxxFdzv8q42HVC7SoXkDlGfoWo4MnJj44qkrT3LMY1xdzUlnJ7vi5PbuOwVEpAWA241y8yOvl59n6X9fkFfHNijcySWJnbKTLSIZXXaUtUd6h+PFFTYZF2E7WjZKPkifKuL1Gywntkt0zKhD5Ljm5khlNZZpKma/pHLgFmkBEJZlatUj5+PWs7tabmvduJ6nupLJApQro65kCKNT90Jxkbf7LWQGfdo1wYRbE8OoEDkP2A4cqcT+I9bJdlQfRa99RDJEXACkHqvRg9OI4tcj/CeBDyOFXCpIl7uc36xWTvzuol6B1ymklw7N7ePy//6SE5MOaZHK9zbSAiAd/aPXPiaZEWR2dv/BqE30kb9ZtnZu2RB329hhwpgsnNShWeRsMY9+sNx9pyyFAOTn1bFMyTrtjjPQoUWDpIPapfK9jbQASAd5Hh+OZEYTQXjWpsOTyE595oXzToiF5TVGSb19aFe88MOTbW0jVnemV9vGcd9/Nbw7rrLI3uZELgQ584JTqOmswe491Yrf+vFpCXY+PT5YzzaNzUd5IpXPS6QFwGyPae2CwOvoQPefbuYjbGwQz5GK21rbpvXRrXVw+Uq9JMaxo25e7NE2/tx3DuuGji0aegrrfYxJHzusZyuM7puSuIWe8bqyXfCOPqDo0qooYaDi9m6rJoCvTmFA0UgLgBdcfJPDwKsKSF8H4Ec/HYQuUfWsXjKrueHlUn872lnHblXV4K4tLUqtMc+AurQqivtdhypkgDPPOG49q4vrMX4WDUaJMYph1tNBsu+d2ABSxPb9FSk/p9ErxcrNzExhXmy02rd9U8/nMi8869jCe1LpZgq5hpNReVp1oF6SZPzg1I4JZad2ru20rUZldq+X1b55FnlE8/Nqy567/mTXNpqn9CqJw4OYBeUy7UMOdNjjGOtFgZUKw/NkE0e1MISL6KGpk9qFdL2RFgBOGMM4BIlRALx8wymYaOFqZqRJg3y895PT8aTHBU2v3XRKjR5cxy5stBPHtXIPj3D4qP85q/m63r75NEcvC51zerTC/345xHU/L7LJ2FFfcGIsvmFDi5e5oU28JzvMMwAV19RcyI4VJlaC2YjXBZdmHhh5gmX5oQprF08jegc+tHsrfPDTwXjm6n6ezm10Ghh/06n446UnuvYTfhEBYEN7hU7ID2T4xZvUz0dvhVlA3/ZN0aDA24hw4HEtEzoRP6PKbgrhEbZ7DKttxBxfqX/H5kq60jZN6qOjhxSaRlSMbEVau4KYjJun9Cp9uyzqcibPpee6uF9ydpp6+dZC3sviynoFeejRpjGaNdRm0T7uaZMG+bistH1tHQEjAsDE4gfOxeIHzg2t/nQuTPIzqgzThbGFzUOt0kyrGEnjri1Vqst46LFNrN1s9eOsZIWqMU/HT0A/r+eIGm7Psup7Nu7aUpzfyz54mxkvHjpuYVwW3hdeP6OKCAATRfXylRK4+MVPlEqvnHCstRuany4lTFWElS+1Kp0sRv9WobjdbAD5hvthlVjHKrCX15/EPAM4tXML12NU8/JGFbcOXvUWndOzNc5WMOTrtLAJ51zfMGP438pY5sIpS7c61pXuhPCACICUU9dt7hoAk24fbFkeVl/ulOvAiWRmQy2LLGYPin2msT82fh7ao9ZmogsO6xmAN8wTABU/8YgtHQgcL89WD4f7YVYl2alRlz88vOZzuUklmsn3UgRAGuh+TBHu/557nmCv/O6i3ujc0l4vHpZa4WiVPyOw7VobhWY6OWO4Xad5dH186yI8aDL66TVYTfm92ofMnk5i302eGSu+c9yeKlXrr4Z3Rx+THS+bFv6Jr1kamPKzM0Kp96pTOjiuUg3rndh98Kiv48wddYG+eEtBUAXpKz31jsT7UWMDsNjfzkBohzm4l4paLXu6kPRQUenyC3l41h0fJZfT3DzkONzsEhLefLvXP3aBWsNSgMwADPzh0sRMP+OuLcUd51iHdo4CVm6QQWF+MbwIqAtPTNTXq2J84f95TX/LfX52TjdceGKbuKRAfrncR3z3MH/3XMAtzLeXhZNe7C0qz+gobaV4qeZA0b9jM1x8Ulv84RL7TGLpQmYABi4vTXxRz+nZGuf0bG2xd/bhZwYQlBG4bdP62Lz7kGPdTtP2zsUNsbb8AIDYtLt+Eh2k3nmcXNLMVv/bslEh/naVN/9tO/y438o6AGfcFlt5yQNhngEYD/UzE9PbptvG8vPq4PHv9/VRU/gozQCIaDgRrSSiMiIa67DfJUTERFSqfS8goheIaDERLSSiIYZ9Z2p1LtD+1E3xQmDcNLhTSs5j1Z+Zi/QXr8DgmaOXdSlulFDmly6tGuGeET3wdEAdvJB6hru4blqp6fRgbW6kWvj++bI+KT2fEVcBQER5AJ4GcD6AngCuJKIECyYRFQH4KYDZhuKbAICZewMYBuDPRMalULiamftqf9v8X4aggtWDfXoX9bg4yZ07saykZbwx9YDFKsvbh3ZNON5teu82pSci3HRGZ7RKYeINP3S3CUcgxLvvJov5aUm2+/dqnjrTQ97qoFH5FQcAKGPmtcxcAWA8gFEW+z0M4PcADhvKegKYAQBaB78bQOJqHSFtnNktPQ/fv64txR8ujY18TnEIOa2/TEbDsLsPeDgjuI/uVDfeXz+wBJ/8MjGcyAs/dI8dJLjjRcWjY+edYy53ery8dO6qE4l0Og2pCIC2ADYavm/Symogon4A2jPzJNOxCwGMJKK6RNQJQH8ARkX7C5r6514SpWfomH/ggrw6KZvuntKpRdx6gWE9W9foxq30+cdrISis3o10hUno0kp9RP7AyBPQwSL43lnHi6bTjQKF0b2fgGsnl1gPNBJnALUPmBfh4Jd6+enzxUn6zJpK53EAP7fY/DxiAmMugL8A+ByAPs+/WlMNDdb+rrGpfwwRzSWiueXl5ck2VzCSgo70ruHH46M7z8AjF/XC4CTVTbpHTp0kVUBBMPvXQzHnnnMst6l0YII931Pw8DLHkHJj2h1n4MFRtWs9vr53mH3IbQcjcH4ICzmL6uXjozvPDLxeFVSuZjPiR+3ttDKdIgC9AMwkovUATgUwgYhKmbmSme/QdPyjADQFsAoAmHmz9n8fgNcQUzUlwMzPMnMpM5cWF6dPV5YLJLhdpuCc3VoVoUurIhTWzcMQmyX3VlPg7/WJRePUZw1EtaM+uylzR20R3PAT1GO7qGIOpd26cb2EFdD6jOZHNrHqveQhAKLrCaQiwL2qgLq2jj2DOs0bFtSESzc/T9+38Ab0wvHHxIzNZ3dX9x5UNVAHjYoYnQOgq6bC2QzgCgBX6RuZeQ+AmiebiGYC+AUzzyWiBgCImQ8Q0TAAlcy8jIjqAmjKzNuJKB/AhQA+CuyqBEvsdOP9OzbDvG92hX7+kX2OxZldi1GoMOX9yVld8H+DOmHGiphvgEpf2LZpfSx98Lyk47GbWfHwcCW/8kaFdfHVPUNRr27i+Vc8PNxzpxXN7l9NJ25nB/r63mEJ+RYuPkk9Mujyh4ajMMkZXJdWRVj64HmeZynpwPVKmbkSwK0ApgJYDuBNZl5KRA8R0UiXw1sB+JqIlgP4FWrVPIUAphLRIgALEBMs//J3Ce4km6MzV7BbeJWfl7qupkmDfKWVtEQUFwKbQBitvchOI+mGhXUDHznXy89TmvpXM6NBQV1LFVW9/DzPcaB+dGbmZr0KE5VV3nYCoHnDgoR7ZbQxdW7ZEBdquR5q4bh9jffPr4E2Gzp/QHEhGDNPBjDZVHafzb5DDJ/XAzjeYp8DiBmEU0KbJvWwbMveVJ0ua9BnBOkMUe2G8QXs37FZRi2jNxO05WFU37YY1bctSsaafStyG5Xo2V5W+hqf7xm/GJKwXX/G+nVomrhN+SzZiVirIoT5ldHfi1+P6JHythhReskyV0bVkGwQsDACBGYjl/Z3D79xjE0eBytuGuw8k+rVtgl6t22C+75nnQUsVdx+dhf8+EznuEJBEwkBkOtS3C96n9qrbRM0DynjkFpkT/s7lE33zsuo1IrLbIyPx5uysgXpZXRWGhch2dG6sXt4cS/eOFbuuEbq5edh4m2DfOXdDpI7zz0+6VSWXskORVWSZFN41lRi1JXvPFDh6djjihtijRabxwmVhWYVlfaxnfV759S1vnbjKVi73b0tYfPqjaf4PvanQ7vaxgwyCtGfD+uGc3q2xvl/neX7XEYy8c1wWsj3jx/0x5FK97y8ghqREACCRoBuoMVFhUoCQMXw6eT211AzBDvNUAZ2aYmBKQpp4YSXhWJm7himFnH2Ni00RlBk29jILQaQkZaNCpPKVx0FIqEC8vLQ5DIJI6skJMDtZ9d2RHZT9kGKnbKT0W9oj1Z4eNQJuPv89NopnPjgp4Px4v9ZLmMJhLDWA7xwfWaGpVC93H/8oD96t61NxvLGmFPjtr/3k4F46sqTkmpLrmsPIiEAvn+yfZKUKBHkQjCja51deIMWjdTsCpt3HbLdRkS45rSSpMI/A2p6Zb/0aNM41JhKYYW+GNilRUaqgFQZ3usYdDBkZ+toyhPdrlkDyzzPXsjm30cFUQFFmGRGlvoqSiAW92b8nI0Oezuzde9h952S4P3bBqGNotfIjJ+fGcpyf1Vm3XVWQorNsLx0CaQ0wr1pcCf8a9a6cBphYswZnX0PTML0Zr73wp44vUuL8E6QJiIxAxBimFfIJvPC6It1jmlcz3OKxFTTq20TtGikNgPoXNzIc87fIGnfvAE6F8eHBQhzncbA49zVdPdc0LMmXWfYNKmf7/vYMLQ1eqTaUzo1R/djcm9BqQiACGFcWQskpwLS3zUn9USOq09Thv4T//3q4BPY2MUtqjl3itdfMLOnc4Yd+O+aUzvi01+dhV4GW0MuIQIgwlw3sMRx+4je9sZz3XdfVyNZpVY8cKRSqR1XDhAbjSPab6yqxvKCW2TVVGM3aGjWIN8yYJoxcmgYwoCI0K5Z+maEYRMZAbD+sQvw/m2D0t2MjGH9YxfgZw7J7u+7sCf+frV6tI4zLOLzqL6Ol/aPxfg5yWIpvlA7A1AJkeCpXoW+P8hZ3Fe/Hhr3fYBFIiC7VR/z7zvXMmTy+b3b4JgMz+yWyURGAAjecAvIVZOpi+z396q7zqyxaOZQO0hPvU7t0Yt7xz4o3BzX/NKmOvIsng+VQHBCcERKAGRwzLOMw+091KfbeidvPTqVlzkIyPE3VufqU7yr2mrUcwrn/r9BzgLAPCCwCp3B7P89FdnhnWgJABlj4vHL++CX5yUEaE3AOBJ75KJecduuObVjTWekv6xWQazkhQwG/ak1/p5PfL9PYPWmCrMAsOrorR6Z31zgvAhQr0ceN+9ESwBI/4+L+7XDT87qklBuHo0ZR5tXn9IxLgzzw6N71fiP6y+1OTsWAOzwGF9IsEb/jY0++xed5B4x00xSHWQA745Z5WPpWsocd6riokLc6BLNs1ZAigjwSqQEgB2/u6g3PrzjjHQ3I6NwtQFo/536hQUbdyudK4rv7Ss3nIJx15aq7VxjZwmvPQDwi3PV4hHpTLx1EP50We1MxG2G3bh+vBuy1XoL8zU6BQqsOa+M7HwTKQFgXmGpc9UpHdCttf9AXlFCD6nAKhJAET17Ui6725kZ1LUlzumpljO2XbPYquuGhcEuuNM7zvraQr4fnu5ixDXRu10Tpdj95vPp6NdlhMFx+6mEKdfzNQcZJjsqRCoUxIEjEkZWFasY/S/fMKBGUKqEaValR5vGeObqfjgjxHg62cxvR/fC0O6tcWK7pknVYzfTMhv0/eL18BsGdcL8jbsxadGW2raY2jjGZaEaEFsgN2f9LrQqEndQryiJTCIaTkQriaiMiMY67HcJETERlWrfC4joBSJaTEQLiWiIYd/+WnkZET1JKZjHpTL3bbZh/mWs1A2DuxajteZz3Vhbsl/aMdGX2w/n926TNXlUU02Dgrq4ICGPrXe6tU5cSAUgwaCfKogIT18Vv7rZ/NiprFNr2qAAwxRnU0I8rgKAiPIAPA3gfAA9AVxJRAm564ioCMBPAcw2FN8EAMzcG8AwAH8mIv2cz2jbu2p/w/1fhpAs5pffzQbQunE9TPnZYDw0Or1p9AR1Li9tjwm3nl7z3Ww8VREAD4+yv99e5Mdz11nbP8yPXRTtQ6lEZQYwAEAZM69l5goA4wGMstjvYQC/B2AM7dgTwAwAYOZtAHYDKCWiNgAaM/OXHHv6XgIw2u9FCMnTpH5M16pnpSotaeZ6TPdjGqOwbmYHghPisVIj6Z2skwpI33JmN+vQ314Z2sN6xD6gUzM0b1Cr968SCRAqKgKgLQBjrN9NWlkNRNQPQHtmnmQ6diGAkURUl4g6AegPoL12/CanOsNAHiV7GmkGxvd+cjoW3n8uBncVfXxUqGZ1G4DjLhbbZt11lqe2nN29NZo0yMeovsdqbfN0uOCRpM3mmkrncQA/t9j8PGKd+1wAfwHwOQBPllgiGkNEc4lobnl5eVJtXbNtf1LH5zL6e5ZXh5IKySt4o0XDAlx0Urhjn3tGxBZSFdp4ydTYAEI4t9/Q2j84tSMAYOBxuReDP5NQsbptRmzUrtNOK9MpAtALwEzNjnsMgAlENJKZ5wK4Q9+RiD4HsArALq0euzprYOZnATwLAKWlpUmNB3YfOprM4ZFAzOSpZd69w0Ktv2OLBrjpjM64ycKbJiFDnOl7n3ZqIZBbNirA9v3BLvo7uaR53OJDIRxUZgBzAHQlok5EVADgCgAT9I3MvIeZWzJzCTOXAPgSwEhmnktEDYioIQAQ0TAAlcy8jJm3ANhLRKdq3j/XAvhvwNeWwGWaz/K9FybYsAUh8pgd8a6yiB1EBIzqeyx+PaJ74jafw4eh3YOxKwjecZ0BMHMlEd0KYCqAPADPM/NSInoIwFxmnuBweCsAU4moGrER/jWGbbcA+DeA+gA+0P5CpUWjwppRxcPvLwv7dFmF2NoEM60swiwTEf56RXKJ1s0M7toS01dsC7ROQQ0lx2tmngxgsqnsPpt9hxg+rwdgGXlMUw/1stompB59MVAQvuDXDyzBvz9f75hQRkgNTu68+oj//dsGYfryxA54cJeWhn2Db5uOjD3Sh6y8EeJINmIqEdCjTWy1cMMCebzSjcrMrlfbJpYpD+sagrWxg6E42dmjePqkDwmeIQAIRgX09FX9MOPnQxKSxQjh8u8fnmy7YtbrfXULFR7GPZUonulDBIAAIDHDlx8uOLENOrVsWPNd8i+khiHHt8KSB8+z3FbS0psb5jc7DliWqz4X9fJru5QWNoHc6iaEHhcBkC4iKwBe+r8B6W5CziKvc+oxC9sTjm0MAPjFue7Jf4zMXb/L03mctlsJjfdvG4TPxp4dVyYqoPQRWQHQp33TdDch5xEVUOqw+63r1gn2Fbc6j7HMGOJ6yPGJ7p292japCSioIxOA9BFZK53oHePRp+FWeVq9Ij9tennuulI88dEqALXeXX5g5oS1AVZPhzHM+p8v64O7z+8OBlDcKJY7Yv69wxzDTIgKKH1EVgDUL5AgZka8xINRRWYA4XOORVC1LXtq4zF6tsP4uGeHjsYEwJ5DFSguKsSxTeMTvTRzSeoydelW7ycVAiGyAqCwbh7W/G6EUrzxKKAnSwvi90hm1Cmos/Z3I2qErFHY7jpQgcqq5Gd0zLX1Hj6qPSAO1dXs4xGjwBJSS2RtAEDs5ZB8ojF+c0EP1Muvg6YN3FPwuVE7o5ffNkzqGJ5f40i/sppRpVlW61okQXpo1Am2Af/OMESBtRLjOw8EG/MHCGbQIfgjsjMAIZ7RJ7XF6ICjUopsTQ/VXCsArGYA155WgmtPK7E8tk2TWgNtzE4Wf7w+s7DC7/0OUu0oeCPSMwAhHEQBlHqMfWg1M5688iSM6H0MOnoMx/z9k9sjP49wepcWlsLDqa/2u+5DBED6kBmAEDgFmtrBLv68EC55ROjVtgn+fnV/z8c2bVCA1Y+M8HVev/249P/pQwSAEDgX92uHDTsP4uYhXdLdlMiQb4jbc1G/dg57hke7ZvXdd7JAZgDpQ4ZoQuDk59XBL8/rXpNfWEgt5lALQWKVA7pHm9iqY78eR1v3ihdQupA3VBByjDAH1Faj/JdvGICvv9mFBj6jv1ZU+nMfFZJHZgCCICRFy0aFOPcEyf2QjYgAEARBGVHX5xYiAARBUCaMEN/djykKvE5BDREAgpBjhLm6PYyqz+xW7L6TEApKAoCIhhPRSiIqI6KxDvtdQkRMRKXa93wiepGIFhPRciK627Dveq18ARHNTf5SBEHISkStlDZczfZElAfgaQDDAGwCMIeIJjDzMtN+RQB+CmC2ofgyAIXM3JuIGgBYRkSva8niAeAsZt4ewHUIgpACwuirZR1A+lCZAQwAUMbMa5m5AsB4AKMs9nsYwO8BGJ16GUBDIqoLoD6ACgB7k2uyIAhOtCoqDK3uMNRLp3ZuASCYXBSCN1QEQFsAGw3fN2llNRBRPwDtmXmS6di3ABwAsAXABgB/Yuad2jYG8CERzSOiMX4aLwhCIsZVwdlA43oxRUTvtk3S3JLokfRCMCKqA+BxANdbbB4AoArAsQCaAZhFRB8x81oAg5h5MxG1AjCNiFYw8ycW9Y8BMAYAOnTokGxzBUFIgjDG6Ce2a4rrTuuIGwd3DqF2wQmVocJmAO0N39tpZTpFAHoBmElE6wGcCmCCZgi+CsAUZj7KzNsAfAagFACYebP2fxuAdxETFgkw87PMXMrMpcXF4i0gCOmgOES1Ul4dwoOjeqG9x8ilQvKoCIA5ALoSUSciKgBwBYAJ+kZm3sPMLZm5hJlLAHwJYCQzz0VM7XM2ABBRQ8SEwwoiaqgZjfXycwEsCfC6BEEIkHduHog/XdYHdURPn1O4qoCYuZKIbgUwFUAegOeZeSkRPQRgLjNPcDj8aQAvENFSxGaPLzDzIiLqDOBdzaBUF8BrzDwl2YsRBCEc2jdvICP0HETJBsDMkwFMNpXdZ7PvEMPn/Yi5gpr3WQugj5eGCoIgCMGSXe4CgiAIQmCIABAEQYgoIgAEQRAiiggAQRCEiCICQBAEIaKIABAEQYgoIgAEQRAiiggAQRCEiJJ0MDhBEDKDp6/qh4aFeeluhpBFiAAQhBzhghPbpLsJQpYhKiBBEISIIgJAEAQhoogAEARBiCgiAARBECKKCABBEISIIgJAEAQhoogAEARBiCgiAARBECIKMXO626AMEZUD+Mbn4S0BbA+wOZmKXGduIdeZW6TrOjsyc7G5MKsEQDIQ0VxmLk13O8JGrjO3kOvMLTLtOkUFJAiCEFFEAAiCIESUKAmAZ9PdgBQh15lbyHXmFhl1nZGxAQiCIAjxRGkGIAiCIBiIhAAgouFEtJKIyohobLrb4wYRPU9E24hoiaGsORFNI6LV2v9mWjkR0ZPatS0ion6GY67T9l9NRNcZyvsT0WLtmCeJiFJ7hTXtaE9EHxPRMiJaSkQ/1cpz6lqJqB4RfUVEC7XrfFAr70REs7W2vUFEBVp5ofa9TNteYqjrbq18JRGdZyjPmGeciPKIaD4Rva99z7nrJKL12nO1gIjmamXZ99wyc07/AcgDsAZAZwAFABYC6Jnudrm0+QwA/QAsMZT9AcBY7fNYAL/XPo8A8AEAAnAqgNlaeXMAa7X/zbTPzbRtX2n7knbs+Wm6zjYA+mmfiwCsAtAz165VO3cj7XM+gNlam94EcIVW/g8AN2ufbwHwD+3zFQDe0D731J7fQgCdtOc6L9OecQB3AngNwPva95y7TgDrAbQ0lWXdcxuFGcAAAGXMvJaZKwCMBzAqzW1yhJk/AbDTVDwKwIva5xcBjDaUv8QxvgTQlIjaADgPwDRm3snMuwBMAzBc29aYmb/k2JP2kqGulMLMW5j5a+3zPgDLAbRFjl2r1t792td87Y8BnA3gLa3cfJ369b8FYKg2AhwFYDwzH2HmdQDKEHu+M+YZJ6J2AC4AME77TsjB67Qh657bKAiAtgA2Gr5v0sqyjdbMvEX7vBVAa+2z3fU5lW+yKE8r2vT/JMRGxzl3rZpaZAGAbYi96GsA7GbmSou21VyPtn0PgBbwfv3p4C8A7gJQrX1vgdy8TgbwIRHNI6IxWlnWPbeSEzgLYWYmopxx3yKiRgDeBvAzZt5rVHfmyrUycxWAvkTUFMC7ALqnt0XBQ0QXAtjGzPOIaEiamxM2g5h5MxG1AjCNiFYYN2bLcxuFGcBmAO0N39tpZdnGd9rUENr/bVq53fU5lbezKE8LRJSPWOf/KjO/oxXn5LUCADPvBvAxgNMQUwXogzBj22quR9veBMAOeL/+VHM6gJFEtB4x9czZAP6K3LtOMPNm7f82xAT6AGTjc5sOA0oq/xCb5axFzJikG45OSHe7FNpdgngj8B8Rb2D6g/b5AsQbmL7iWgPTOsSMS820z83Z2sA0Ik3XSIjpN/9iKs+pawVQDKCp9rk+gFkALgTwH8QbR2/RPv8E8cbRN7XPJyDeOLoWMcNoxj3jAIag1gicU9cJoCGAIsPnzwEMz8bnNm0PSIpv2AjEPEzWALgn3e1RaO/rALYAOIqY/u8GxHSj0wGsBvCR4UEhAE9r17YYQKmhnv9DzIBWBuCHhvJSAEu0Y/4GbUFgGq5zEGK61EUAFmh/I3LtWgGcCGC+dp1LANynlXfWXvQyxDrJQq28nva9TNve2VDXPdq1rITBMyTTnnHEC4Ccuk7tehZqf0v1dmTjcysrgQVBECJKFGwAgiAIggUiAARBECKKCABBEISIIgJAEAQhoogAEARBiCgiAARBECKKCABBEISIIgJAEAQhovw/V5O1s0+xKZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np.arange(len(accuvs[:-1000])),accuvs[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints/SMSDatasetB1NES128/PEPS_16x9_Z2_Binary_TAT_19_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "state_dict={}\n",
    "state_dict['state_dict'] = model.state_dict()\n",
    "state_dict['optimizer_TYPE']='Adam'\n",
    "state_dict['optimizer_lr']  =0.001\n",
    "state_dict['batch_size']    =200\n",
    "state_dict['loss_fn']       ='BCEWithLogitsLoss'\n",
    "state_dict['losses']    =losses\n",
    "state_dict['accuts']    =accuts\n",
    "state_dict['accuvs']    =accuvs\n",
    "state_dict['script']    ='''\n",
    "from models.extend_model import *\n",
    "model =  PEPS_16x9_Z2_Binary_Aggregation_Wrapper(TensorAttention,\n",
    "                                    \"models/arbitary_shape/patch_partions_3column_12units.pt\", alpha_list = 0.2,fixed_virtual_dim=2)(out_features=1)\n",
    "'''\n",
    "#torch.save(state_dict,\"checkpoints/SMSDatasetB1NES128/PEPS_16x9_Z2_Binary_TAT_19_v2/overfitting_model_20220519.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022 \t Loss: 0.5536 \t Acct: 0.7300 \t Accu: 0.5020 \t Time: 100.51 s\r"
     ]
    }
   ],
   "source": [
    "data_loader = test_loader\n",
    "device = 'cuda'\n",
    "model  = model.to(device).eval()\n",
    "model.eval()\n",
    "prefetcher = DataSimfetcher(data_loader, device=device)\n",
    "inter_b    = logsys.create_progress_bar(len(data_loader))\n",
    "labels     = []\n",
    "logits     = []\n",
    "with torch.no_grad():\n",
    "    while inter_b.update_step():\n",
    "        #image,label,_=  prefetcher.next()\n",
    "        label,image = prefetcher.next()[:2]\n",
    "        binary     = preprocess_images(image)\n",
    "        logit      = model(binary).squeeze()\n",
    "        #loss       = loss_fn(logit ,label.squeeze())\n",
    "        labels.append(label)\n",
    "        logits.append(logit)\n",
    "labels  = torch.cat(labels)\n",
    "logits  = torch.cat(logits)\n",
    "if len(logits.shape)==2:\n",
    "    pred_labels  = torch.argmax(logits,-1)\n",
    "    accu =  torch.sum(pred_labels == labels)/len(pred_labels)\n",
    "elif len(logits.shape)==1:\n",
    "    accu =  torch.sum(torch.round(torch.sigmoid(logits)) == labels)/len(labels)\n",
    "else:raise NotImplementedError\n",
    "#accu = loss\n",
    "#accues.append(accu)\n",
    "#master_bar.update_graph_multiply([[losses[-100:],g_norm,accues[-100:]]])\n",
    "accu = accu.item()\n",
    "master_bar.lwrite('Epoch: %.3i \\t Loss: %.4f \\t Acct: %.4f \\t Accu: %.4f \\t Time: %.2f s' %(epoch, loss, accut,accu,time.time() - start_time),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sample(self, bs, random_start=False):\n",
    "    \"\"\"\n",
    "    Sample images/spin configurations\n",
    "    \"\"\"\n",
    "\n",
    "    device = self.tensors.device\n",
    "    samples = torch.empty([bs, self.n], device=device)\n",
    "\n",
    "    # if random_start = True, force s_1 = -1/+1 randomly\n",
    "    if random_start:\n",
    "        samples[:, 0] = torch.randint(2, size=(bs, ), dtype=torch.float, device=device)\n",
    "    else:\n",
    "        samples[:, 0] = 0.\n",
    "\n",
    "    for idx in range(self.n - 1):\n",
    "        if idx == 0:\n",
    "            # sample s_2 from p(s_2 | s_1)\n",
    "            embedded_data = torch.stack([samples[:, 0], 1.0 - samples[:, 0]], dim=1)  # (bs, 2)\n",
    "            mats          = torch.einsum('lri,bi->blr', self.tensors[0, :, :, :] , embedded_data)\n",
    "            left_vec      = mats[:, 0, :].unsqueeze(1)  # (bs, 1, D)\n",
    "            logits        = torch.einsum('blr, ri->bli', left_vec,(self.tensors[1, :, :, :] )[:, 0, :]).squeeze(1)\n",
    "            samples[:, 1] = torch.bernoulli(torch.softmax(logits, dim=1)[:, 0])\n",
    "        else:\n",
    "            # then sample s_3 from  p(s_3 | s_1, s_2) and so on\n",
    "            embedded_data = torch.stack([samples[:, idx], 1.0 - samples[:, idx]], dim=1)  # (bs, 2)\n",
    "            mats = torch.einsum('lri,bi->blr', self.tensors[idx, :, :, :] , embedded_data)\n",
    "            left_vec = torch.bmm(left_vec, mats)  # (bs, 1, D)\n",
    "            logits = torch.einsum('blr, ri->bli', left_vec,\n",
    "                                  (self.tensors[idx + 1, :, :, :] )[:, 0, :]).squeeze(1)\n",
    "            samples[:, idx + 1] = torch.bernoulli(torch.softmax(logits, dim=1)[:, 0])\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
