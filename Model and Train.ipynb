{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mnist_data = np.load('archive/tn-for-unsup-ml/data/binarized_mnist.npz')\n",
    "train_data = torch.from_numpy(mnist_data['train_data'])\n",
    "test_data = torch.from_numpy(mnist_data['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.0,), std=(1.0,))\n",
    "])\n",
    "DATAPATH    = '/media/tianning/DATA/DATASET/MNIST/'\n",
    "mnist_train = datasets.MNIST(DATAPATH, train=True, download=False, transform=transform)\n",
    "mnist_test  = datasets.MNIST(DATAPATH, train=False,download=False, transform=transform)\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltool.visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0667824f98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(torch.sum(mnist_train.data>0,0)[4:25,4:25].numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple MPS layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     2,
     14,
     38,
     51,
     58,
     68
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MPSLinear(nn.Module):\n",
    "    '''\n",
    "    For a naive Linear Layer(in_features,out_features,\n",
    "                             in_physics_bond = 2, out_physics_bond=2, virtual_bond_dim=2, \n",
    "                             bias=True,label_position='center',init_std=1e-10\n",
    "                                       ): \n",
    "        input  (B, in_features)\n",
    "        output (B, out_features)\n",
    "    For s simplest MPSLayer(in_features: int, out_features: int, \n",
    "                            in_physics_bond: int, out_physics_bond: int, virtual_bond_dim:int,\n",
    "                            bias: bool = True, label_position: int or str): \n",
    "        input  (B, in_features , in_physics_bond)\n",
    "        output (B, out_features,out_physics_bond)\n",
    "    '''\n",
    "    def __init__(self, in_features,out_features,\n",
    "                                       in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=2, \n",
    "                                       bias=True,label_position='center',init_std=1e-10):\n",
    "        super(MPSLinear, self).__init__()\n",
    "        if label_position is 'center':\n",
    "            label_position = in_features//2\n",
    "        assert type(label_position) is int\n",
    "        self.in_features   = in_features\n",
    "        self.out_features  = out_features\n",
    "        self.vbd           = virtual_bond_dim\n",
    "        self.ipb           = in_physics_bond\n",
    "        self.opb           = out_physics_bond\n",
    "        self.hn            = label_position\n",
    "        left_num           = self.hn\n",
    "        right_num          = in_features - left_num\n",
    "\n",
    "        bias_mat = torch.eye(self.vbd).unsqueeze(-1).repeat(1,1,self.ipb)\n",
    "        self.left_tensors = nn.Parameter(init_std * torch.randn(left_num         ,self.vbd,self.vbd, self.ipb)+ bias_mat)\n",
    "        self.rigt_tensors = nn.Parameter(init_std * torch.randn(right_num        ,self.vbd,self.vbd, self.ipb)+ bias_mat)\n",
    "        \n",
    "        bias_mat = torch.eye(self.vbd).unsqueeze(-1).repeat(1,1,self.opb)\n",
    "        self.cent_tensors = nn.Parameter(init_std * torch.randn(self.out_features,self.vbd,self.vbd, self.opb)+ bias_mat)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_chain_contraction_fast(tensor):\n",
    "        size   = int(tensor.shape[0])\n",
    "        while size > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "        return tensor.squeeze(0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_chain_contraction_memory_save(tensor):\n",
    "        size      = int(tensor.shape[0])\n",
    "        now_tensor= tensor[0]\n",
    "        for next_tensor in tensor[1:]:\n",
    "            now_tensor = torch.einsum(\"bik,bkj->bij\",now_tensor, next_tensor)\n",
    "        return now_tensor\n",
    "    \n",
    "    def get_chain_contraction(self,tensor):\n",
    "        size   = int(tensor.shape[0])\n",
    "        D      = int(tensor.shape[-1])\n",
    "        print(size)\n",
    "        print(D)\n",
    "        if D>30:\n",
    "            return self.get_chain_contraction_memory_save(tensor)\n",
    "        else:\n",
    "            return self.get_chain_contraction_fast(tensor)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        # the input data shape is (B,L,pd)\n",
    "        # expand to convolution patch\n",
    "        embedded_data= input_data\n",
    "        left_tensors = torch.einsum('wijp,nwp->wnij',self.left_tensors,embedded_data[:,:self.hn])#i.e. (K,NB,b,b)\n",
    "        rigt_tensors = torch.einsum('wijp,nwp->wnij',self.rigt_tensors,embedded_data[:,-self.hn:])#i.e.(K,NB,b,b)\n",
    "\n",
    "        left_tensors = self.get_chain_contraction(left_tensors) #i.e. (NB,b,b)\n",
    "        rigt_tensors = self.get_chain_contraction(rigt_tensors) #i.e. (NB,b,b)\n",
    "\n",
    "        tensor  = torch.einsum('bip,oplt,bli->bot',left_tensors,self.cent_tensors,rigt_tensors)\n",
    "        # (NB,b,b) <-> (T,b,b,o) <-> (NB,b,b) ==> (NB,T,t)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     3,
     70
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensornetwork as tn\n",
    "from tensornetwork import contractors\n",
    "tn.set_default_backend(\"pytorch\")\n",
    "class MPSLinear_tn_loop(nn.Module):\n",
    "    '''\n",
    "    For s simplest MPSLayer(in_features: int, out_features: int,\n",
    "                            in_physics_bond: int, \n",
    "                            out_physics_bond: int, virtual_bond_dim:int,\n",
    "                            bias: bool = True, label_position: int or str):\n",
    "        input  (B, in_features , in_physics_bond)\n",
    "        output (B, out_features)\n",
    "    '''\n",
    "    def __init__(self, in_features,out_features,\n",
    "                       in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=2,\n",
    "                       bias=True,label_position='center',init_std=1e-10,**kargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if label_position is 'center':\n",
    "            label_position = in_features//2\n",
    "        assert type(label_position) is int\n",
    "        self.in_features   = in_features\n",
    "        self.out_features  = out_features\n",
    "        self.vbd           = virtual_bond_dim\n",
    "        self.ipb           = in_physics_bond\n",
    "        self.opb           = out_physics_bond\n",
    "        self.hn            = label_position\n",
    "        \n",
    "        left_num           = self.hn\n",
    "        right_num          = in_features - left_num\n",
    "\n",
    "        bias_mat     = torch.eye(self.ipb,self.vbd)\n",
    "        left_end     = init_std * torch.randn(self.ipb,self.vbd) + bias_mat\n",
    "\n",
    "        bias_mat     = torch.eye(self.vbd, self.ipb)\n",
    "        right_end    = init_std * torch.randn(self.vbd, self.ipb)+ bias_mat\n",
    "\n",
    "        bias_mat     = torch.eye(self.vbd).unsqueeze(1).repeat(1,self.ipb,1)\n",
    "        left_tensors = init_std * torch.randn(left_num-1 ,self.vbd, self.ipb , self.vbd)+ bias_mat\n",
    "        rigt_tensors = init_std * torch.randn(right_num-1,self.vbd, self.ipb,self.vbd)+ bias_mat\n",
    "\n",
    "\n",
    "        bias_mat     = torch.eye(self.vbd).unsqueeze(1).repeat(1,self.out_features,1)\n",
    "        cent_tensors = init_std * torch.randn(self.vbd,self.out_features,self.vbd)+ bias_mat\n",
    "\n",
    "        mps_var      = [left_end] + list(left_tensors)  + [cent_tensors] + list(rigt_tensors) + [right_end]\n",
    "        self.mps_var = [nn.Parameter(v) for v in mps_var]\n",
    "        self.center  = left_num\n",
    "        for i, v in enumerate(self.mps_var):\n",
    "            self.register_parameter(f'mps{i}', param=v)\n",
    "\n",
    "    def contract_mps_with_input(self,input):\n",
    "        assert len(input) == len(self.mps_var)-1\n",
    "        mps_list_1   = self.mps_var\n",
    "        mps_nodes_1  = [tn.Node(v, name=f\"t{i}\") for i,v in enumerate(mps_list_1)]\n",
    "        mps_edges_1  = [mps_nodes_1[i][-1]^mps_nodes_1[i+1][0] for i in range(len(mps_nodes_1)-1)]\n",
    "        inp_nodes    = [tn.Node(v, name=f\"i{i}\") for i,v in enumerate(input)]\n",
    "        for i,input_node in enumerate(inp_nodes):\n",
    "            j = i if i < self.center else i+1\n",
    "            mps_physicd_edge = mps_nodes_1[j][0] if j==0 else mps_nodes_1[j][1]\n",
    "            inp_physics_edge = input_node[0]\n",
    "            tn.connect(mps_physicd_edge,inp_physics_edge,name=f\"p_{i}\")\n",
    "\n",
    "        ans = contractors.auto(mps_nodes_1+inp_nodes,\n",
    "                              output_edge_order=[mps_nodes_1[self.center][1]]).tensor\n",
    "        return ans\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = torch.stack([self.contract_mps_with_input(single_input) for single_input in inputs])\n",
    "        return out\n",
    "    \n",
    "class MPSLinear_tn_batch(MPSLinear_tn_loop):\n",
    "    def forward(self, inputs):\n",
    "        num        = len(self.mps_var)\n",
    "        mps_nodes  = [tn.Node(v, name=f\"t{i}\") for i,v in enumerate(self.mps_var)]\n",
    "        mps_edges  = [mps_nodes[i][-1]^mps_nodes[i+1][0] for i in range(num-1)]\n",
    "\n",
    "\n",
    "        inputs= inputs.permute(1,2,0)#(B,num,k)->(num,k,B)\n",
    "        out   = torch.diag_embed(inputs)#(num,k,B)->(num,k,B,B)\n",
    "        out   = out.permute(0,2,1,3)#(num,k,B,B)->(num,B,k,B)\n",
    "        out   = [v for v in out]\n",
    "        out[0]= torch.diagonal(out[0], dim1=0, dim2=-1).transpose(1,0)#(B,k,B) -> #(B,k)\n",
    "\n",
    "        inp_nodes=[tn.Node(v, name=f\"i{i}\") for i,v in enumerate(out)]\n",
    "        inp_edges=[inp_nodes[0][0]^inp_nodes[1][0]]+ [\n",
    "            inp_nodes[i][-1]^inp_nodes[i+1][0] for i in range(1,len(inp_nodes)-1)]\n",
    "\n",
    "        for i,input_node in enumerate(inp_nodes):\n",
    "            j = i if i < self.center else i+1\n",
    "            mps_physicd_edge = mps_nodes[j][0] if j==0 else mps_nodes[j][1]\n",
    "            inp_physics_edge = input_node[1]\n",
    "            tn.connect(mps_physicd_edge,inp_physics_edge,name=f\"p_{i}\")\n",
    "\n",
    "        ans = contractors.auto(mps_nodes+inp_nodes,output_edge_order=[inp_nodes[-1][2],mps_nodes[self.center][1]]).tensor\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = MPSLinear_tn_loop(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=10,\n",
    "                  bias=False,label_position='center',init_std=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = MPSLinear_tn_batch(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=10,\n",
    "                  bias=False,label_position='center',init_std=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_input = torch.randn(2,28*28,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### AMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from models.amps import AMPSShare\n",
    "class AMPSShare(nn.Module):\n",
    "    '''\n",
    "    This version may fast, but will cost much more memory\n",
    "    n        : the length of input tensor sequence\n",
    "    bond_dim : the virtual bond dim. The capacity of model.\n",
    "    phys_dim : the feature/class number\n",
    "    ------------------------------\n",
    "    Input:  any data/spin configurations, shape: (B, n ,phys_dim)\n",
    "    Output: prob_matrix of each sample, shape: (B, n, phys_dim), pass softmax to get probility.\n",
    "    -------------------------------\n",
    "    Weight Cost:\n",
    "        n * bond_dim * bond_dim * phys_dim\n",
    "    '''\n",
    "    def __init__(self, n=784, bond_dim=10, phys_dim=2,std=1e-8):\n",
    "        super(AMPSShare, self).__init__()\n",
    "        # Initialize AMPS model parameters, which is a (n, D, D, 2) tensor\n",
    "        self.register_buffer('bias_mat', torch.eye(bond_dim).unsqueeze(-1).repeat(1,1,phys_dim))\n",
    "        # bias_mat: which is realy important when n>>1\n",
    "        self.tensors = nn.Parameter(std * torch.randn(n, bond_dim, bond_dim, phys_dim)+self.bias_mat)\n",
    "        # Set attributes\n",
    "        self.n = n\n",
    "        self.bond_dim = bond_dim\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, embedded_data):\n",
    "\n",
    "        bs = embedded_data.shape[0]\n",
    "        # local feature map, x_j -> [x_j, 1-x_j]\n",
    "        #-> embedded_data = torch.stack([data, 1.0 - data], dim=2)  # (bs, n, 2)\n",
    "        ##logx_hat = torch.zeros_like(embedded_data)\n",
    "        ##logx_hat[:, 0, :] = F.log_softmax(self.tensors[0, 0, 0], dim=0)\n",
    "        prob_matrix = self.tensors[0, 0, 0].repeat((bs,1)).unsqueeze(1) # (bs,1,2)\n",
    "        mats = torch.einsum('lri,bi->blr', self.tensors[0] , embedded_data[:, 0, :])\n",
    "        left_vec = mats[:, 0:1, :]  # (bs,  D)\n",
    "        for idx in range(1, self.n):\n",
    "            # compute p(s_2 | s_1) and so on\n",
    "            logits = torch.einsum('br, ri->bi', left_vec.squeeze(1),self.tensors[idx,:,0,:])\n",
    "            #(bs,D) <-> (D,2) ->(bs,2)\n",
    "            prob_matrix = torch.cat([prob_matrix,logits.unsqueeze(1)], dim=1)\n",
    "            #(bs, n-2, 2) + (bs,1,2) -> (bs, n-1, 2)\n",
    "            ##logx_hat[:, idx, :] = F.log_softmax(logits, dim=1)\n",
    "            mats = torch.einsum('lri,bi->blr', self.tensors[idx, :, :, :] , embedded_data[:, idx, :])\n",
    "            #(D,D,2) <-> (bs,2) ->(bs,D,D)\n",
    "            left_vec = torch.bmm(left_vec, mats)  # (bs, 1, D)\n",
    "        # compute log prob\n",
    "        return prob_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tensornetwork_base import TN_Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     22,
     40,
     52,
     88,
     122
    ]
   },
   "outputs": [],
   "source": [
    "class PEPS_einsum_uniform_shape(TN_Base):\n",
    "    def __init__(self, W,H,out_features,\n",
    "                       in_physics_bond = 2, virtual_bond_dim=2,\n",
    "                       bias=True,label_position='center',init_std=1e-10,contraction_mode = 'recursion'):\n",
    "        super().__init__()\n",
    "        #label_position at 'corner':\n",
    "        label_pos_x        = W\n",
    "        label_pos_y        = H\n",
    "        self.W             = W\n",
    "        self.H             = H\n",
    "        \n",
    "        self.out_features  = O = out_features\n",
    "        self.vbd           = D = virtual_bond_dim\n",
    "        self.ipb           = P = in_physics_bond\n",
    "        self.label_pos_x   = label_pos_x\n",
    "        self.label_pos_y   = label_pos_y\n",
    "        \n",
    "        self.bulk_tensors = nn.Parameter(self.rde2D((     (W-2)*(H-2),P,D,D,D,D),init_std))\n",
    "        self.edge_tensors = nn.Parameter(self.rde2D( (2*(W-2)+2*(H-2),P,D,D,D),init_std))\n",
    "        self.corn_tensors = nn.Parameter(self.rde2D(                 (3,P,D,D),init_std))\n",
    "        self.cent_tensors = nn.Parameter(self.rde2D(                 (O,P,D,D),init_std))\n",
    "    @staticmethod\n",
    "    def rde2D(shape,init_std,offset=2):\n",
    "        size_shape = shape[:offset]\n",
    "        bias_shape = shape[offset:]\n",
    "        if len(bias_shape) ==2 :\n",
    "            bias_mat   = torch.eye(*bias_shape)\n",
    "        elif len(bias_shape) == 3:\n",
    "            a,b,c   = bias_shape\n",
    "            bias_mat   = torch.kron(torch.ones(a),torch.eye(b,c)).reshape(a,b,c)\n",
    "        elif len(bias_shape) == 4:\n",
    "            a,b,c,d   = bias_shape\n",
    "            bias_mat   = torch.kron(torch.ones(a,b),torch.eye(c,d)).reshape(a,b,c,d)\n",
    "#         bias_mat   = torch.zeros(bias_shape)\n",
    "#         diag_idx   = [list(range(min(bias_shape)))]*len(bias_shape)\n",
    "#         bias_mat[diag_idx] = 1\n",
    "#         for i in range(offset):bias_mat=bias_mat.unsqueeze(0)\n",
    "        bias_mat   = bias_mat.repeat(*size_shape,*([1]*len(bias_shape)))\n",
    "        tensor     = init_std * torch.randn(*shape)+ bias_mat\n",
    "        return tensor                                    \n",
    "    def mpo_line(self,i,bulk_tensors,edge_tensors,corn_tensors,cent_tensors):\n",
    "        W=self.W\n",
    "        H=self.H\n",
    "        if i == 0:\n",
    "            return  corn_tensors[0],edge_tensors[0:W-2],corn_tensors[1]\n",
    "        elif i == H - 1:\n",
    "            return  corn_tensors[2],edge_tensors[-(W-2):],cent_tensors\n",
    "        else:\n",
    "            return  edge_tensors[W-4+2*i],bulk_tensors[(W-2)*(i-1):(W-2)*i],edge_tensors[W-4+2*i+1]\n",
    "    \n",
    "\n",
    "    @staticmethod                                  \n",
    "    def batch_contract_mps_mpo(mps_list,mpo_list):\n",
    "        # mps_list                                    --D--|--D--\n",
    "        # (D,D)-(D,D,D)-(D,D,D)-...-(D,D,D)-(D,D)          D\n",
    "        #  -b-    -c-  -b-   -c-  -b-     -b- \n",
    "        # |a         |a         |a          |a  \n",
    "        # the order i.e. 'abcd' counterclockwise and start from the down index. (so down index must a )\n",
    "        #  mpo_list                                           \n",
    "        # (P,D,P)-(D,P,D,P)-(D,P,D,P)-...-(D,P,D,P)-(D,P,P)  \n",
    "        #  |c            |c        |c           |b                        \n",
    "        #   -b-      -d-  -b-  -d-  -b-     -c-  \n",
    "        # |a            |a        |a           |a                                             \n",
    "        assert len(mps_list) > 2\n",
    "        assert len(mpo_list) > 2\n",
    "        stack_unit_mps = (len(mps_list)==3 and len(mps_list[0].shape)+ 2 == len(mps_list[1].shape))\n",
    "        stack_unit_mpo = (len(mpo_list)==3 and len(mpo_list[0].shape)+ 2 == len(mpo_list[1].shape))\n",
    "        mps_left,mps_rigt = mps_list[0],mps_list[-1]\n",
    "        mpo_left,mpo_rigt = mpo_list[0],mpo_list[-1]\n",
    "        new_mps_list= []\n",
    "        tensor = torch.einsum(\"  kab,kcda->kcbd\",mps_left,mpo_left).flatten(-2,-1)\n",
    "        new_mps_list.append(tensor)\n",
    "        if stack_unit_mps and stack_unit_mpo:\n",
    "            mps_inne = mps_list[1]\n",
    "            mpo_inne = mpo_list[1]                            \n",
    "            tensor = torch.einsum(\"lkabc,lkdeaf->lkdbecf\",mps_inne,mpo_inne).flatten(-4,-3).flatten(-2,-1)\n",
    "            new_mps_list.append(tensor)\n",
    "        else:\n",
    "            if stack_unit_mps:mps_inne = list(*mps_list[1:-1])\n",
    "            if stack_unit_mpo:mpo_inne = list(*mpo_list[1:-1])\n",
    "            for mps,mpo in zip(mps_inne,mpo_inne):\n",
    "                tensor =torch.einsum(\"kabc,kdeaf->kdebcf\",mps,mpo).flatten(-4,-3).flatten(-2,-1)     \n",
    "                new_mps_list.append(tensor)\n",
    "        tensor = torch.einsum(\"  kab,kcad->kcbd\",mps_rigt,mpo_rigt).flatten(-2,-1)      \n",
    "        new_mps_list.append(tensor)\n",
    "        return new_mps_list                                 \n",
    "    \n",
    "    @staticmethod\n",
    "    def batch_contract_mps_mps(mps_list,mpo_list):\n",
    "        # mps_list                                    --D--|--D--\n",
    "        # (D,D)-(D,D,D)-(D,D,D)-...-(D,D,D)-(D,D)          D\n",
    "        # (D,D)-(D,D,D)-(D,D,D)-...-(D,D,D)-(D,D,O)\n",
    "        #  -b-    -c-  -b-   -c-  -b-     -b- \n",
    "        # |a         |a         |a          |a                                \n",
    "        # \n",
    "        # |b            |b        |b        |b                        \n",
    "        #  -a-      -c-  -a-  -c-  -a-   -c- -a                     \n",
    "        assert len(mps_list) > 2\n",
    "        assert len(mpo_list) > 2\n",
    "        stack_unit_mps = (len(mps_list)==3 and len(mps_list[0].shape)+ 2 == len(mps_list[1].shape))\n",
    "        stack_unit_mpo = (len(mpo_list)==3 and len(mpo_list[0].shape)+ 2 == len(mpo_list[1].shape))\n",
    "        mps_left,mps_rigt = mps_list[0],mps_list[-1]\n",
    "        mpo_left,mpo_rigt = mpo_list[0],mpo_list[-1]\n",
    "        new_mps_list= []\n",
    "        tensor = torch.einsum(\"  kab,kca->kbc\",mps_left,mpo_left).flatten(-2,-1)\n",
    "        new_mps_list.append(tensor)\n",
    "        if stack_unit_mps and stack_unit_mpo:\n",
    "            mps_inne = mps_list[1]\n",
    "            mpo_inne = mpo_list[1]                            \n",
    "            tensor = torch.einsum(\"lkabc,lkdae->lkbdce\",mps_inne,mpo_inne).flatten(-4,-3).flatten(-2,-1)\n",
    "            new_mps_list.append(tensor)\n",
    "        else:\n",
    "            if stack_unit_mps:mps_inne = list(*mps_list[1:-1])\n",
    "            if stack_unit_mpo:mpo_inne = list(*mpo_list[1:-1])\n",
    "            for mps,mpo in zip(mps_inne,mpo_inne):\n",
    "                tensor =torch.einsum(\"kabc,kdae->kbdce\",mps,mpo).flatten(-4,-3).flatten(-2,-1)     \n",
    "                new_mps_list.append(tensor)\n",
    "        tensor = torch.einsum(\"  kab,okac->kobc\",mps_rigt,mpo_rigt).flatten(-2,-1)      \n",
    "        new_mps_list.append(tensor)\n",
    "        return new_mps_list                                                        \n",
    "    \n",
    "    @staticmethod\n",
    "    def flatten_image_input(batch_image_input):\n",
    "        bulk_input = batch_image_input[...,1:-1,1:-1,:].flatten(1,2)\n",
    "        edge_input = torch.cat([batch_image_input[...,0,1:-1,:],\n",
    "                                batch_image_input[...,1:-1,[0,-1],:].flatten(-3,-2),\n",
    "                                batch_image_input[...,-1,1:-1,:]\n",
    "                               ],1)\n",
    "        corn_input = batch_image_input[...,[0,0,-1],[0,-1,0],:]\n",
    "        cent_input = batch_image_input[...,-1,-1,:]\n",
    "        return bulk_input,edge_input,corn_input,cent_input\n",
    "    \n",
    "    def forward(self, input_data,contraction_mode='top2bot',batch_method='physics_index_first'):\n",
    "        # the input data shape is (B,L,L,pd)\n",
    "        if batch_method == 'physics_index_first':\n",
    "            bulk_input,edge_input,corn_input,cent_input = self.flatten_image_input(input_data)\n",
    "            bulk_tensors = torch.einsum(\"lpabcd,klp->lkabcd\",self.bulk_tensors,bulk_input)\n",
    "            edge_tensors = torch.einsum(\" lpabc,klp->lkabc\" ,self.edge_tensors,edge_input)\n",
    "            corn_tensors = torch.einsum(\"  lpab,klp->lkab\"  ,self.corn_tensors,corn_input)\n",
    "            cent_tensors = torch.einsum(\"   opab,kp->okab\"  ,self.cent_tensors,cent_input)\n",
    "        if contraction_mode == 'top2bot':\n",
    "            tensor     = self.mpo_line(0,bulk_tensors,edge_tensors,corn_tensors,cent_tensors)\n",
    "            for i in range(1,self.H-1):\n",
    "                #print(get_mps_size_list(tensor))\n",
    "                mpo    = self.mpo_line(i,bulk_tensors,edge_tensors,corn_tensors,cent_tensors)\n",
    "                #print(get_mps_size_list(mpo))\n",
    "                #print(\"=============\")\n",
    "                tensor = self.batch_contract_mps_mpo(tensor,mpo)    \n",
    "                #tensor = right_mps_form(tensor)\n",
    "                #tensor,scale = approxmate_mps_line(tensor,max_singular_values=100)                                            \n",
    "            mps    = self.mpo_line(self.H-1,bulk_tensors,edge_tensors,corn_tensors,cent_tensors)\n",
    "            \n",
    "            #print(get_mps_size_list(mps))\n",
    "            tensor_left,tensor_inne,tensor_rigt = self.batch_contract_mps_mps(tensor,mps)\n",
    "            tensor_inne = self.get_batch_chain_contraction_fast(tensor_inne)     \n",
    "            tensor  = torch.einsum('ka,kba,kob->ko',tensor_left,tensor_inne,tensor_rigt)\n",
    "            return tensor       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PEPS_einsum_uniform_shape_6x6_fast(PEPS_einsum_uniform_shape):\n",
    "    def __init__(self, out_features,**kargs):\n",
    "        super().__init__(6,6,out_features,**kargs)\n",
    "        \n",
    "    def forward(self,input_data):\n",
    "        bulk_input,edge_input,corn_input,cent_input = self.flatten_image_input(input_data)\n",
    "        bulk_tensors = torch.einsum(\"lpabcd,klp->lkabcd\",self.bulk_tensors,bulk_input)\n",
    "        edge_tensors = torch.einsum(\" lpabc,klp->lkabc\" ,self.edge_tensors,edge_input)\n",
    "        corn_tensors = torch.einsum(\"  lpab,klp->lkab\"  ,self.corn_tensors,corn_input)\n",
    "        cent_tensors = torch.einsum(\"   opab,kp->okab\"  ,self.cent_tensors,cent_input)\n",
    "        #corner_contraction\n",
    "        W=H=6;\n",
    "        L=4              ;corn_index = [0,1,2,3]\n",
    "        L=2*(W-2)+2*(H-2);edge_index1=[0,(W-2)+1,W-2+2*(H-3),L-1] # [0,5,10,15] for 6x6\n",
    "        L=(W-2)*(H-2)    ;bulk_index = [0,W-3,(W-2)*(H-3),L-1]# [0,3,12,15] for 6x6\n",
    "        L=2*(W-2)+2*(H-2);edge_index2=[(W-2),(W-2)-1,L-(W-2),L-(W-2)-1] # [4,3,12,11] for 6x6\n",
    "        corner123_contraction = torch.einsum(\"lkab,lkcdb,lkefcg,lkgah->lkhedf\",\n",
    "                                       corn_tensors[corn_index[:3]],\n",
    "                                       edge_tensors[edge_index1[:3]],\n",
    "                                       bulk_tensors[bulk_index[:3]],\n",
    "                                       edge_tensors[edge_index2[:3]],\n",
    "                                      ).flatten(-4,-3).flatten(-2,-1)\n",
    "        corner4_contraction = torch.einsum(\"okab,kceb,khicg,kfga->okfhei\" ,\n",
    "                                       cent_tensors,\n",
    "                                       edge_tensors[edge_index1[-1]],\n",
    "                                       bulk_tensors[bulk_index[-1]],\n",
    "                                       edge_tensors[edge_index2[-1]],\n",
    "                                      ).flatten(-4,-3).flatten(-2,-1)\n",
    "\n",
    "        L=2*(W-2)+2*(H-2);egde_index = [1,W-4,W,W+1,-(W-2)-4,-(W-2)-3,L-3,L-2]\n",
    "        # [1,2,6,7,8,9,13,14] for 6x6\n",
    "        L=(W-2)*(H-2);bulk_index = [1,W-4,W-2,W-2+W-2-1,L-(W-2)-(W-3)-1,L-(W-2)-1,L-3,L-2]\n",
    "        # [1,2,4,7,8,11,13,14] for 6x6\n",
    "        edge_fast_contraction = torch.einsum(\"lkabc,lkefah->lkebfch\" ,\n",
    "                                               edge_tensors[egde_index],\n",
    "                                               bulk_tensors[bulk_index],\n",
    "                                              ).flatten(-4,-3).flatten(-2,-1)\n",
    "\n",
    "        L=(W-2)*(H-2);bulk_index = [W-1,2*(W-2)-2,L-W-(W-2)+3,L-(W)]# [5,6,9,10] for 6x6\n",
    "        edge_index1 = [0,3,4,7]\n",
    "        edge_index2 = [2,1,6,5]\n",
    "        corner123_contraction = torch.einsum(\"lkab,lkcdb,lkefcg,lkgah->lkhedf\" ,\n",
    "                                       corner123_contraction,\n",
    "                                       edge_fast_contraction[edge_index1[:3]],\n",
    "                                       bulk_tensors[bulk_index[:3]],\n",
    "                                       edge_fast_contraction[edge_index2[:3]],\n",
    "                                      ).flatten(-4,-3).flatten(-2,-1)\n",
    "\n",
    "        corner4_contraction = torch.einsum(\"okab,kcdb,kefcg,kgah->okhedf\",\n",
    "                                       corner4_contraction,\n",
    "                                       edge_fast_contraction[edge_index1[-1]],\n",
    "                                       bulk_tensors[bulk_index[-1]],\n",
    "                                       edge_fast_contraction[edge_index2[-1]],\n",
    "                                      ).flatten(-4,-3).flatten(-2,-1)\n",
    "        tensor  = torch.einsum(\"kab,kbc->kac\",corner123_contraction[0],corner123_contraction[1])\n",
    "        tensor  = torch.einsum(\"kab,kbc->kac\",tensor,corner123_contraction[2])\n",
    "        tensor  = torch.einsum(\"kab,okba->ko\",tensor,corner4_contraction)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PEPS_einsum_uniform_shape_6x6_fast(10,in_physics_bond = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### naive contractor: face dimenstion explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# tensor = torch.randn(8,8,6,6,6,6)\n",
    "# while tensor.shape[0]>2:\n",
    "#     print(tensor.shape)\n",
    "#     lu_tensor = tensor[0::2,0::2]\n",
    "#     ld_tensor = tensor[0::2,1::2]\n",
    "#     ru_tensor = tensor[1::2,0::2]\n",
    "#     rd_tensor = tensor[1::2,1::2]\n",
    "#     tensor     = torch.einsum(\"xyabcd,xyhdij,xycefg,xyigkl->xyahbefkjl\",\n",
    "#                             lu_tensor,\n",
    "#                             ld_tensor,\n",
    "#                             ru_tensor,\n",
    "#                             rd_tensor).flatten(4,5).flatten(-4,-3).flatten(2,3).flatten(-2,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### cotengra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensornetwork.contractors.opt_einsum_paths.utils import *\n",
    "from opt_einsum.paths import greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import opt_einsum as oe\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import opt_einsum as oe\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"pytorch\")\n",
    "    from tensornetwork.contractors.opt_einsum_paths.utils import *\n",
    "tensor = torch.randn(16,16,2,2,2,2).cuda()\n",
    "\n",
    "node_array = []\n",
    "W,H = tensor.shape[:2]\n",
    "for i in range(W):\n",
    "    node_line = []\n",
    "    for j in range(H):\n",
    "        node = tn.Node(tensor[i][j],name=f\"{i}-{j}\")\n",
    "        node_line.append(node)\n",
    "    node_array.append(node_line)\n",
    "\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        if j==H-1:tn.connect(node_array[i][j][2],node_array[i  ][0  ][0],f\"{i}{j}<->{i}{0}\")\n",
    "        else:     tn.connect(node_array[i][j][2],node_array[i  ][j+1][0],f\"{i}{j}<->{i}{j+1}\")\n",
    "        if i==W-1:tn.connect(node_array[i][j][3],node_array[0  ][j  ][1],f\"{i}{j}<->{0}{j}\")\n",
    "        else:     tn.connect(node_array[i][j][3],node_array[i+1][j  ][1],f\"{i}{j}<->{i+1}{j}\")\n",
    "\n",
    "node_list = [item for sublist in node_array for item in sublist]\n",
    "nodes = node_list\n",
    "input_sets = [set(node.edges) for node in nodes]\n",
    "output_set = get_subgraph_dangling(nodes)\n",
    "size_dict = {edge: edge.dimension for edge in get_all_edges(nodes)}\n",
    "\n",
    "operands = []\n",
    "for node,edge_label in zip(node_list,input_sets):\n",
    "    operands+=[node.tensor,[edge.name for edge in edge_label]]\n",
    "\n",
    "path,info = oe.contract_path(*operands)\n",
    "\n",
    "small_cores =[node.tensor for node in node_list]\n",
    "import tqdm as tqdm\n",
    "import cotengra as ctg\n",
    "\n",
    "sf = ctg.SliceFinder(info, target_size=2**27)\n",
    "inds_to_slice, cost_of_slicing = sf.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "import cotengra as ctg\n",
    "\n",
    "sf = ctg.SliceFinder(info, target_size=2**27)\n",
    "inds_to_slice, cost_of_slicing = sf.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67108864\n",
      "2.9753197715379116\n"
     ]
    }
   ],
   "source": [
    "print(cost_of_slicing.size    ) # the new largest intermediate\n",
    "print(cost_of_slicing.overhead)  # theoretical 'slowdown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/262144 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/262144 [00:00<30:05, 145.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 30/262144 [00:00<44:00, 99.28it/s] \u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 41/262144 [00:00<47:41, 91.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 51/262144 [00:00<49:42, 87.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 60/262144 [00:00<50:56, 85.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 69/262144 [00:00<51:48, 84.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 78/262144 [00:00<52:24, 83.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 87/262144 [00:00<53:13, 82.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 96/262144 [00:01<53:42, 81.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 105/262144 [00:01<53:43, 81.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 114/262144 [00:01<53:46, 81.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 123/262144 [00:01<53:46, 81.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 132/262144 [00:01<53:47, 81.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 141/262144 [00:01<53:47, 81.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 150/262144 [00:01<53:49, 81.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 159/262144 [00:01<53:48, 81.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 168/262144 [00:01<53:57, 80.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 177/262144 [00:02<54:10, 80.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 186/262144 [00:02<54:04, 80.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 195/262144 [00:02<54:04, 80.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 204/262144 [00:02<54:00, 80.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 213/262144 [00:02<54:03, 80.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 222/262144 [00:02<54:02, 80.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 231/262144 [00:02<53:58, 80.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 240/262144 [00:02<53:56, 80.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 249/262144 [00:02<54:02, 80.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 258/262144 [00:03<54:13, 80.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 267/262144 [00:03<54:05, 80.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 276/262144 [00:03<54:01, 80.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 285/262144 [00:03<54:05, 80.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 294/262144 [00:03<54:06, 80.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 303/262144 [00:03<54:04, 80.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 312/262144 [00:03<54:07, 80.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 321/262144 [00:03<54:03, 80.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 330/262144 [00:04<54:05, 80.68it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7c7e88620acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSlicedContractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmall_cores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 100%|██████████| 512/512 [00:55<00:00,  9.30it/s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-7c7e88620acd>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSlicedContractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmall_cores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontract_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 100%|██████████| 512/512 [00:55<00:00,  9.30it/s]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/cotengra/slicer.py\u001b[0m in \u001b[0;36mcontract_slice\u001b[0;34m(self, i, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m    585\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sliced_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *arrays, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contract_with_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_constants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_constants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_contract\u001b[0;34m(self, arrays, out, backend, evaluate_constants)\u001b[0m\n\u001b[1;32m    696\u001b[0m                               \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                               \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_constants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                               **self.einsum_kwargs)\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_contract_with_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_constants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# Contract!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0mnew_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtmp_operands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;31m# Build a new view if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/sharing.py\u001b[0m in \u001b[0;36mcached_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached_tensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcurrently_sharing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# hash based on the (axes_x,axes_y) form of axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/contract.py\u001b[0m in \u001b[0;36m_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \"\"\"\n\u001b[1;32m    373\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensordot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opt_einsum/backends/torch.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TORCH_HAS_TENSORDOT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mxnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, dims)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mdims_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mdims_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_b\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcartesian_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sc = sf.SlicedContractor([*small_cores])\n",
    "result = sum(sc.contract_slice(i) for i in tqdm.trange(sc.nslices))\n",
    "# 100%|██████████| 512/512 [00:55<00:00,  9.30it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import opt_einsum as oe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8), (0, 7), (0, 7), (0, 6), (0, 5), (0, 4), (0, 3), (1, 2), (0, 1), (0, 9), (0, 8), (0, 8), (0, 7), (0, 6), (1, 5), (1, 4), (1, 3), (1, 4), (3, 13), (7, 12), (3, 11), (3, 10), (3, 9), (3, 8), (6, 7), (3, 6), (3, 5), (3, 4), (2, 6), (2, 10), (1, 11), (4, 11), (0, 13), (5, 12), (5, 12), (6, 12), (5, 12), (6, 11), (6, 11), (6, 12), (4, 12), (6, 13), (7, 14), (7, 13), (4, 12), (6, 11), (6, 11), (6, 10), (6, 10), (0, 11), (0, 11), (5, 10), (0, 9), (4, 9), (2, 9), (1, 8), (3, 7), (3, 7), (3, 9), (3, 9), (3, 10), (3, 10), (3, 9), (3, 8), (1, 7), (2, 6), (6, 7), (2, 6), (2, 5), (2, 5), (5, 17), (6, 18), (12, 21), (4, 23), (13, 22), (4, 21), (15, 23), (10, 23), (10, 22), (7, 27), (11, 27), (11, 27), (11, 26), (7, 29), (9, 29), (8, 28), (15, 29), (7, 28), (13, 29), (6, 31), (6, 30), (13, 29), (16, 29), (1, 20), (21, 26), (21, 25), (6, 24), (15, 30), (15, 29), (8, 28), (10, 27), (6, 27), (10, 26), (8, 25), (9, 24), (5, 23), (2, 21), (10, 25), (5, 24), (2, 23), (1, 33), (1, 21), (0, 31), (2, 19), (4, 21), (5, 10), (7, 9), (4, 20), (1, 19), (0, 20), (14, 15), (6, 10), (3, 9), (2, 9), (5, 15), (0, 13), (6, 9), (5, 8), (4, 13), (3, 13), (1, 8), (1, 12), (0, 6), (1, 10), (3, 9), (7, 8), (3, 7), (0, 6), (2, 5), (3, 4), (2, 3), (0, 2), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(12,12,4,4,4,4)/2\n",
    "#tensor     = torch.randn(2,2,16,16,16,16)/10\n",
    "computer_vie_tn(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contractor Test perodic condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test_the_TRG_on_even_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Contractor2D/test_the_TRG_on_even_number.py\n"
     ]
    }
   ],
   "source": [
    "# #%%writefile Contractor2D/test_the_TRG_on_even_number.py\n",
    "# import torch\n",
    "# from Contractor2D.utils import apply_SVD\n",
    "\n",
    "# tensor = torch.randn(4,4,2,2,2,2)\n",
    "\n",
    "# print(uniform_shape_tensor_contractor_tn(tensor))\n",
    "\n",
    "# truncate= None\n",
    "# lu_tensor = tensor[0::2,0::2]\n",
    "# ld_tensor = tensor[0::2,1::2]\n",
    "# ru_tensor = tensor[1::2,0::2]\n",
    "# rd_tensor = tensor[1::2,1::2]\n",
    "# lu_lu,lu_rd = apply_SVD(lu_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk ,kcd\n",
    "# rd_lu,rd_rd = apply_SVD(rd_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk, kcd\n",
    "# ld_ld,ld_ru = apply_SVD(ld_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "# ru_ld,ru_ru = apply_SVD(ru_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "# tensor1     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                             lu_rd,\n",
    "#                             ld_ru,\n",
    "#                             rd_lu,\n",
    "#                             ru_ld)\n",
    "# tensor2     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                            rd_rd,\n",
    "#                            ru_ru.roll(-1,1),\n",
    "#                            lu_lu.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "#                            ld_ld.roll(-1,0))\n",
    "\n",
    "# print(tensor1_and_tensor2_contractor_tn(tensor1,tensor2))\n",
    "\n",
    "# #print(torch.einsum(\"abcd,cdab->\",tensor1[0,0],tensor2[0,0]))\n",
    "# left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "# lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# # new_tensor  = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "# #                            right,\n",
    "# #                            uppe,\n",
    "# #                            left.roll(shifts=-1,dims=0),\n",
    "# #                            lower.roll(shifts=1,dims=1))\n",
    "# new_tensor  = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\",\n",
    "#                            lower,\n",
    "#                            right.roll(-1,1),\n",
    "#                            uppe.roll(-1,1),\n",
    "#                            left.roll(shifts=(-1, -1), dims=(0, 1))\n",
    "#                            )\n",
    "# print(uniform_shape_tensor_contractor_tn(new_tensor))\n",
    "\n",
    "# torch.einsum(\"abn,nji,lkh,hdc,caw,wkj,ilm,mbd->\",\n",
    "#              lu_lu[0,0],lu_rd[0,0],\n",
    "#              rd_lu[0,0],rd_rd[0,0],\n",
    "#              ld_ld[0,0],ld_ru[0,0],\n",
    "#              ru_ld[0,0],ru_ru[0,0])\n",
    "\n",
    "# torch.einsum(\"abji,lkdc,jcak,dilb->\",\n",
    "#              torch.einsum(\"abn,nji->abji\",lu_lu[0,0],lu_rd[0,0]),\n",
    "#              torch.einsum(\"lkh,hdc->lkdc\",rd_lu[0,0],rd_rd[0,0]),\n",
    "#              torch.einsum(\"caw,wkj->jcak\",ld_ld[0,0],ld_ru[0,0]),\n",
    "#              torch.einsum(\"ilm,mbd->dilb\",ru_ld[0,0],ru_ru[0,0]))\n",
    "\n",
    "# torch.dist(torch.einsum(\"abn,nji->abji\",lu_lu[0,0],lu_rd[0,0]),lu_tensor)\n",
    "\n",
    "# torch.dist(torch.einsum(\"lkh,hdc->lkdc\",rd_lu[0,0],rd_rd[0,0]),rd_tensor)\n",
    "\n",
    "# torch.dist(torch.einsum(\"caw,wkj->jcak\",ld_ld[0,0],ld_ru[0,0]),ld_tensor)\n",
    "\n",
    "# torch.dist(torch.einsum(\"ilm,mbd->dilb\",ru_ld[0,0],ru_ru[0,0]),ru_tensor)\n",
    "\n",
    "# torch.dist(lu_tensor[0,0],tensor[0,0])\n",
    "\n",
    "# torch.dist(rd_tensor[0,0],tensor[1,1])\n",
    "\n",
    "# torch.dist(ld_tensor[0,0],tensor[0,1])\n",
    "\n",
    "# torch.dist(ru_tensor[0,0],tensor[1,0])\n",
    "\n",
    "# torch.einsum(\"abji,lkdc,jcak,dilb->\",\n",
    "#              torch.einsum(\"abn,nji->abji\",lu_lu[0,0],lu_rd[0,0]),\n",
    "#              torch.einsum(\"lkh,hdc->lkdc\",rd_lu[0,0],rd_rd[0,0]),\n",
    "#              torch.einsum(\"caw,wkj->jcak\",ld_ld[0,0],ld_ru[0,0]),\n",
    "#              torch.einsum(\"ilm,mbd->dilb\",ru_ld[0,0],ru_ru[0,0]))\n",
    "\n",
    "# torch.einsum(\"abcd,idjb,ckal,jlik->\",lu_tensor[0,0],ld_tensor[0,0],ru_tensor[0,0],rd_tensor[0,0])\n",
    "\n",
    "# torch.einsum(\"abcd,ciaj,ldkb,kjli->\",lu_tensor[0,0],ld_tensor[0,0],ru_tensor[0,0],rd_tensor[0,0])\n",
    "\n",
    "# tensor1 = tensor[0,0]\n",
    "# tensor2 = tensor[0,1]\n",
    "# tensor3 = tensor[1,0]\n",
    "# tensor4 = tensor[1,1]\n",
    "# torch.einsum(\"abcd,idjb,ckal,jlik->\",tensor1,tensor2,tensor3,tensor4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### check_the_brdc_coding_on_even_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Contractor2D/check_the_brdc_coding_on_even_number.py\n"
     ]
    }
   ],
   "source": [
    "# #%%writefile Contractor2D/check_the_brdc_coding_on_even_number.py\n",
    "# import torch\n",
    "# from Contractor2D.utils import apply_SVD\n",
    "\n",
    "# tensor     = torch.randn(8,8,2,2,2,2)/2\n",
    "# uniform_shape_tensor_contractor_tn(tensor)\n",
    "\n",
    "# lu_lu_origin,lu_rd_origin = apply_SVD(tensor[0::2,0::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk ,kcd\n",
    "# rd_lu_origin,rd_rd_origin = apply_SVD(tensor[1::2,1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld_origin,ld_ru_origin = apply_SVD(tensor[0::2,1::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# ru_ld_origin,ru_ru_origin = apply_SVD(tensor[1::2,0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "\n",
    "# lu_lu,lu_rd = apply_SVD(bulk_tensor[0::2,0::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk ,kcd\n",
    "# rd_lu,rd_rd = apply_SVD(bulk_tensor[1::2,1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld,ld_ru = apply_SVD(bulk_tensor[0::2,1::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# ru_ld,ru_ru = apply_SVD(bulk_tensor[1::2,0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "\n",
    "# print(lu_lu.shape)\n",
    "# print(rd_lu.shape)\n",
    "# print(ld_ld.shape)\n",
    "# print(ru_ld.shape)\n",
    "\n",
    "# lu_lu_should = lu_lu_origin[:,:]    ;lu_rd_should =lu_rd_origin[:,:]\n",
    "# rd_lu_should = rd_lu_origin[:-1,:-1];rd_rd_should =rd_rd_origin[:-1,:-1]\n",
    "# ld_ld_should = ld_ld_origin[:,:-1]  ;ld_ru_should =ld_ru_origin[:,:-1]\n",
    "# ru_ld_should = ru_ld_origin[:-1,:]  ;ru_ru_should =ru_ru_origin[:-1,:]\n",
    "\n",
    "# print(lu_lu_should.shape)\n",
    "# print(rd_lu_should.shape)\n",
    "# print(ld_ld_should.shape)\n",
    "# print(ru_ld_should.shape)\n",
    "\n",
    "# print(torch.dist(lu_lu_should,lu_lu),torch.dist(lu_rd_should,lu_rd))\n",
    "# print(torch.dist(rd_lu_should,rd_lu),torch.dist(rd_rd_should,rd_rd))\n",
    "# print(torch.dist(ld_ld_should,ld_ld),torch.dist(ld_ru_should,ld_ru))\n",
    "# print(torch.dist(ru_ld_should,ru_ld),torch.dist(ru_ru_should,ru_ru))\n",
    "\n",
    "# # right_edge == bulk_tensor[-1]\n",
    "# # down_edge  == bulk_tensor[:,-1]\n",
    "\n",
    "# rd_lu_r,rd_rd_r = apply_SVD(right_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ru_ld_r,ru_ru_r = apply_SVD(right_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_d,rd_rd_d = apply_SVD( down_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld_d,ld_ru_d = apply_SVD( down_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_c,rd_rd_c = apply_SVD(corn_tensor     ,left_bond=[0,1],right_bond=[2,3],truncate=None)# bck, kda\n",
    "\n",
    "# rd_lu_r_should=rd_lu_origin[-1,:-1];rd_rd_r_should=rd_rd_origin[-1,:-1]\n",
    "# ru_ld_r_should=ru_ld_origin[-1,:]  ;ru_ru_r_should=ru_ru_origin[-1,:]\n",
    "# rd_lu_d_should=rd_lu_origin[:-1,-1];rd_rd_d_should=rd_rd_origin[:-1,-1]\n",
    "# ld_ld_d_should=ld_ld_origin[:,-1]  ;ld_ru_d_should=ld_ru_origin[:,-1]\n",
    "# rd_lu_c_should=rd_lu_origin[ -1,-1];rd_rd_c_should=rd_rd_origin[ -1,-1]\n",
    "\n",
    "# print(rd_lu_r_should.shape)\n",
    "# print(ru_ld_r_should.shape)\n",
    "# print(rd_lu_d_should.shape)\n",
    "# print(ld_ld_d_should.shape)\n",
    "# print(rd_lu_c_should.shape)\n",
    "\n",
    "# print(torch.dist(rd_lu_r_should,rd_lu_r),torch.dist(rd_rd_r_should,rd_rd_r))\n",
    "# print(torch.dist(ru_ld_r_should,ru_ld_r),torch.dist(ru_ru_r_should,ru_ru_r))\n",
    "# print(torch.dist(rd_lu_d_should,rd_lu_d),torch.dist(rd_rd_d_should,rd_rd_d))\n",
    "# print(torch.dist(ld_ld_d_should,ld_ld_d),torch.dist(ld_ru_d_should,ld_ru_d))\n",
    "# print(torch.dist(rd_lu_c_should,rd_lu_c),torch.dist(rd_rd_c_should,rd_rd_c))\n",
    "\n",
    "# tensor1     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                                     lu_rd_origin,\n",
    "#                                     ld_ru_origin,\n",
    "#                                     rd_lu_origin,\n",
    "#                                     ru_ld_origin)\n",
    "# tensor2     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                            rd_rd_origin,\n",
    "#                            ru_ru_origin.roll(shifts=-1,dims=1),\n",
    "#                            lu_lu_origin.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "#                            ld_ld_origin.roll(shifts=-1,dims=0))\n",
    "\n",
    "# bulk_tensor1_should= tensor1[:-1,:-1]\n",
    "# rigt_tensor1_should= tensor1[-1,:-1]\n",
    "# down_tensor1_should= tensor1[:-1,-1]\n",
    "# corn_tensor1_should= tensor1[-1,-1]\n",
    "# bulk_tensor2_should= tensor2[:-1,:-1]\n",
    "# rigt_tensor2_should= tensor2[-1,:-1]\n",
    "# down_tensor2_should= tensor2[:-1,-1]\n",
    "# corn_tensor2_should= tensor2[-1,-1]\n",
    "\n",
    "# print(bulk_tensor1_should.shape)\n",
    "# print(rigt_tensor1_should.shape)  \n",
    "# print(down_tensor1_should.shape)  \n",
    "# print(corn_tensor1_should.shape)  \n",
    "# print(bulk_tensor2_should.shape)  \n",
    "# print(rigt_tensor2_should.shape)  \n",
    "# print(down_tensor2_should.shape)  \n",
    "# print(corn_tensor2_should.shape)  \n",
    "\n",
    "# bulk_tensor1 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",lu_rd[:-1,:-1],ld_ru[:-1],rd_lu,ru_ld[:,:-1])\n",
    "# rigt_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[-1,:-1],ld_ru[-1]   ,rd_lu_r, ru_ld_r[:-1])\n",
    "# down_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[:-1,-1],ld_ru_d[:-1],rd_lu_d, ru_ld[:,-1])\n",
    "# corn_tensor1 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",lu_rd[-1,-1] ,ld_ru_d[-1] ,rd_lu_c, ru_ld_r[-1])\n",
    "# bulk_tensor2 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",rd_rd,ru_ru[:,1:],lu_lu[1:,1:],ld_ld[1:])\n",
    "# rigt_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_r, ru_ru_r[1:],lu_lu[0,1:], ld_ld[0])\n",
    "# down_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_d, ru_ru[:,0],lu_lu[1:,0], ld_ld_d[1:])\n",
    "# corn_tensor2 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",rd_rd_c, ru_ru_r[0],lu_lu[0,0] , ld_ld_d[0])\n",
    "\n",
    "# print(bulk_tensor1.shape)\n",
    "# print(rigt_tensor1.shape)  \n",
    "# print(down_tensor1.shape)  \n",
    "# print(corn_tensor1.shape)  \n",
    "# print(bulk_tensor2.shape)  \n",
    "# print(rigt_tensor2.shape)  \n",
    "# print(down_tensor2.shape)  \n",
    "# print(corn_tensor2.shape)  \n",
    "\n",
    "# print(torch.dist(bulk_tensor1,bulk_tensor1_should))\n",
    "# print(torch.dist(rigt_tensor1,rigt_tensor1_should))  \n",
    "# print(torch.dist(down_tensor1,down_tensor1_should))  \n",
    "# print(torch.dist(corn_tensor1,corn_tensor1_should))  \n",
    "# print(torch.dist(bulk_tensor2,bulk_tensor2_should))  \n",
    "# print(torch.dist(rigt_tensor2,rigt_tensor2_should))  \n",
    "# print(torch.dist(down_tensor2,down_tensor2_should))  \n",
    "# print(torch.dist(corn_tensor2,corn_tensor2_should)) \n",
    "\n",
    "# computer_vie_tn(tensor)\n",
    "\n",
    "# truncate = None\n",
    "# bulk_left ,bulk_right= apply_SVD(bulk_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# rigt_left ,rigt_right= apply_SVD(rigt_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# down_left ,down_right= apply_SVD(down_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# corn_left ,corn_right= apply_SVD(corn_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# bulk_lower,bulk_uppe = apply_SVD(bulk_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# rigt_lower,rigt_uppe = apply_SVD(rigt_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# down_lower,down_uppe = apply_SVD(down_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# corn_lower,corn_uppe = apply_SVD(corn_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "\n",
    "# bulk_tensor           = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\", bulk_lower[:-1,:-1],bulk_right[:-1,1:],bulk_uppe[:-1,1:],bulk_left[1:,1:])\n",
    "# rigt_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , rigt_lower[:-1]    ,rigt_right[1:]    ,rigt_uppe[1:]    ,bulk_left[0,1:])\n",
    "# down_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , down_lower[:-1]    ,bulk_right[:-1,0] ,bulk_uppe[:-1,0] ,bulk_left[1:,0])\n",
    "# bulk_rigt_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[-1,:-1] ,bulk_right[-1,1:] ,bulk_uppe[-1,1:] ,rigt_left[1:])\n",
    "# bulk_down_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[:-1,-1] ,down_right[:-1]   ,down_uppe[:-1]   ,down_left[1:])\n",
    "# bulk_down_corn_tensor = torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , bulk_lower[-1,-1]  ,down_right[-1]    ,down_uppe[-1]    ,corn_left)\n",
    "# corn_right_bulk_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , corn_lower         ,rigt_right[0]     ,rigt_uppe[0]     ,bulk_left[0,0])\n",
    "# right_corn_down_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , rigt_lower[-1]     ,corn_right        ,corn_uppe        ,down_left[0])\n",
    "# down_bulk_right_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , down_lower[-1]     ,bulk_right[-1,0]  ,bulk_uppe[-1,0]  ,rigt_left[0])\n",
    "\n",
    "# left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "# lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# new_tensor_should  = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\",\n",
    "#                    lower,\n",
    "#                    right.roll(-1,1),\n",
    "#                    uppe.roll(-1,1),\n",
    "#                    left.roll(shifts=(-1, -1), dims=(0, 1))\n",
    "#                    )\n",
    "\n",
    "# print(bulk_tensor.shape)           \n",
    "# print(rigt_bulk_tensor.shape)      \n",
    "# print(down_bulk_tensor.shape)      \n",
    "# print(bulk_rigt_tensor.shape)      \n",
    "# print(bulk_down_tensor.shape)      \n",
    "# print(bulk_down_corn_tensor.shape) \n",
    "# print(corn_right_bulk_tensor.shape)\n",
    "# print(right_corn_down_tensor.shape)\n",
    "# print(down_bulk_right_tensor.shape)\n",
    "\n",
    "# bulk_tensor_should            = new_tensor_should[:-2, :-2]\n",
    "# rigt_bulk_tensor_should       = new_tensor_should[-1,:-2]\n",
    "# down_bulk_tensor_should       = new_tensor_should[:-2,-1]\n",
    "# bulk_rigt_tensor_should       = new_tensor_should[-2,:-2]\n",
    "# bulk_down_tensor_should       = new_tensor_should[:-2,-2]\n",
    "# bulk_down_corn_tensor_should  = new_tensor_should[-2,-2]\n",
    "# corn_right_bulk_tensor_should = new_tensor_should[-1,-1]\n",
    "# right_corn_down_tensor_should = new_tensor_should[-1,-2]\n",
    "# down_bulk_right_tensor_should = new_tensor_should[-2,-1]\n",
    "\n",
    "# print(bulk_tensor_should.shape           )\n",
    "# print(rigt_bulk_tensor_should.shape      )\n",
    "# print(down_bulk_tensor_should.shape      )\n",
    "# print(bulk_rigt_tensor_should.shape      )\n",
    "# print(bulk_down_tensor_should.shape      )\n",
    "# print(bulk_down_corn_tensor_should.shape )\n",
    "# print(corn_right_bulk_tensor_should.shape)\n",
    "# print(right_corn_down_tensor_should.shape)\n",
    "# print(down_bulk_right_tensor_should.shape)\n",
    "\n",
    "# print(torch.dist(bulk_tensor_should            ,bulk_tensor           ))\n",
    "# print(torch.dist(rigt_bulk_tensor_should       ,rigt_bulk_tensor      ))\n",
    "# print(torch.dist(down_bulk_tensor_should       ,down_bulk_tensor      ))\n",
    "# print(torch.dist(bulk_rigt_tensor_should       ,bulk_rigt_tensor      ))\n",
    "# print(torch.dist(bulk_down_tensor_should       ,bulk_down_tensor      ))\n",
    "# print(torch.dist(bulk_down_corn_tensor_should  ,bulk_down_corn_tensor ))\n",
    "# print(torch.dist(corn_right_bulk_tensor_should ,corn_right_bulk_tensor))\n",
    "# print(torch.dist(right_corn_down_tensor_should ,right_corn_down_tensor))\n",
    "# print(torch.dist(down_bulk_right_tensor_should ,down_bulk_right_tensor))\n",
    "\n",
    "# new_tensor = torch.cat(\n",
    "#     [torch.cat([bulk_tensor,bulk_rigt_tensor.unsqueeze(0),rigt_bulk_tensor.unsqueeze(0)]),\n",
    "#      torch.cat([bulk_down_tensor,bulk_down_corn_tensor.unsqueeze(0),right_corn_down_tensor.unsqueeze(0)]).unsqueeze(1),\n",
    "#      torch.cat([down_bulk_tensor,down_bulk_right_tensor.unsqueeze(0),corn_right_bulk_tensor.unsqueeze(0)]).unsqueeze(1),\n",
    "#     ],dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #%%writefile Contractor2D/test_for_TRG_odd_number.py\n",
    "# import torch\n",
    "\n",
    "# from Contractor2D.utils import apply_SVD\n",
    "\n",
    "# tensor = torch.randn(5,5,2,2,2,2)\n",
    "# print(f\"the input tensor network store in {tensor.shape}\")\n",
    "# print(f\"the ultimate result should be {uniform_shape_tensor_contractor_tn(tensor)}\")\n",
    "\n",
    "\n",
    "# right_edge1,right_edge2 = tensor[-2:]\n",
    "# right_edge = torch.einsum(\"wabcd,wedfg->waebcfg\",right_edge1,right_edge2).flatten(1,2).flatten(-3,-2)\n",
    "# down_edge1, down_edge2 = tensor[:-2,-2:].transpose(1,0)\n",
    "# corn_tensor1,corn_tensor2= right_edge[-2:]\n",
    "# bulk_tensor= tensor[:-2,:-2]\n",
    "# right_edge = right_edge[:-2]\n",
    "# down_edge  = torch.einsum(\"wabcd,wcefg->wabefdg\",down_edge1,down_edge2).flatten(2,3).flatten(-2,-1)\n",
    "# corn_tensor= torch.einsum(\"abcd,cefg->abefdg\",corn_tensor1,corn_tensor2).flatten(1,2).flatten(-2,-1)\n",
    "\n",
    "# print(f\"the input bulk_right_left_corner tensor :\")\n",
    "# print(f\"bulk_tensor.shape = {bulk_tensor.shape}\")\n",
    "# print(f\"right_edge.shape  = {right_edge.shape }\")\n",
    "# print(f\"down_edge.shape   = {down_edge.shape  }\")\n",
    "# print(f\"corn_tensor.shape = {corn_tensor.shape}\")\n",
    "\n",
    "# print(f\"the bulk_right_left_corner_contractor result {bulk_right_left_corner_contractor_tn(bulk_tensor,right_edge,down_edge,corn_tensor)}\")\n",
    "\n",
    "# lu_lu,lu_rd = apply_SVD(bulk_tensor[0::2,0::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk ,kcd\n",
    "# rd_lu,rd_rd = apply_SVD(bulk_tensor[1::2,1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld,ld_ru = apply_SVD(bulk_tensor[0::2,1::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# ru_ld,ru_ru = apply_SVD(bulk_tensor[1::2,0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "\n",
    "# rd_lu_r,rd_rd_r = apply_SVD(right_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ru_ld_r,ru_ru_r = apply_SVD(right_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_d,rd_rd_d = apply_SVD( down_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld_d,ld_ru_d = apply_SVD( down_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_c,rd_rd_c = apply_SVD(corn_tensor     ,left_bond=[0,1],right_bond=[2,3],truncate=None)# bck, kda\n",
    "\n",
    "# bulk_tensor1 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",lu_rd[:-1,:-1],ld_ru[:-1],rd_lu,ru_ld[:,:-1])\n",
    "# rigt_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[-1,:-1],ld_ru[-1]   ,rd_lu_r, ru_ld_r[:-1])\n",
    "# down_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[:-1,-1],ld_ru_d[:-1],rd_lu_d, ru_ld[:,-1])\n",
    "# corn_tensor1 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",lu_rd[-1,-1] ,ld_ru_d[-1] ,rd_lu_c, ru_ld_r[-1])\n",
    "# bulk_tensor2 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",rd_rd,ru_ru[:,1:],lu_lu[1:,1:],ld_ld[1:])\n",
    "# rigt_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_r, ru_ru_r[1:],lu_lu[0,1:], ld_ld[0])\n",
    "# down_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_d, ru_ru[:,0],lu_lu[1:,0], ld_ld_d[1:])\n",
    "# corn_tensor2 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",rd_rd_c, ru_ru_r[0],lu_lu[0,0] , ld_ld_d[0])\n",
    "\n",
    "# print(\"phase1:result\")\n",
    "# print(f\"bulk_tensor1.shape={bulk_tensor1.shape}\")\n",
    "# print(f\"rigt_tensor1.shape={rigt_tensor1.shape}\")\n",
    "# print(f\"down_tensor1.shape={down_tensor1.shape}\")\n",
    "# print(f\"corn_tensor1.shape={corn_tensor1.shape}\")\n",
    "# print()\n",
    "# print(f\"bulk_tensor2.shape={bulk_tensor2.shape}\")\n",
    "# print(f\"rigt_tensor2.shape={rigt_tensor2.shape}\")\n",
    "# print(f\"down_tensor2.shape={down_tensor2.shape}\")\n",
    "# print(f\"corn_tensor2.shape={corn_tensor2.shape}\")\n",
    "\n",
    "# # if we don't truncate at this step, the map will become too complex and can not code as unique way,\n",
    "# # so in such case, we will require truncate 16 in here and the recommend input not big than 32.\n",
    "# truncate = None\n",
    "# bulk_left ,bulk_right= apply_SVD(bulk_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# rigt_left ,rigt_right= apply_SVD(rigt_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# down_left ,down_right= apply_SVD(down_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# corn_left ,corn_right= apply_SVD(corn_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# bulk_lower,bulk_uppe = apply_SVD(bulk_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# rigt_lower,rigt_uppe = apply_SVD(rigt_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# down_lower,down_uppe = apply_SVD(down_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# corn_lower,corn_uppe = apply_SVD(corn_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "\n",
    "# bulk_tensor_cent      = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\", bulk_lower[:-1,:-1],bulk_right[:-1,1:],bulk_uppe[:-1,1:],bulk_left[1:,1:])\n",
    "# rigt_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , rigt_lower[:-1]    ,rigt_right[1:]    ,rigt_uppe[1:]    ,bulk_left[0,1:])\n",
    "# down_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , down_lower[:-1]    ,bulk_right[:-1,0] ,bulk_uppe[:-1,0] ,bulk_left[1:,0])\n",
    "# bulk_rigt_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[-1,:-1] ,bulk_right[-1,1:] ,bulk_uppe[-1,1:] ,rigt_left[1:])\n",
    "# bulk_down_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[:-1,-1] ,down_right[:-1]   ,down_uppe[:-1]   ,down_left[1:])\n",
    "# bulk_down_corn_tensor = torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , bulk_lower[-1,-1]  ,down_right[-1]    ,down_uppe[-1]    ,corn_left)\n",
    "# corn_right_bulk_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , corn_lower         ,rigt_right[0]     ,rigt_uppe[0]     ,bulk_left[0,0])\n",
    "# right_corn_down_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , rigt_lower[-1]     ,corn_right        ,corn_uppe        ,down_left[0])\n",
    "# down_bulk_right_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , down_lower[-1]     ,bulk_right[-1,0]  ,bulk_uppe[-1,0]  ,rigt_left[0])\n",
    "# # down_bulk_tensor       = down_bulk_tensor[None]\n",
    "# # down_bulk_right_tensor = down_bulk_right_tensor[None]\n",
    "# # corn_right_bulk_tensor = corn_right_bulk_tensor[None]\n",
    "# # bulk_down_corn_tensor  = bulk_down_corn_tensor[None]\n",
    "\n",
    "# print(\"phase2 result:\")\n",
    "# print(f\"bulk_tensor_cent.shape      ={bulk_tensor_cent.shape      }\")\n",
    "# print(f\"down_bulk_tensor.shape      ={down_bulk_tensor.shape      }\")      \n",
    "# print(f\"bulk_rigt_tensor.shape      ={bulk_rigt_tensor.shape      }\")   \n",
    "# print(f\"down_bulk_right_tensor.shape={down_bulk_right_tensor.shape}\")\n",
    "# print(f\"rigt_bulk_tensor.shape      ={rigt_bulk_tensor.shape      }\")      \n",
    "# print(f\"corn_right_bulk_tensor.shape={corn_right_bulk_tensor.shape}\")\n",
    "# print(f\"bulk_down_tensor.shape      ={bulk_down_tensor.shape      }\") \n",
    "# print(f\"bulk_down_corn_tensor.shape ={bulk_down_corn_tensor.shape }\") \n",
    "# print(f\"right_corn_down_tensor.shape={right_corn_down_tensor.shape}\")\n",
    "\n",
    "# bulk_tensor = torch.cat([\n",
    "#     torch.cat([bulk_tensor_cent,down_bulk_tensor[None]],1),\n",
    "#     torch.cat([bulk_rigt_tensor,down_bulk_right_tensor[None]])[None]\n",
    "# ])\n",
    "\n",
    "# bulk_tensor.shape\n",
    "\n",
    "# bulk_tensor = down_bulk_right_tensor[None][None]\n",
    "\n",
    "# right_edge = torch.cat([rigt_bulk_tensor,corn_right_bulk_tensor[None]])\n",
    "# down_edge  = torch.cat([bulk_down_tensor,bulk_down_corn_tensor[None]])\n",
    "# corn_tensor =right_corn_down_tensor\n",
    "\n",
    "# print(f\"next tensor shape:\")\n",
    "# print(f\"bulk_tensor.shape = {bulk_tensor.shape}\")\n",
    "# print(f\"right_edge.shape  = {right_edge.shape }\")\n",
    "# print(f\"down_edge.shape   = {down_edge.shape  }\")\n",
    "# print(f\"corn_tensor.shape = {corn_tensor.shape}\")\n",
    "\n",
    "# print(f\"next tensor contraction result:\")\n",
    "# print(bulk_right_left_corner_contractor_tn(bulk_tensor,right_edge,down_edge,corn_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### time test for different engine (only bulk $2^n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch_semiring_einsum\n",
    "# def TRT_semiring(tensor,truncate=None, block_size=5):\n",
    "#     W,H = tensor.shape[:2]\n",
    "#     einsum_1 = torch_semiring_einsum.compile_equation(\"whicd,whjac,whbak,whdbl->whijkl\")\n",
    "#     einsum_2 = torch_semiring_einsum.compile_equation(\"whadi,whjba,whkcb,whdcl->whijkl\")\n",
    "#     einsum_3 = torch_semiring_einsum.compile_equation(\"abcd,ciaj,ldkb,kjli->\")\n",
    "#     while True:\n",
    "#         print(tensor.shape)\n",
    "#         W,H = tensor.shape[:2]\n",
    "#         #if W<=1 or H<=1:break\n",
    "#         lu_tensor = tensor[0::2,0::2]\n",
    "#         ld_tensor = tensor[0::2,1::2]\n",
    "#         ru_tensor = tensor[1::2,0::2]\n",
    "#         rd_tensor = tensor[1::2,1::2]\n",
    "#         if W<=2 or H<=2:break\n",
    "            \n",
    "#         lu_lu,lu_rd = apply_SVD(lu_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk ,kcd\n",
    "#         rd_lu,rd_rd = apply_SVD(rd_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk, kcd\n",
    "#         ld_ld,ld_ru = apply_SVD(ld_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "#         ru_ld,ru_ru = apply_SVD(ru_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "#         tensor1     = torch_semiring_einsum.einsum(einsum_1,\n",
    "#                                     lu_rd,\n",
    "#                                     ld_ru,\n",
    "#                                     rd_lu,\n",
    "#                                     ru_ld,block_size=block_size)\n",
    "#         tensor2     = torch_semiring_einsum.einsum(einsum_1,\n",
    "#                                    rd_rd,\n",
    "#                                    ru_ru.roll(shifts=-1,dims=1),\n",
    "#                                    lu_lu.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "#                                    ld_ld.roll(shifts=-1,dims=0),block_size=block_size)\n",
    "#         left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "#         lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "#         tensor  = torch_semiring_einsum.einsum(einsum_2,\n",
    "#                            lower,\n",
    "#                            right.roll(-1,1),\n",
    "#                            uppe.roll(-1,1),\n",
    "#                            left.roll(shifts=(-1, -1), dims=(0, 1)),block_size=block_size\n",
    "#                            )\n",
    "#     value   = torch_semiring_einsum.einsum(einsum_3,\n",
    "#                            lu_tensor[0,0],ld_tensor[0,0],\n",
    "#                            ru_tensor[0,0],rd_tensor[0,0],block_size=block_size)\n",
    "#     #value = torch.einsum(\"abab->\",tensor[0,0])\n",
    "#     return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.randn(14,14,2,2,2,2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def TRT(tensor,truncate=None,einsum_engin=torch.einsum):\n",
    "    W,H = tensor.shape[:2]\n",
    "    while True:\n",
    "        print(tensor.shape)\n",
    "        W,H = tensor.shape[:2]\n",
    "        #if W<=1 or H<=1:break\n",
    "        lu_tensor = tensor[0::2,0::2]\n",
    "        ld_tensor = tensor[0::2,1::2]\n",
    "        ru_tensor = tensor[1::2,0::2]\n",
    "        rd_tensor = tensor[1::2,1::2]\n",
    "        if W<=2 or H<=2:break\n",
    "#         print(\"SVD:time\"),\n",
    "#         start_time  = time.time()\n",
    "        lu_lu,lu_rd = apply_SVD(lu_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk ,kcd\n",
    "        rd_lu,rd_rd = apply_SVD(rd_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk, kcd\n",
    "        ld_ld,ld_ru = apply_SVD(ld_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "        ru_ld,ru_ru = apply_SVD(ru_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)       \n",
    "#         print(\"einsum:time\"),\n",
    "#         start_time  = time.time()\n",
    "        tensor1     = einsum_engin(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "                                    lu_rd,\n",
    "                                    ld_ru,\n",
    "                                    rd_lu,\n",
    "                                    ru_ld)\n",
    "        tensor2     = einsum_engin(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "                                   rd_rd,\n",
    "                                   ru_ru.roll(shifts=-1,dims=1),\n",
    "                                   lu_lu.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "                                   ld_ld.roll(shifts=-1,dims=0))\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)\n",
    "#         print(\"SVD:time\"),\n",
    "#         start_time  = time.time()\n",
    "        \n",
    "        left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "        lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)\n",
    "#         print(\"einsum:time\"),\n",
    "#         start_time  = time.time()\n",
    "        tensor  = einsum_engin(\"whadi,whjba,whkcb,whdcl->whijkl\",\n",
    "                           lower,\n",
    "                           right.roll(-1,1),\n",
    "                           uppe.roll(-1,1),\n",
    "                           left.roll(shifts=(-1, -1), dims=(0, 1))\n",
    "                           )\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)\n",
    "    value   = einsum_engin(\"abcd,ciaj,ldkb,kjli->\",lu_tensor[0,0],ld_tensor[0,0],ru_tensor[0,0],rd_tensor[0,0])\n",
    "    #value = torch.einsum(\"abab->\",tensor[0,0])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.randn(14,14,2,2,2,2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "engine=lambda equ,*karg:torch_semiring_einsum.einsum(torch_semiring_einsum.compile_equation(equ),*karg,block_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 ms ± 10 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(4,4,2,2,2,2)\n",
    "%timeit TRT(tensor,truncate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.67 ms ± 1.43 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit TRT(tensor,truncate=None,einsum_engin=contract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Boundary MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=4;P=4;L=7;\n",
    "# top_mps_list    = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]\n",
    "# middle_mpo_list = [[rd_engine(P,D,P)]+[rd_engine(L-2,D,P,D,P)]+ [rd_engine(D,P,P)]\n",
    "#                   for _ in range(L-2)]\n",
    "# bottom_mps_list = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]\n",
    "# peps  = [top_mps_list]+middle_mpo_list+[bottom_mps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=4;P=4;L=7;\n",
    "top_mps_list    = [rd_engine(D,D)]     + [rd_engine(L-2,D,D,D)]  + [rd_engine(D,D)]\n",
    "middle_mpo_list = [[rd_engine(D,D,D)]  + [rd_engine(L-2,D,D,D,D)]+ [rd_engine(D,D,D)]\n",
    "                  for _ in range(L-2)]\n",
    "bottom_mps_list = [rd_engine(D,D)]     + [rd_engine(L-2,D,D,D)]  + [rd_engine(D,D)]\n",
    "peps  = [top_mps_list]+middle_mpo_list+[bottom_mps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     3,
     64,
     94,
     127,
     131,
     159
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from engine.torch_dense import approxmate_mps_line\n",
    "import torch\n",
    "import numpy as np\n",
    "def truncated_SVD(tensor,output='RQ',max_singular_values= None,\n",
    "                  max_truncation_error= None,\n",
    "                  relative = True,\n",
    "                  normlized= True,\n",
    "                  verbose  = False,auto_check_diagonal=False):\n",
    "    # the canonocal\n",
    "    # tensor is batched\n",
    "    reduce = False\n",
    "    u=s=v = None\n",
    "    if len(tensor.shape)==2:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        reduce = True\n",
    "    if auto_check_diagonal:\n",
    "        out = diagonal_tensor_svd_torch_dense(tensor)\n",
    "        if out is not None:u, s, v = out\n",
    "    if u is None: u, s, v = torch.svd(tensor)\n",
    "\n",
    "\n",
    "    max_singular_values = s.shape[-1] if max_singular_values is None else max_singular_values\n",
    "\n",
    "    if max_truncation_error is not None:\n",
    "        # Cumulative norms of singular values in ascending order\n",
    "        s_sorted, _ = torch.sort(s**2,-1)\n",
    "        trunc_errs  = torch.sqrt(torch.cumsum(s_sorted, -1))\n",
    "        # If relative is true, rescale max_truncation error with the largest\n",
    "        # singular value to yield the absolute maximal truncation error.\n",
    "        abs_max_truncation_error = max_truncation_error * s[:,0:1] if relative else max_truncation_error\n",
    "        # We must keep at least this many singular values to ensure the\n",
    "        # truncation error is <= abs_max_truncation_error.\n",
    "        num_sing_vals_err = torch.sum(trunc_errs > abs_max_truncation_error,-1).max()\n",
    "        if max_singular_values>num_sing_vals_err and verbose:\n",
    "            print(f\"use {num_sing_vals_err}/{max_singular_values} sing vals\")\n",
    "    else:\n",
    "        num_sing_vals_err  = max_singular_values\n",
    "\n",
    "    num_sing_vals_keep = min(max_singular_values, num_sing_vals_err)\n",
    "\n",
    "\n",
    "    #s_rest = s[...,num_sing_vals_keep:]\n",
    "    u      = u[...,:num_sing_vals_keep]\n",
    "    s      = s[...,:num_sing_vals_keep]\n",
    "    v      = v[...,:num_sing_vals_keep]\n",
    "    v      = torch.transpose(v, -1, -2)#vh\n",
    "\n",
    "    if num_sing_vals_keep == s.shape[-1] and normlized:\n",
    "        Z = 1.0*torch.ones(s.shape[0])\n",
    "    else:\n",
    "        Z = torch.sum(s**2,-1).sqrt()\n",
    "\n",
    "    if output == 'RQ':\n",
    "        R = torch.einsum('iab,ibc->iac',u ,torch.diag_embed(s))\n",
    "        Q = v\n",
    "        output = [R,Q,Z]\n",
    "    elif output == 'QR':\n",
    "        Q = u\n",
    "        R = torch.einsum('iab,ibc->iac',torch.diag_embed(s),v)\n",
    "        output = [Q,R,Z]\n",
    "    else:\n",
    "        output = [u,s,v,Z]\n",
    "    if reduce:output = [t[0] for t in output]\n",
    "    return output\n",
    "def left_canonicalize_MPS(mps_line,Decomposition_Engine=torch.qr,\n",
    "                          normlization =True):\n",
    "    # for any not canonical mps line\n",
    "    # the chain size (D,P,D)\n",
    "    new_chain = []\n",
    "    R         = None\n",
    "    #Z_list    = []# record the scale information for each unit.\n",
    "    # for a perfect MPS state, we expect the norm for each tensor is 1.\n",
    "    for i,tensor in enumerate(mps_line):\n",
    "        if len(tensor.shape)==2:\n",
    "            new_tensor = torch.einsum('ab,bd->ad',R,tensor) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "        elif len(tensor.shape)==3:\n",
    "            new_tensor = torch.einsum('ab,bcd->acd',R,tensor) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "            a,b,c = shape\n",
    "            new_tensor = new_tensor.reshape(a*b,c)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if i == len(mps_line) - 1:\n",
    "            Z = torch.norm(new_tensor)\n",
    "            if normlization:new_tensor /= (Z)\n",
    "            new_chain.append(new_tensor.reshape(*shape[:-1],-1))\n",
    "        else:\n",
    "            Q,R = Decomposition_Engine(new_tensor)[:2]\n",
    "            Q   = Q.reshape(*shape[:-1],-1)\n",
    "            new_chain.append(Q)\n",
    "\n",
    "    return new_chain,[Z]\n",
    "def right_canonicalize_MPS(mps_line,Decomposition_Engine=truncated_SVD,\n",
    "                          #normlization =True\n",
    "                          ):\n",
    "    new_chain = []\n",
    "    R         = None\n",
    "    Z_list    = []\n",
    "    svd_Z         = torch.Tensor([1.0])#input has already been normalized\n",
    "    for i,tensor in enumerate(mps_line[::-1]):\n",
    "\n",
    "        if len(tensor.shape)==2:\n",
    "            new_tensor = torch.einsum('ab,bc->ac',tensor, R) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "        elif len(tensor.shape)==3:\n",
    "            new_tensor = torch.einsum('alb,bc->alc',tensor, R) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "            a,b,c      = shape\n",
    "            new_tensor = new_tensor.reshape(a,b*c)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        Z = torch.norm(new_tensor)\n",
    "        Z_list.append(Z)\n",
    "        new_tensor /= Z\n",
    "        # normlization is necessary; directly use the SVD_Z may cause problem due to precision\n",
    "        #print(f\"svd_Z{svd_Z.item()}<->all_Z {Z.item()} <-> after_Z{torch.norm(new_tensor).item()}\")\n",
    "        #if normlization:new_tensor /= Z\n",
    "        if i == len(mps_line) - 1:\n",
    "            new_chain.append(new_tensor.reshape(-1,*shape[1:]))\n",
    "        else:\n",
    "            R,Q,svd_Z = Decomposition_Engine(new_tensor)\n",
    "            Q   = Q.reshape(-1,*shape[1:])\n",
    "            new_chain.append(Q)\n",
    "    new_chain=new_chain[::-1]\n",
    "    return new_chain,Z_list\n",
    "def torchrq(tensor):\n",
    "    q, r = torch.qr(torch.transpose(tensor, -2, -1))\n",
    "    r, q = torch.transpose(r, -2, -1), torch.transpose(q, -2, -1)  #M=r*q at this point\n",
    "    return r,qx\n",
    "def approxmate_mps_line(mps_line,\n",
    "                        max_singular_values= None,max_truncation_error= None,relative = True,\n",
    "                        mode='full',left_method='qr'\n",
    "                       ):\n",
    "\n",
    "\n",
    "    scalar = 1\n",
    "    if mode != 'right':\n",
    "        if left_method == 'qr':\n",
    "            DCEngine = torch.linalg.qr if float(torch.__version__[:4])>1.07 else torch.qr\n",
    "        elif left_method == 'svd':\n",
    "            DCEngine = lambda x:truncated_SVD(x,output='QR')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        mps_line,Z_list = left_canonicalize_MPS(mps_line,Decomposition_Engine=DCEngine)\n",
    "        #print(get_mps_size_list(mps_line))\n",
    "        print(f\"   left canonical scalar:{np.prod(Z_list)}\")\n",
    "        scalar *= np.prod(Z_list)\n",
    "        #print(f\"now tensor norm: {torch.norm(mps_line[-1])}\")\n",
    "    SVD_Engine = lambda x:truncated_SVD(x,max_singular_values = max_singular_values,\n",
    "                                          max_truncation_error= max_truncation_error,\n",
    "                                          relative = relative)\n",
    "    mps_line,Z_list = right_canonicalize_MPS(mps_line,Decomposition_Engine=SVD_Engine)\n",
    "    #print(get_mps_size_list(mps_line))\n",
    "    #print(f\"   right canonical Z:{[np.round(t.item(),3) for t in Z_list]}\")\n",
    "    print(f\"   right canonical scalar:{np.prod(Z_list)}\")\n",
    "    scalar *= np.prod(Z_list)\n",
    "    return mps_line,scalar\n",
    "def diagonal_tensor_svd_torch_dense(tensor):\n",
    "    # support batch tensor\n",
    "    reduce = False\n",
    "    if len(tensor.shape)==2:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        reduce = True\n",
    "    W,H   = tensor.shape[-2:]\n",
    "    batch_shape= tensor.shape[:-2]\n",
    "    u = s = v = None\n",
    "    if W>=H:\n",
    "        batch_diag = torch.matmul(tensor.transpose(-1,-2),tensor)#auto broadcast, or can use bmm\n",
    "    else:\n",
    "        batch_diag = torch.matmul(tensor,tensor.transpose(-1,-2))#auto broadcast, or can use bmm\n",
    "    diagnol_num = torch.diagonal(a,dim1=-2,dim2=-1).nelement()\n",
    "    tensor_num  = tensor.nelement()\n",
    "    if diagnol_num != tensor_num:return None\n",
    "    A,A        = batch_diag.shape[-2:]\n",
    "    batch_diag = batch_diag[...,range(A),range(A)]\n",
    "    batch_diag,batch_order= batch_diag.sort(-1,descending=True)\n",
    "    #fast_V      = [get_sort_matrix(order).to_dense() for order in batch_order]\n",
    "    batch_order = batch_order.flatten(start_dim=0,end_dim=-2)\n",
    "    K,A         = batch_order.shape\n",
    "    s           = batch_diag.sqrt()\n",
    "    s           = s.reshape(*batch_shape,A)\n",
    "    if W>=H:\n",
    "        v = torch.sparse_coo_tensor([list(range(K*A)),batch_order.flatten().tolist()], [1.0]*K*A,(K*A,A))\n",
    "        v = v.to_dense().reshape(-1,A,A)\n",
    "        u = torch.bmm(tensor,v.transpose(-1,-2)/s.unsqueeze(-2))\n",
    "        v = v.reshape(*batch_shape,A,A)\n",
    "        u = u.reshape(*batch_shape,W,A)\n",
    "    else:\n",
    "        u = torch.sparse_coo_tensor([list(range(K*A)),batch_order.flatten().tolist()], [1.0]*K*A,(K*A,A))\n",
    "        u = u.to_dense().reshape(-1,A,A).transpose(-1,-2)\n",
    "        v = torch.bmm(u.transpose(-1,-2)/s.unsqueeze(-1),tensor)#TODO: case when s==0\n",
    "        u = u.reshape(*batch_shape,A,A)\n",
    "        v = v.reshape(*batch_shape,A,H)\n",
    "    output = [u,s,v.transpose(-1,-2)]\n",
    "    if reduce:\n",
    "        output = [t[0] for t in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0+cu102'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left canonical scalar:0.5324269533157349\n",
      "   right canonical scalar:0.9999996423721313\n",
      "(4, 4)-(4, 4, 16)- 3x(16, 4, 16) -(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.17249836027622223\n",
      "   right canonical scalar:0.9999997615814209\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.141238734126091\n",
      "   right canonical scalar:0.9999996423721313\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.13460852205753326\n",
      "   right canonical scalar:0.9999997019767761\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.1684599667787552\n",
      "   right canonical scalar:1.000000238418579\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n"
     ]
    }
   ],
   "source": [
    "tensor = peps[0]\n",
    "for mpo in peps[1:-1]:\n",
    "    tensor = contract_mps_mpo(tensor,mpo)\n",
    "    #print(get_mps_size_list(tensor))\n",
    "    tensor = right_mps_form(tensor)\n",
    "    tensor,scale = approxmate_mps_line(tensor,max_singular_values=100)\n",
    "    print(get_mps_size_list(tensor))\n",
    "tensor = contract_two_mps_tn(tensor,peps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def onebyoneBMPS(tensor,truncate=None,einsum_engin=torch.einsum):\n",
    "    W,H = tensor.shape[:2]\n",
    "    while W > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Batch Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from utils import *\n",
    "\n",
    "D=10\n",
    "P=4\n",
    "L=14\n",
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    return x\n",
    "def generate_test_data(L=10,num=20,k=2):\n",
    "    #images,labels = iter(train_loader).next()\n",
    "    #inputs = preprocess_sum_one(images)\n",
    "    inputs = rd_engine(L,num,k)\n",
    "    inputs = inputs.permute(1,2,0)#(B,num,k)->(num,k,B)\n",
    "    inputs = torch.diag_embed(inputs)#(num,k,B)->(num,k,B,B)\n",
    "    inputs = inputs.permute(0,2,1,3)#(num,k,B,B)->(num,B,k,B)\n",
    "    #inputs= [v for v in inputs]\n",
    "    #inputs[0]= torch.diagonal(inputs[0], dim1=0, dim2=-1)#(B,k,B) -> #(k,B)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.935704472353725e-15\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(result_should-the_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "mnist_data = np.load('archive/tn-for-unsup-ml/data/binarized_mnist.npz')\n",
    "train_data = torch.from_numpy(mnist_data['train_data'])\n",
    "test_data  = torch.from_numpy(mnist_data['test_data'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=(0.0,), std=(1.0,))\n",
    "])\n",
    "DATAPATH    = '/media/tianning/DATA/DATASET/MNIST/'\n",
    "mnist_train = datasets.MNIST(DATAPATH, train=True, download=False, transform=transform)\n",
    "mnist_test  = datasets.MNIST(DATAPATH, train=False,download=False, transform=transform)\n",
    "train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=1000, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test , batch_size=1000, shuffle=False)\n",
    "images,labels = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "now_result = np.einsum(\"abc,cd,de,ef,fg,gh,hi,ijk->abjk\",\n",
    "                       imps.get_tensor(len(imps) - 1), \n",
    "                       inv_sqrtl, \n",
    "                       U, \n",
    "                       np.sqrt(lam),np.sqrt(lam), \n",
    "                       V, \n",
    "                       inv_sqrtr, \n",
    "                       imps.tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(images1,images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.051250619959221\n",
      "1.5940558554691758e-14\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(now_result-result_should))\n",
    "print(np.linalg.norm(now_result-the_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### torch.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a,X,U,S,V,Y,b = canonicalize(imps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from engine.torch_dense import *\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "left  = np.einsum('ea,ab,bc->ec',X,U,np.sqrt(S)).real\n",
    "right = np.einsum('ea,ab,bc->ec',np.sqrt(S),V,Y).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.35 +0.j, -1.134+0.j,  0.117+0.j,  2.915+0.j],\n",
       "       [ 1.193+0.j,  1.376+0.j, -3.855+0.j,  0.129+0.j],\n",
       "       [-1.92 +0.j,  2.322+0.j,  0.48 +0.j,  0.344+0.j],\n",
       "       [ 2.836+0.j,  0.947+0.j,  0.577+0.j, -0.068+0.j]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ab,ca->bc',X,Y).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### diagonal.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from engine.sparse import *\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imps = FiniteMPS.random(d=[3,3]*5,\n",
    "      D=[2]*9,\n",
    "      dtype=np.float64)\n",
    "imps.canonicalize()\n",
    "imps.position(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "L=10\n",
    "idx1     = list(range(2*L))\n",
    "idx2     = [i for i in range(L) for j in range(2)]\n",
    "mps_unit = sparse.COO([idx1,idx2],np.random.randn(2*L),(2*L,L)).reshape((L,L*2))\n",
    "R,Q = diagonal_tensor_RQ(mps_unit)\n",
    "print(R.shape)\n",
    "print(Q.shape)\n",
    "# u0,s0,v0 = diagonal_tensor_svd_sparse_2D(mps_unit)\n",
    "# u1,s1,v1 = np.linalg.svd(mps_unit.todense(),full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=np.random.randint(3,(100,100)).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save(\"tttest\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b=np.load(\"tttest.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 43], dtype=uint8)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "code_folding": [
     1,
     3,
     8,
     14,
     39
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Efficient_Sparse_Matrix_List_Saver:\n",
    "    def __init__(self,dtype = 'sparse'):\n",
    "        self.dtype = dtype\n",
    "    def save(self,sparse_matrix_list,save_dir):\n",
    "        if self.dtype == 'sparse':\n",
    "            self.save_sparse_data(sparse_matrix_list,save_dir)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def load(self,save_dir):\n",
    "        if self.dtype == 'sparse':\n",
    "            return self.load_sparse_data(save_dir)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    @staticmethod \n",
    "    def save_sparse_data(sparse_matrix_list,save_dir):\n",
    "        max_shape_len = max([len(t.shape) for t in sparse_matrix_list])\n",
    "        save_indexes  = []\n",
    "        save_shapes   = []\n",
    "        for i in range(len(sparse_matrix_list)):\n",
    "            save_index = sparse_matrix_list[i].coords.transpose()\n",
    "            save_shape = list(sparse_matrix_list[i].shape)\n",
    "            if len(save_shape)< max_shape_len:\n",
    "                padding     = max_shape_len-len(save_shape)\n",
    "                save_index  = np.pad(save_index,[[0,0],[0,padding]])\n",
    "                save_shape  = save_shape+[1]*padding\n",
    "            save_indexes.append(save_index) \n",
    "            save_shapes.append(save_shape)\n",
    "        all_indexs  = np.concatenate(save_indexes)\n",
    "        all_values  = np.concatenate([t.data for t in sparse_matrix_list])\n",
    "        all_shape   = np.stack(save_shapes)\n",
    "        all_idx_size= np.array([len(t.shape) for t in sparse_matrix_list])\n",
    "        nnz_list    = np.array([t.nnz for t in sparse_matrix_list])\n",
    "        assert sum(nnz_list) == len(all_indexs) == len(all_values)\n",
    "        np.save(os.path.join(save_dir,\"all_indexs\"),all_indexs)\n",
    "        np.save(os.path.join(save_dir,\"all_idx_size\"),all_idx_size)\n",
    "        np.save(os.path.join(save_dir,\"all_values\"),all_values)\n",
    "        np.save(os.path.join(save_dir,\"all_shape\"),all_shape)\n",
    "        np.save(os.path.join(save_dir,\"nnz_list\"),nnz_list)\n",
    "    @staticmethod\n",
    "    def load_sparse_data(save_dir):\n",
    "        all_indexs   = np.load(os.path.join(save_dir,\"all_indexs.npy\"))\n",
    "        all_idx_size = np.load(os.path.join(save_dir,\"all_idx_size.npy\"))\n",
    "        all_values   = np.load(os.path.join(save_dir,\"all_values.npy\"))\n",
    "        all_shape    = np.load(os.path.join(save_dir,\"all_shape.npy\"))\n",
    "        nnz_list     = np.load(os.path.join(save_dir,\"nnz_list.npy\"))\n",
    "        \n",
    "        sparse_matrix_list =[]\n",
    "        start = 0 \n",
    "        for nnz,sz,shape in zip(nnz_list,all_idx_size,all_shape):\n",
    "            indexs = all_indexs[start:start+nnz][...,:sz].transpose()\n",
    "            values = all_values[start:start+nnz]\n",
    "            shape  = shape[:sz]\n",
    "            tensor = sparse.COO(indexs,values,shape.tolist())\n",
    "            start  = start+nnz\n",
    "            sparse_matrix_list.append(tensor)\n",
    "        return sparse_matrix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L=10\n",
    "idx1     = list(range(2*L))\n",
    "idx2     = [i for i in range(L) for j in range(2)]\n",
    "#mps_unit = sparse.COO([idx1,idx2],np.random.randn(2*L),(2*L,L)).reshape((L,2,L))\n",
    "mps_line = ([sparse.as_coo(np.random.randn(2,L))]+\n",
    "            [sparse.COO([idx1,idx2],np.random.randn(2*L),(2*L,L)).reshape((L,2,L))\n",
    "               for i in range(9)])\n",
    "#mps_line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=1000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test , batch_size=1000, shuffle=False)\n",
    "images,labels = iter(train_loader).next()\n",
    "origin_inputs = preprocess_sum_one(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "L=origin_inputs.shape[0]\n",
    "idx1     = list(range(2*L))\n",
    "idx2     = [i for i in range(L) for j in range(2)]\n",
    "mps_line=[sparse.as_coo(origin_inputs[:,0,:].transpose(1,0).numpy())]\n",
    "for tensor in origin_inputs.permute(1,0,2)[1:]:\n",
    "    #print(tensor.flatten().numpy().shape)\n",
    "    mps_line.append(sparse.COO([idx1,idx2],tensor.flatten().numpy(),(2*L,L)).reshape((L,2,L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next_line,z = right_canonicalize_MPS_sparse(mps_line,final_normlization =True,all_renormlization=False)\n",
    "\n",
    "# Decomposition_Engine=lambda x:truncated_SVD_sparse(x,output='QR',\n",
    "#                                                     max_truncation_error=0.00,\n",
    "#                                                     max_singular_values=100,\n",
    "#                                                    )\n",
    "# next_line_2,z_2 = left_canonicalize_MPS_sparse(next_line,final_normlization =True,\n",
    "#                                                all_renormlization=True,\n",
    "#                                                Decomposition_Engine=Decomposition_Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.67 ms ± 33.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "u,s,v = diagonal_tensor_svd_sparse(sparse_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### diagonal.torch.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def diagonal_tensor_svd_torch_dense(tensor):\n",
    "    reduce = False\n",
    "    if len(tensor.shape)==2:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        reduce = True\n",
    "    W,H   = tensor.shape[-2:]\n",
    "    batch_shape= tensor.shape[:-2]\n",
    "    u = s = v = None\n",
    "    if W>=H:\n",
    "        batch_diag = torch.matmul(tensor.transpose(-1,-2),tensor)#auto broadcast, or can use bmm\n",
    "    else:\n",
    "        batch_diag = torch.matmul(tensor,tensor.transpose(-1,-2))#auto broadcast, or can use bmm\n",
    "    A,A        = batch_diag.shape[-2:]\n",
    "    batch_diag = batch_diag[...,range(A),range(A)]\n",
    "    batch_diag,batch_order= batch_diag.sort(-1,descending=True)\n",
    "    #fast_V      = [get_sort_matrix(order).to_dense() for order in batch_order]\n",
    "    batch_order = batch_order.flatten(start_dim=0,end_dim=-2)\n",
    "    K,A         = batch_order.shape\n",
    "    s           = batch_diag.sqrt()\n",
    "    s           = s.reshape(*batch_shape,A)\n",
    "    if W>=H:\n",
    "        v = torch.sparse_coo_tensor([list(range(K*A)),batch_order.flatten().tolist()], [1.0]*K*A,(K*A,A))\n",
    "        v = v.to_dense().reshape(-1,A,A)\n",
    "        u = torch.bmm(tensor,v.transpose(-1,-2)/s.unsqueeze(-2))\n",
    "        v = v.reshape(*batch_shape,A,A)\n",
    "        u = u.reshape(*batch_shape,W,A)\n",
    "    else:\n",
    "        u = torch.sparse_coo_tensor([list(range(K*A)),batch_order.flatten().tolist()], [1.0]*K*A,(K*A,A))\n",
    "        u = u.to_dense().reshape(-1,A,A).transpose(-1,-2)\n",
    "        v = torch.bmm(u.transpose(-1,-2)/s.unsqueeze(-1),tensor)#TODO: case when s==0\n",
    "        u = u.reshape(*batch_shape,A,A)\n",
    "        v = v.reshape(*batch_shape,A,H)\n",
    "    output = [u,s,v.transpose(-1,-2)]\n",
    "    if reduce:\n",
    "        output = [t[0] for t in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abfe4bc2fdb4e06b505cc3e15235fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# i=0\n",
    "# for images,labels in tqdm(train_loader):\n",
    "#     #images,labels = iter(train_loader).next()\n",
    "#     inputs = preprocess_sum_one(images)\n",
    "#     #inputs= rd_engine(10,50,2)\n",
    "#     inputs= inputs.permute(1,2,0)#(B,num,k)->(num,k,B)\n",
    "#     inputs= torch.diag_embed(inputs)#(num,k,B)->(num,k,B,B)\n",
    "#     inputs= inputs.permute(0,2,1,3)#(num,k,B,B)->(num,B,k,B)\n",
    "#     inputs= [v for v in inputs]\n",
    "#     inputs[0]= torch.diagonal(inputs[0], dim1=0, dim2=-1)#(B,k,B) -> #(k,B)\n",
    "#     DCEngine = lambda x:truncated_SVD(x,output='QR',max_truncation_error=0.00,max_singular_values=100)\n",
    "#     mps_line,Z_list = left_canonicalize_MPS(inputs,Decomposition_Engine=DCEngine,normlization=False)\n",
    "#     state_dict = {}\n",
    "#     state_dict['xdata']=dict([[i,t] for i,t in enumerate(mps_line)])\n",
    "#     state_dict['ydata']=labels\n",
    "#     torch.save(state_dict,f'offline_SVD_data/preprocess_sum_one.cut100/mps_line_{i}.pt')\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use 1/2 sing vals\n"
     ]
    }
   ],
   "source": [
    "Q,R        = Decomposition_Engine(inputs[0])[:2]\n",
    "new_tensor = torch.einsum('ab,bcd->acd',R,inputs[1])\n",
    "shape      = new_tensor.shape\n",
    "a,b,c = shape\n",
    "new_tensor = new_tensor.reshape(a*b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### diagonal.torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L=10\n",
    "# idx1     = list(range(2*L))\n",
    "# idx2     = [i for i in range(L) for j in range(2)]\n",
    "# #mps_unit = sparse.COO([idx1,idx2],np.random.randn(2*L),(2*L,L)).reshape((L,2,L))\n",
    "# mps_line = ([sparse.as_coo(np.random.randn(2,L))]+\n",
    "#             [sparse.COO([idx1,idx2],np.random.randn(2*L),(2*L,L)).reshape((L,2,L))\n",
    "#                for i in range(9)])\n",
    "# #mps_line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L=10\n",
    "# num=1\n",
    "# k=2\n",
    "# origin_inputs = rd_engine(L,num,k)\n",
    "# inputs = origin_inputs.permute(1,2,0)#(B,num,k)->(num,k,B)\n",
    "# inputs = torch.diag_embed(inputs)#(num,k,B)->(num,k,B,B)\n",
    "# inputs = inputs.permute(0,2,1,3)#(num,k,B,B)->(num,B,k,B)\n",
    "# num,B,k,B     = inputs.shape\n",
    "# inputs        = inputs.reshape(num,B*k,B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch_sparse import coalesce\n",
    "# idx1          = list(range(2*L))\n",
    "# idx2          = [i for i in range(L) for j in range(2)]\n",
    "# index         = torch.tensor([idx1,idx2])\n",
    "# sparse_tensor = torch.sparse_coo_tensor(index, origin_inputs.flatten(), (2*L,L)).coalesce()\n",
    "# sparse_tensor = reshape_sparse_tensor(sparse_tensor,(L,2*L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0,
     3,
     15,
     26,
     31,
     42,
     74,
     80,
     92,
     124,
     160,
     180,
     191
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transpose_sparse_tensor(coalesce_sparse_tensor):\n",
    "    return coalesce_sparse_tensor.transpose(1,0).coalesce()\n",
    "\n",
    "def matmul_sparse_tensor(coalesce_sparse_tensor_1,coalesce_sparse_tensor_2):\n",
    "    m,k1   = coalesce_sparse_tensor_1.size()\n",
    "    k2,n   = coalesce_sparse_tensor_2.size()\n",
    "    assert k1==k2\n",
    "    indexA = coalesce_sparse_tensor_1.indices()\n",
    "    valueA = coalesce_sparse_tensor_1.values()\n",
    "    \n",
    "    indexB = coalesce_sparse_tensor_2.indices()\n",
    "    valueB = coalesce_sparse_tensor_2.values()\n",
    "\n",
    "    indexC, valueC = torch_sparse.spspmm(indexA, valueA, indexB, valueB, m, k1, n)\n",
    "    return torch.sparse_coo_tensor(indexC, valueC, (m,n)).coalesce()\n",
    "def reshape_sparse_tensor(coalesce_sparse_tensor,target_shape):\n",
    "    indices = coalesce_sparse_tensor.indices().tolist()\n",
    "    size    = coalesce_sparse_tensor.size()\n",
    "    assert np.prod(size)==np.prod(target_shape)\n",
    "    target_indices = np.stack(np.unravel_index(np.ravel_multi_index(indices,size),target_shape))\n",
    "    target_indices = torch.Tensor(target_indices)\n",
    "    target_indices = target_indices.to(coalesce_sparse_tensor.device)\n",
    "    tensor = torch.sparse_coo_tensor(target_indices, coalesce_sparse_tensor.values(), target_shape).coalesce()\n",
    "    #tensor.to(coalesce_sparse_tensor.device)\n",
    "    return tensor\n",
    "\n",
    "def sparse_diagonal(dense_matrix):\n",
    "    A = len(dense_matrix)\n",
    "    idx = torch.stack([torch.arange(A),torch.arange(A)]).to(dense_matrix.device)\n",
    "    return torch.sparse_coo_tensor(idx,dense_matrix, (A,A)).coalesce()\n",
    "\n",
    "def sparse_take_first(coalesce_sparse_tensor,row_num,axis=0):\n",
    "    size    = list(coalesce_sparse_tensor.shape)\n",
    "    if row_num > size[axis]:return coalesce_sparse_tensor\n",
    "    indexes = coalesce_sparse_tensor.indices()\n",
    "    value   = coalesce_sparse_tensor.values()\n",
    "    good_i=indexes[axis]<row_num\n",
    "    indexes = torch.stack([t[good_i] for t in indexes])\n",
    "    value   = value[good_i]\n",
    "    size[axis] = row_num\n",
    "    return torch.sparse_coo_tensor(indexes,value, size).coalesce().to(coalesce_sparse_tensor.device)\n",
    "\n",
    "def diagonal_tensor_svd_torch_sparse_2D(tensor):\n",
    "    W,H   = tensor.size()\n",
    "    if W>=H:\n",
    "        MstarM     = matmul_sparse_tensor(transpose_sparse_tensor(tensor),tensor)\n",
    "    else:\n",
    "        MstarM     = matmul_sparse_tensor(tensor,transpose_sparse_tensor(tensor))\n",
    "    diagonal_svd_flag = True\n",
    "    a,b = MstarM.indices()\n",
    "    if (a!=b).any():\n",
    "        return None\n",
    "    L,L = MstarM.shape\n",
    "    batch_diag  = torch.Tensor([MstarM[i,i] for i in range(L)]).to(tensor.device)\n",
    "    #batch_diag = MstarM.values()\n",
    "    # in sparse representation, only value > 0 takes.\n",
    "    batch_diag,batch_order = batch_diag.sort(-1,descending=True)\n",
    "    s          = batch_diag.sqrt()\n",
    "    nonzero_num= torch.sum(s>0)\n",
    "    s          = s[:nonzero_num]\n",
    "    A = len(batch_order)\n",
    "    index = torch.stack([torch.arange(A).to(tensor.device),batch_order])\n",
    "    value = torch.ones(A).to(tensor.device)\n",
    "    if W>=H: \n",
    "        v = torch.sparse_coo_tensor(index,value,(A,A)).coalesce()\n",
    "        if nonzero_num < A :v = sparse_take_first(v,nonzero_num,axis=1)\n",
    "        u = matmul_sparse_tensor(tensor,matmul_sparse_tensor(transpose_sparse_tensor(v),sparse_diagonal(1/s)))\n",
    "    else:\n",
    "        u = torch.sparse_coo_tensor(index,value,(A,A)).coalesce()\n",
    "        if nonzero_num < A :u = sparse_take_first(u,nonzero_num,axis=0)\n",
    "        v = matmul_sparse_tensor(matmul_sparse_tensor(sparse_diagonal(1/s),u),tensor)\n",
    "        u = transpose_sparse_tensor(u)\n",
    "    return u,s,v\n",
    "\n",
    "def reciprocal_sparse_tensor(coalesce_sparse_tensor):\n",
    "    size   = coalesce_sparse_tensor.size()\n",
    "    indexA = coalesce_sparse_tensor.indices()\n",
    "    valueA = coalesce_sparse_tensor.values()\n",
    "    return torch.sparse_coo_tensor(indexA, 1/valueA, size).coalesce()\n",
    "\n",
    "def diagonal_tensor_RQ_torch_sparse(tensor):\n",
    "    W,H   = tensor.shape\n",
    "    assert W<=H\n",
    "    MstarM     = matmul_sparse_tensor(tensor,transpose_sparse_tensor(tensor))\n",
    "    a,b = MstarM.indices()\n",
    "    assert (a==b).any()\n",
    "    #batch_diag = MstarM.values()\n",
    "    #s = batch_diag.sqrt()\n",
    "    R = MstarM.sqrt()\n",
    "    Q = matmul_sparse_tensor(reciprocal_sparse_tensor(R),tensor)\n",
    "    return R,Q\n",
    "\n",
    "def right_canonicalize_MPS_torch_sparse(mps_line,Decomposition_Engine=diagonal_tensor_RQ_torch_sparse,\n",
    "                          final_normlization =True,all_renormlization=False\n",
    "                          ):\n",
    "    new_chain = []\n",
    "    R         = None\n",
    "    Z_list    = []\n",
    "    # assume every mps_unit is store (kD,D)\n",
    "    for i,tensor in enumerate(mps_line[::-1]):\n",
    "        if R is not None:\n",
    "            new_tensor = matmul_sparse_tensor(tensor,R)   \n",
    "        else:\n",
    "            new_tensor = tensor\n",
    "        kD,D  = new_tensor.shape\n",
    "        if kD>D:\n",
    "            new_tensor = reshape_sparse_tensor(new_tensor,(D,kD))\n",
    "\n",
    "        if i == len(mps_line) - 1:\n",
    "            if final_normlization:\n",
    "                Z = (new_tensor**2).values().sum().sqrt()\n",
    "                new_tensor /= Z\n",
    "                Z_list.append(Z)\n",
    "            new_chain.append(new_tensor)\n",
    "        else:\n",
    "            if all_renormlization:\n",
    "                Z = (new_tensor**2).values().sum().sqrt()\n",
    "                new_tensor /= Z\n",
    "                Z_list.append(Z)\n",
    "            R,Q = Decomposition_Engine(new_tensor)[:2]\n",
    "            new_chain.append(Q)\n",
    "    new_chain=new_chain[::-1]\n",
    "    return new_chain,Z_list\n",
    "\n",
    "def left_canonicalize_MPS_torch_sparse(mps_line,Decomposition_Engine=None,\n",
    "                          final_normlization =True,all_renormlization=False):\n",
    "    # for any not canonical mps line\n",
    "    # the chain size (D,P,D)\n",
    "    new_chain = []\n",
    "    R         = None\n",
    "    Z_list    = []# record the scale information for each unit.\n",
    "    # assume every mps_unit is store (D,kD)\n",
    "    for i,tensor in enumerate(tqdm(mps_line)):\n",
    "        if R is not None:\n",
    "            D,kD       = tensor.shape\n",
    "            B,D        = R.shape\n",
    "            new_tensor = matmul_sparse_tensor(R,tensor)   \n",
    "            new_shape  = new_tensor.shape\n",
    "            new_tensor = reshape_sparse_tensor(new_tensor,(B*kD//D,D))\n",
    "        else:\n",
    "            new_tensor = tensor        \n",
    "        if i == len(mps_line) - 1:\n",
    "            if final_normlization:\n",
    "                Z = (new_tensor**2).values().sum().sqrt()\n",
    "                new_tensor /= Z\n",
    "                Z_list.append(Z)\n",
    "            new_chain.append(new_tensor)\n",
    "        else:\n",
    "            if all_renormlization:\n",
    "                Z = (new_tensor**2).values().sum().sqrt()\n",
    "                new_tensor /= Z\n",
    "                Z_list.append(Z)\n",
    "            Q,R,_,diagonal_svd_flag = Decomposition_Engine(new_tensor)\n",
    "            new_chain.append(Q)\n",
    "           # print(Q.shape)\n",
    "#         if not diagonal_svd_flag:\n",
    "#             print(f\"full matrix SVD at unit {i}\")\n",
    "\n",
    "    return new_chain,Z_list\n",
    "\n",
    "def truncated_SVD_torch_sparse(tensor,output='RQ',max_singular_values= None,\n",
    "                          max_truncation_error= None,\n",
    "                          relative = True,\n",
    "                          normlized= True,\n",
    "                          verbose  = False):\n",
    "    # the canonocal\n",
    "    # tensor is batched\n",
    "    assert len(tensor.shape)  == 2\n",
    "    diagonal_svd_flag = True\n",
    "    out = diagonal_tensor_svd_torch_sparse_2D(tensor)\n",
    "    if out is None:\n",
    "        diagonal_svd_flag=False\n",
    "        q = min(tensor.size())\n",
    "        q = min(q,max_singular_values) \n",
    "        u, s, v = torch.svd_lowrank(tensor,q=q)\n",
    "        v       = v.T\n",
    "        #print(q,s.shape)\n",
    "    else:\n",
    "        u, s, v = out\n",
    "        max_singular_values = s.shape[-1] if max_singular_values is None else max_singular_values\n",
    "        if max_truncation_error is not None and len(s)>max_singular_values:\n",
    "            # Cumulative norms of singular values in ascending order\n",
    "            s_normlized  = s**2\n",
    "            s_normlized /= torch.sum(s_normlized)\n",
    "            s_sorted,_   = torch.sort(s_normlized)# 0.1,0.2,...,0.4\n",
    "            trunc_errs   = torch.sqrt(torch.cumsum(s_sorted,0))# 0.1,0.3,....1\n",
    "            # If relative is true, rescale max_truncation error with the largest\n",
    "            # singular value to yield the absolute maximal truncation error.\n",
    "            num_sing_vals_err = torch.sum(trunc_errs > max_truncation_error)\n",
    "            if max_singular_values>num_sing_vals_err and verbose:\n",
    "                print(f\"use {num_sing_vals_err}/{max_singular_values} sing vals\")\n",
    "        else:\n",
    "            num_sing_vals_err  = max_singular_values\n",
    "\n",
    "        nk = min(max_singular_values, num_sing_vals_err)\n",
    "\n",
    "\n",
    "        #s_rest = s[...,num_sing_vals_keep:]\n",
    "\n",
    "        u  = sparse_take_first(u,nk,axis=1) if u.is_sparse else u[...,:nk]\n",
    "        s  = s[...,:nk]\n",
    "        v  = sparse_take_first(v,nk,axis=0) if v.is_sparse else v[:nk,:]\n",
    "    Z  = None #maybe add in the furture\n",
    "    if output == 'RQ':\n",
    "        R = u*s[None] if not u.is_sparse else matmul_sparse_tensor(u,sparse_diagonal(s))\n",
    "        Q = v\n",
    "        if not R.is_sparse:R = R.to_sparse() \n",
    "        if not Q.is_sparse:Q = Q.to_sparse() \n",
    "        output = [R,Q,Z,diagonal_svd_flag]\n",
    "    elif output == 'QR':\n",
    "        Q = u\n",
    "        R = s[:,None]*v if not  v.is_sparse else matmul_sparse_tensor(sparse_diagonal(s),v)\n",
    "        if not R.is_sparse:R = R.to_sparse()\n",
    "        if not Q.is_sparse:Q = Q.to_sparse()\n",
    "        output = [Q,R,Z,diagonal_svd_flag]\n",
    "    else:\n",
    "        if not u.is_sparse:u = u.to_sparse() \n",
    "        if not v.is_sparse:v = v.to_sparse() \n",
    "        output = [u,s,v,Z,diagonal_svd_flag]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=1000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test , batch_size=1000, shuffle=False)\n",
    "images,labels = iter(train_loader).next()\n",
    "origin_inputs = preprocess_sum_one(images)\n",
    "\n",
    "L=origin_inputs.shape[0]\n",
    "idx1          = list(range(2*L))\n",
    "idx2          = [i for i in range(L) for j in range(2)]\n",
    "index         = torch.tensor([idx1,idx2])\n",
    "mps_line=[(origin_inputs[:,0,:].transpose(1,0)).to_sparse()]\n",
    "for tensor in origin_inputs.permute(1,0,2)[1:]:\n",
    "    sparse_tensor = torch.sparse_coo_tensor(index, tensor.flatten(), (2*L,L)).coalesce()      \n",
    "    #sparse_tensor = reshape_sparse_tensor(sparse_tensor,(L,2*L))                              \n",
    "    mps_line.append(sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mps_line=[t.cuda() for t in mps_line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mps_line,Z = right_canonicalize_MPS_torch_sparse(mps_line,final_normlization =True,all_renormlization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860f1c30906e46208b3c22b98cd2c437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Decomposition_Engine=lambda x:truncated_SVD_torch_sparse(x,output='QR',\n",
    "                                                        max_truncation_error=0.00,\n",
    "                                                        max_singular_values=1,\n",
    "                                                       )\n",
    "mps_line2,z_2 = left_canonicalize_MPS_torch_sparse(mps_line,final_normlization =True,\n",
    "                                               all_renormlization=True,\n",
    "                                               Decomposition_Engine=Decomposition_Engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### torch_sparse MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_sparse\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "mnist_data = np.load('archive/tn-for-unsup-ml/data/binarized_mnist.npz')\n",
    "train_data = torch.from_numpy(mnist_data['train_data'])\n",
    "test_data  = torch.from_numpy(mnist_data['test_data'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=(0.0,), std=(1.0,))\n",
    "])\n",
    "DATAPATH    = '/media/tianning/DATA/DATASET/MNIST/'\n",
    "mnist_train = datasets.MNIST(DATAPATH, train=True, download=False, transform=transform)\n",
    "mnist_test  = datasets.MNIST(DATAPATH, train=False,download=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=1000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test , batch_size=1000, shuffle=False)\n",
    "images,labels = iter(train_loader).next()\n",
    "origin_inputs = preprocess_sum_one(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "L=origin_inputs.shape[0]\n",
    "idx1  = [0]*L+[1]*L\n",
    "idx2  = [(L+1)*i for i in range(L)]\n",
    "idx2  = idx2 + idx2\n",
    "index = torch.tensor([idx2,idx1])\n",
    "input_mps=[(origin_inputs[:,0,:]).to_sparse().to(device)]\n",
    "for tensor in origin_inputs.permute(1,0,2)[1:]:\n",
    "    sparse_tensor = torch.sparse_coo_tensor(index, tensor.flatten(), (L*L,2)).coalesce()      \n",
    "    #sparse_tensor = reshape_sparse_tensor(sparse_tensor,(L,2*L))                              \n",
    "    input_mps.append(sparse_tensor.to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# left_tensors = [torch.einsum('cpd,apb->acbd', input_data[i]  ,self.mps_var[i]).flatten(0,1).flatten(-2,-1) for i in range(self.hn)]\n",
    "# rigt_tensors = [torch.einsum('cpd,apb->acbd', input_data[i-1]  ,self.mps_var[i]).flatten(0,1).flatten(-2,-1) for i in range(self.hn+1,len(self.mps_var))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=2\n",
    "P=2\n",
    "L=14\n",
    "B=3\n",
    "inputs = rd_engine(B,L,P)    \n",
    "# inputs = inputs.permute(1,2,0)#(B,num,k)->(num,k,B)\n",
    "# inputs = torch.diag_embed(inputs)#(num,k,B)->(num,k,B,B)\n",
    "# inputs = inputs.flatten(-2,-1)\n",
    "#inputs = inputs.permute(0,2,1,3)#(num,k,B,B)->(num,B,k,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "B=inputs.shape[0]\n",
    "idx1  = [0]*B+[1]*B\n",
    "idx2  = [(B+1)*i for i in range(B)]\n",
    "idx2  = idx2 + idx2\n",
    "index = torch.tensor([idx2,idx1])\n",
    "input_mps=[(inputs[:,0,:]).to_sparse()]\n",
    "for tensor in inputs.permute(1,0,2)[1:]:\n",
    "    sparse_tensor = torch.sparse_coo_tensor(index, tensor.flatten(), (B*B,2)).coalesce()      \n",
    "    #sparse_tensor = reshape_sparse_tensor(sparse_tensor,(L,2*L))                              \n",
    "    input_mps.append(sparse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    return x\n",
    "Ds  = [1]+list(np.random.randint(3,10,L-1))+[1]\n",
    "mps_var    = [rd_engine(Ds[i],P,Ds[i+1]) for i in range(L)]  \n",
    "print(Ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 9, 6, 4, 6, 6, 3, 6, 5, 6, 6, 3, 1]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "out=[]\n",
    "for inp,mps_unit in zip(input_mps,mps_var):\n",
    "    out.append(torch.sparse.mm(inp,mps_unit.permute(1,0,2).flatten(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2])\n"
     ]
    }
   ],
   "source": [
    "idx=4\n",
    "print(input_mps[idx].shape)\n",
    "D1,P,D2=mps_var[idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_values= input_mps[idx].values().reshape(B,P)@mps_var[idx].permute(1,0,2).flatten(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 24])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0746, -0.0109,  0.0053,  0.0967,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0675, -0.0708,  0.1573, -0.1155,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0360,  0.2245,  0.0516,  0.1981,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2776, -0.2080, -0.0120,  0.1236,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1618,  0.0482,  0.0096, -0.0264,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1465, -0.1226, -0.1381,  0.1770,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1027, -0.0411, -0.0813,  0.1604,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1903, -0.0082,  0.2638, -0.0855,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0499,  0.3243,  0.0474,  0.2284,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4458, -0.2313, -0.0468,  0.2555,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2578,  0.0307,  0.0229, -0.0274,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.1947, -0.2375, -0.2380,  0.2213,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0436,  0.0053, -0.0069, -0.0554],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0353,  0.0452, -0.0900,  0.0707],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0254, -0.1306, -0.0312, -0.1177],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.1596,  0.1240,  0.0057, -0.0686],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0931, -0.0297, -0.0052,  0.0158],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0860,  0.0687,  0.0787, -0.1045]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.block_diag(*all_values.reshape(B,D1,D2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LDLD=out[idx].reshape(B,B,Ds[idx],Ds[idx+1]).permute(0,2,1,3).flatten(0,1).flatten(-2,-1).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 12])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDLD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.075 -0.011  0.005  0.097  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.067 -0.071  0.157 -0.115  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [-0.036  0.225  0.052  0.198  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.278 -0.208 -0.012  0.124  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.162  0.048  0.01  -0.026  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.146 -0.123 -0.138  0.177  0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.103 -0.041 -0.081  0.16   0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.19  -0.008  0.264 -0.086  0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.05   0.324  0.047  0.228  0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.446 -0.231 -0.047  0.256  0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.258  0.031  0.023 -0.027  0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.195 -0.238 -0.238  0.221  0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.044  0.005\n",
      "  -0.007 -0.055]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.035  0.045\n",
      "  -0.09   0.071]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.025 -0.131\n",
      "  -0.031 -0.118]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.16   0.124\n",
      "   0.006 -0.069]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.093 -0.03\n",
      "  -0.005  0.016]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.    -0.086  0.069\n",
      "   0.079 -0.104]]\n"
     ]
    }
   ],
   "source": [
    "print(LDLD.to_dense().numpy().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the max singular value truncation only good for hundreds dimenstion, for small dimenstion not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "FiniteMPS canonicalize == right_canonicalize_MPS with torchrq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensornetwork.matrixproductstates.finite_mps import FiniteMPS\n",
    "from tensornetwork.matrixproductstates.infinite_mps import InfiniteMPS\n",
    "from typing import Any, List, Optional, Text, Type, Union, Dict, Sequence\n",
    "import numpy as np\n",
    "Tensor = Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# D=6\n",
    "# P=4\n",
    "# L=5\n",
    "# mps_line = [torch.randn(1,P,D)] + [torch.randn(D,P,D) for i in range(L-2)]+ [torch.randn(D,P,1)]\n",
    "# input_mps= [torch.randn(P,D)] + [torch.randn(D,P,D) for i in range(L-2)]+ [torch.randn(D,P)]\n",
    "# approx = approxmate_mps_line(mps_line,max_singular_values=2)\n",
    "# #contract_two_mps(approx,input_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tn.set_default_backend(\"pytorch\")\n",
    "# tn_mps_1 = FiniteMPS(mps_line,canonicalize=False)\n",
    "# tn_mps_1.canonicalize(normalize=False)\n",
    "# tn_mps_1.center_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensornetwork as tn\n",
    "from tensornetwork import contractors\n",
    "tn.set_default_backend(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=10\n",
    "P=4\n",
    "L=14\n",
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     1,
     36
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensornetwork.contractors.opt_einsum_paths.path_contractors import *\n",
    "def my_contracter(nodes: Iterable[AbstractNode],\n",
    "         path = None,\n",
    "         output_edge_order: Optional[Sequence[Edge]] = None,\n",
    "         ignore_edge_order: bool = False,\n",
    "         memory_limit: Optional[int] = None) -> AbstractNode:\n",
    "        \"\"\"Base method for all `opt_einsum` contractors.\n",
    "\n",
    "        Args:\n",
    "        nodes: A collection of connected nodes.\n",
    "        algorithm: `opt_einsum` contraction method to use.\n",
    "        output_edge_order: An optional list of edges. Edges of the\n",
    "        final node in `nodes_set`\n",
    "        are reordered into `output_edge_order`;\n",
    "        if final node has more than one edge,\n",
    "        `output_edge_order` must be provided.\n",
    "        ignore_edge_order: An option to ignore the output edge\n",
    "        order.\n",
    "\n",
    "        Returns:\n",
    "        Final node after full contraction.\n",
    "        \"\"\"\n",
    "        nodes_set = set(nodes)\n",
    "        edges = get_all_edges(nodes_set)\n",
    "        #output edge order has to be determinded before any contraction\n",
    "        #(edges are refreshed after contractions)\n",
    "\n",
    "        if not ignore_edge_order:\n",
    "            if output_edge_order is None:\n",
    "                output_edge_order = list(get_subgraph_dangling(nodes))\n",
    "                if len(output_edge_order) > 1:\n",
    "                    raise ValueError(\"The final node after contraction has more than \"\n",
    "                                 \"one remaining edge. In this case `output_edge_order` \"\n",
    "                                 \"has to be provided.\")\n",
    "\n",
    "            if set(output_edge_order) != get_subgraph_dangling(nodes):\n",
    "                raise ValueError(\"output edges are not equal to the remaining \"\n",
    "                       \"non-contracted edges of the final node.\")\n",
    "\n",
    "        for edge in edges:\n",
    "            if not edge.is_disabled:  #if its disabled we already contracted it\n",
    "                if edge.is_trace():\n",
    "                    nodes_set.remove(edge.node1)\n",
    "                    nodes_set.add(contract_parallel(edge))\n",
    "\n",
    "        if len(nodes_set) == 1:\n",
    "            # There's nothing to contract.\n",
    "            if ignore_edge_order:\n",
    "                return list(nodes_set)[0]\n",
    "            return list(nodes_set)[0].reorder_edges(output_edge_order)\n",
    "\n",
    "        if path is None:\n",
    "            algorithm = functools.partial(opt_einsum.paths.greedy, memory_limit=memory_limit)\n",
    "            # Then apply `opt_einsum`'s algorithm\n",
    "            path, nodes = utils.get_path(nodes_set, algorithm)\n",
    "        else:\n",
    "            nodes = list(nodes_set) \n",
    "        for a, b in path:\n",
    "            new_node = contract_between(nodes[a], nodes[b], allow_outer_product=True)\n",
    "            nodes.append(new_node)\n",
    "            nodes = utils.multi_remove(nodes, [a, b])\n",
    "\n",
    "        # if the final node has more than one edge,\n",
    "        # output_edge_order has to be specified\n",
    "        final_node = nodes[0]  # nodes were connected, we checked this\n",
    "        if not ignore_edge_order:\n",
    "            final_node.reorder_edges(output_edge_order)\n",
    "        return final_node,path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=10;O=10;mps_core = [rd_engine(P,D)] + [rd_engine(D,P,D) for _ in range(L-1)] + [rd_engine(D,O,D)]+ [rd_engine(D,P)]\n",
    "B=20;inp_line = [rd_engine(B,P)] + [rd_engine(B,P,B) for _ in range(L-1)] + [rd_engine(B,P,B)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 18), (12, 28), (10, 27), (14, 17), (25, 26), (0, 25), (21, 24), (20, 23), (19, 22), (5, 21), (1, 20), (1, 19), (1, 18), (8, 17), (11, 16), (12, 15), (5, 14), (4, 13), (7, 12), (7, 11), (8, 10), (4, 9), (0, 8), (2, 7), (1, 6), (0, 5), (0, 4), (0, 3), (0, 2), (0, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4304e-01, -1.5820e-01, -1.3586e-02, -9.4462e-04, -1.3982e-01,\n",
       "          9.9057e-02,  1.7996e-01, -1.1456e-01,  1.4263e-01, -2.4580e-03],\n",
       "        [ 3.4687e-01, -4.8310e-01,  2.2043e-01, -2.1364e-01,  4.1445e-01,\n",
       "         -2.5674e-01,  1.4035e-01,  2.4091e-01, -2.3025e-01, -1.1829e-01],\n",
       "        [ 2.0311e-01, -8.6519e-02,  3.1690e-01,  2.3554e-01,  1.1007e-02,\n",
       "         -4.1188e-01,  4.2258e-02,  3.6463e-01, -3.4286e-01, -4.0262e-01],\n",
       "        [-4.7547e-01,  3.7831e-01,  3.3061e-02,  2.4604e-01, -1.5482e-01,\n",
       "         -4.6528e-02,  4.5324e-03, -4.0088e-01,  3.5013e-01,  8.5555e-02],\n",
       "        [ 9.9050e-02, -8.2526e-02,  1.8456e-01, -1.9140e-02,  1.1645e-01,\n",
       "         -3.5684e-01, -2.0432e-01,  1.1110e-01,  9.2640e-02, -1.0480e-01],\n",
       "        [ 1.6438e-01,  2.7121e-01, -3.3816e-02, -4.1494e-03,  1.2835e-01,\n",
       "          1.2292e-01,  8.6925e-02,  3.1019e-01,  3.8543e-01,  1.7188e-01],\n",
       "        [-5.1561e-02,  1.1307e-01, -5.6854e-01,  1.4294e-01, -2.3753e-01,\n",
       "          2.2774e-01,  3.7236e-01, -3.5108e-01,  1.8630e-01,  3.9439e-02],\n",
       "        [ 5.2912e-02,  2.5094e-01, -1.9803e-01,  4.7044e-03, -1.8587e-01,\n",
       "          4.2176e-01,  1.8772e-01,  3.0565e-01,  1.2801e-01,  1.9867e-01],\n",
       "        [ 1.1517e-01,  1.0643e-01,  1.5878e-01,  5.5360e-02, -1.3950e-01,\n",
       "          5.1184e-02,  8.2136e-02, -2.4243e-01, -8.4872e-02,  1.3361e-02],\n",
       "        [ 1.4793e-01, -1.5811e-01,  5.4551e-05, -8.3154e-02, -2.5979e-01,\n",
       "         -1.9083e-01, -1.5844e-01,  1.5925e-01,  1.1895e-01,  2.1983e-02],\n",
       "        [-3.0817e-01,  2.3907e-01, -1.7756e-01,  2.8272e-01,  1.4382e-01,\n",
       "          5.0498e-01,  4.2695e-01,  3.0219e-01, -2.6779e-01, -1.2777e-01],\n",
       "        [-3.8576e-02,  9.5788e-03,  8.3571e-02, -7.7039e-02,  3.6145e-03,\n",
       "          1.6885e-01,  1.5983e-01, -2.3978e-01, -2.0942e-01,  9.6214e-02],\n",
       "        [ 4.0893e-03, -3.6139e-01,  4.0647e-01, -2.6646e-01,  1.9695e-01,\n",
       "         -1.0266e-01,  1.9087e-01,  5.0853e-01, -3.8624e-02,  1.8222e-04],\n",
       "        [-2.9511e-02,  3.1189e-01,  2.4094e-01,  1.7423e-01, -2.5471e-01,\n",
       "         -8.0931e-02, -3.8496e-02, -1.2300e-02,  6.1083e-02, -6.5881e-02],\n",
       "        [-2.3669e-01, -3.0595e-01, -1.7096e-01,  1.4025e-02,  1.4744e-01,\n",
       "         -3.6415e-02, -1.7377e-01,  2.5554e-01, -1.3301e-01, -8.9086e-02],\n",
       "        [ 1.0746e-01, -8.4043e-02,  8.2953e-02,  1.8575e-01, -1.6408e-01,\n",
       "          1.5920e-01, -7.5862e-02, -2.0506e-01, -3.0581e-01, -7.0687e-02],\n",
       "        [-8.5354e-02,  2.5426e-01,  1.8748e-01,  8.1318e-02, -2.2259e-01,\n",
       "          4.7392e-02, -3.1515e-01,  1.3632e-01, -1.4565e-01, -2.7911e-01],\n",
       "        [ 1.8653e-01, -1.3372e-01,  4.8352e-02, -1.3855e-01, -1.6892e-01,\n",
       "          2.6170e-01,  1.0761e-01, -6.7705e-02, -4.6117e-02,  1.2726e-01],\n",
       "        [-2.1083e-01, -3.7951e-02, -1.1342e-01,  4.1334e-02, -6.1075e-03,\n",
       "          2.7614e-01,  7.5191e-03, -2.2903e-01, -8.9833e-02,  2.0455e-01],\n",
       "        [ 7.3030e-02,  2.0093e-01, -2.6945e-01,  1.6580e-01, -2.3946e-02,\n",
       "          1.6530e-01, -1.5978e-01, -1.2399e-01,  1.9520e-02, -1.6731e-02]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_nodes  = [tn.Node(v, name=f\"t{i}\") for i,v in enumerate(mps_core)]\n",
    "for i in range(len(mps_core)-1):\n",
    "    tn.connect(mps_nodes[i][-1],mps_nodes[i+1][0],name=f\"mps:{i}<->{i+1}\")\n",
    "inp_nodes=[tn.Node(v, name=f\"i{i}\") for i,v in enumerate(inp_line)]\n",
    "tn.connect(inp_nodes[0][0],inp_nodes[1][0],name=f\"inp:{0}<->{1}\")\n",
    "for i in range(1,len(inp_nodes)-1):\n",
    "    tn.connect(inp_nodes[i][-1],inp_nodes[i+1][0],name=f\"inp:{i}<->{i+1}\")\n",
    "for i,input_node in enumerate(inp_nodes):\n",
    "    j = i if i < L else i+1\n",
    "    mps_physicd_edge = mps_nodes[j][0] if j==0 else mps_nodes[j][1]\n",
    "    inp_physics_edge = input_node[1]\n",
    "    tn.connect(mps_physicd_edge,inp_physics_edge,name=f\"phy_{i}\")\n",
    "ans,path = my_contracter(mps_nodes+inp_nodes,\n",
    "                       output_edge_order=[inp_nodes[-1][2],mps_nodes[L][1]])\n",
    "ans.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_mps_list    = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]\n",
    "middle_mpo_list =[[rd_engine(P,D,P)]+[rd_engine(L-2,D,P,D,P)]+ [rd_engine(D,P,P)]\n",
    "                  for _ in range(L-2)]\n",
    "bottom_mps_list = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=2000\n",
    "P=4\n",
    "L=14\n",
    "top_mps_list = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]\n",
    "top_mps_list = right_mps_form(top_mps_list)\n",
    "bottom_mps_list= [0.005*torch.randn_like(a)+a for a in top_mps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bottom_mps_list,Zb = left_canonicalize_MPS(right_mps_form(bottom_mps_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0166)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contract_two_mps(bottom_mps_list,top_mps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left canonical scalar:0.09317030757665634\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 0.942, 0.938, 0.958, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.8452922105789185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0118)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mps_list, scalar = approxmate_mps_line(top_mps_list,max_singular_values=1000)\n",
    "contract_two_mps(bottom_mps_list,new_mps_list)*scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:0/12 (4, 6) - 12x(6, 4, 6) - (6, 4)\n",
      "start:1/12 \n",
      "get mps (4, 36) - 12x(36, 4, 36) - (36, 4)\n",
      "boundary contraction cost:0.0013422966003417969\n",
      "   left canonical scalar:3.0327204513014294e-05\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:1.0\n",
      "low rank approximation cost:0.11043429374694824 scalar:3.0327204513014294e-05\n",
      "==========================================\n",
      "start:2/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0018436908721923828\n",
      "   left canonical scalar:0.000614150136243552\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:1.0000004768371582\n",
      "low rank approximation cost:0.23517894744873047 scalar:0.0006141504272818565\n",
      "==========================================\n",
      "start:3/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0022668838500976562\n",
      "   left canonical scalar:0.000502652779687196\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 0.998, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.9966887831687927\n",
      "low rank approximation cost:1.6759274005889893 scalar:0.0005009883898310363\n",
      "==========================================\n",
      "start:4/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0043697357177734375\n",
      "   left canonical scalar:0.0005132302176207304\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923, 0.91, 0.963, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.808193027973175\n",
      "low rank approximation cost:14.75801420211792 scalar:0.00041478907223790884\n",
      "==========================================\n",
      "start:5/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0012714862823486328\n",
      "   left canonical scalar:0.0005140540306456387\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.906, 0.887, 0.956, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7686819434165955\n",
      "low rank approximation cost:14.943126678466797 scalar:0.0003951440448872745\n",
      "==========================================\n",
      "start:6/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0012161731719970703\n",
      "   left canonical scalar:0.000582900014705956\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.915, 0.888, 0.958, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.777186393737793\n",
      "low rank approximation cost:14.729921102523804 scalar:0.0004530219594016671\n",
      "==========================================\n",
      "start:7/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0012156963348388672\n",
      "   left canonical scalar:0.0005581201403401792\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.915, 0.88, 0.949, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7637606859207153\n",
      "low rank approximation cost:14.766213178634644 scalar:0.00042627021321095526\n",
      "==========================================\n",
      "start:8/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.001264333724975586\n",
      "   left canonical scalar:0.0005442900583148003\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.911, 0.876, 0.946, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7541176080703735\n",
      "low rank approximation cost:14.809579372406006 scalar:0.00041045871330425143\n",
      "==========================================\n",
      "start:9/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0011568069458007812\n",
      "   left canonical scalar:0.0005957805551588535\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 0.878, 0.948, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7570173740386963\n",
      "low rank approximation cost:14.73672342300415 scalar:0.00045101623982191086\n",
      "==========================================\n",
      "start:10/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0012233257293701172\n",
      "   left canonical scalar:0.0006513702101074159\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 0.881, 0.951, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7616574168205261\n",
      "low rank approximation cost:14.767654418945312 scalar:0.0004961209488101304\n",
      "==========================================\n",
      "start:11/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0015985965728759766\n",
      "   left canonical scalar:0.0004802705952897668\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 0.905, 0.879, 0.949, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7546309232711792\n",
      "low rank approximation cost:14.78076958656311 scalar:0.0003624270320869982\n",
      "==========================================\n",
      "start:12/12 \n",
      "get mps (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0012969970703125\n",
      "   left canonical scalar:0.0005720893968828022\n",
      "   right canonical Z:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.912, 0.88, 0.947, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "   right canonical scalar:0.7601038813591003\n",
      "low rank approximation cost:14.883152484893799 scalar:0.0004348473739810288\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "now_mps_list = bottom_mps_list\n",
    "print(f\"start:0/{len(middle_mpo_list)} {get_mps_size_list(now_mps_list)}\")\n",
    "for i,next_mpo in enumerate(middle_mpo_list):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    now_mps_list = contract_mps_mpo(now_mps_list,next_mpo)\n",
    "    print(f\"start:{i+1}/{len(middle_mpo_list)} \")\n",
    "    print(f\"get mps {get_mps_size_list(now_mps_list)}\")\n",
    "    cost       = time.time() - start_time\n",
    "    print(f\"boundary contraction cost:{cost}\")\n",
    "    if now_mps_list[0].shape[-1]>1:\n",
    "        start_time = time.time()\n",
    "        now_mps_list,scalar = approxmate_mps_line(right_mps_form(now_mps_list),\n",
    "                                                  max_singular_values= 1000,\n",
    "                                                  mode='full',\n",
    "                                                 # mode='right' if i>3 else 'full'\n",
    "                                                 )\n",
    "        #now_mps_list = [now_mps_list[0],torch.stack(now_mps_list[1:-1],dim=0),now_mps_list[-1]]\n",
    "        cost       = time.time() - start_time\n",
    "        print(f\"low rank approximation cost:{cost} scalar:{scalar}\")\n",
    "        print(\"==========================================\")\n",
    "#     start_time = time.time()\n",
    "#     value      = contract_two_mps(now_mps_list,top_mps_list)\n",
    "#     cost       = time.time() - start_time\n",
    "#     print(f\"col contraction cost:{cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:0/10 (4, 36) - 10x(36, 4, 36) - (36, 4)\n",
      "boundary contraction cost:0.0007288455963134766\n",
      "low rank approximation cost:0.010253667831420898 scalar:5.410343841081405e-23\n",
      "start:1/10 (4, 24)-(24, 4, 96)-(96, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 216)-(216, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.0014865398406982422\n",
      "low rank approximation cost:0.038602352142333984 scalar:1.2635199960961407e-13\n",
      "start:2/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 1296)-(1296, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.03164219856262207\n",
      "low rank approximation cost:0.9437947273254395 scalar:1.3323970424988897e-13\n",
      "start:3/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.4210662841796875\n",
      "low rank approximation cost:16.94850254058838 scalar:1.2046462453368452e-13\n",
      "start:4/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.42435622215270996\n",
      "low rank approximation cost:16.967477798461914 scalar:1.1561990114818506e-13\n",
      "start:5/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.4237658977508545\n",
      "low rank approximation cost:16.9342782497406 scalar:1.2458078908988426e-13\n",
      "start:6/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.42182374000549316\n",
      "low rank approximation cost:16.97764825820923 scalar:1.2171254678217144e-13\n",
      "start:7/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.4213716983795166\n",
      "low rank approximation cost:17.01645064353943 scalar:1.169041377770211e-13\n",
      "start:8/10 (4, 24)-(24, 4, 96)-(96, 4, 384)-(384, 4, 1536)-(1536, 4, 6000)-(6000, 4, 6000)-(6000, 4, 6000)-(6000, 4, 1536)-(1536, 4, 384)-(384, 4, 96)-(96, 4, 24)-(24, 4)\n",
      "boundary contraction cost:0.42140722274780273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c84427f25f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnow_mps_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mnow_mps_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapproxmate_mps_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_mps_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_mps_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_singular_values\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#now_mps_list = [now_mps_list[0],torch.stack(now_mps_list[1:-1],dim=0),now_mps_list[-1]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcost\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ab53d8dcd2bf>\u001b[0m in \u001b[0;36mapproxmate_mps_line\u001b[0;34m(mps_line, max_singular_values, max_truncation_error, relative)\u001b[0m\n\u001b[1;32m    119\u001b[0m                        ):\n\u001b[1;32m    120\u001b[0m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mmps_line\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_canonicalize_MPS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmps_line\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDecomposition_Engine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     SVD_Engine = lambda x:truncated_SVD(x,max_singular_values = max_singular_values,\n\u001b[1;32m    123\u001b[0m                                           \u001b[0mmax_truncation_error\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmax_truncation_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ab53d8dcd2bf>\u001b[0m in \u001b[0;36mleft_canonicalize_MPS\u001b[0;34m(mps_line, Decomposition_Engine, normlization)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnew_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecomposition_Engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mQ\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mnew_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "now_mps_list = bottom_mps_list\n",
    "for i,next_mpo in enumerate(middle_mpo_list):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    now_mps_list = contract_mps_mpo(now_mps_list,next_mpo)\n",
    "    print(f\"start:{i}/{len(middle_mpo_list)} {get_mps_size_list(now_mps_list)}\")\n",
    "    cost       = time.time() - start_time\n",
    "    print(f\"boundary contraction cost:{cost}\")\n",
    "    if now_mps_list[0].shape[-1]>1:\n",
    "        start_time = time.time()\n",
    "        now_mps_list,scalar = approxmate_mps_line(right_mps_form(now_mps_list),max_singular_values= 1000)\n",
    "        #now_mps_list = [now_mps_list[0],torch.stack(now_mps_list[1:-1],dim=0),now_mps_list[-1]]\n",
    "        cost       = time.time() - start_time\n",
    "        print(f\"low rank approximation cost:{cost} scalar:{scalar}\")\n",
    "#     start_time = time.time()\n",
    "#     value      = contract_two_mps(now_mps_list,top_mps_list)\n",
    "#     cost       = time.time() - start_time\n",
    "#     print(f\"col contraction cost:{cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 4, 16]),\n",
       " torch.Size([16, 4, 64]),\n",
       " torch.Size([64, 4, 256]),\n",
       " torch.Size([256, 4, 64]),\n",
       " torch.Size([64, 4, 16]),\n",
       " torch.Size([16, 4, 4])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.shape for t in now_mps_list[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def contraction_row(tensor1,tensor2,truncate=None,einsum_engin=einsum_engin):\n",
    "    # tensor1 <-> tensor2 :-> tensor\n",
    "    # (N,H,a,b,c,d) <-> (N,H,c,e,f,g) :-> (N,H,a,be,f,dg)\n",
    "    tensor  = einsum_engin(\"whabcd,whcefg->whabefdg\",tensor1,tensor2).flatten(3,4).flatten(-2,-1)\n",
    "    if truncate is None:return tensor \n",
    "    W,H = tensor.shape[:2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def RecursionBMPS(tensor,truncate=None,einsum_engin=torch.einsum):\n",
    "    W,H = tensor.shape[:2]\n",
    "    while W > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = contraction_line(tensor[0:nice_size:2], tensor[1:nice_size:2],truncate=None,einsum_engin=einsum_engin)\n",
    "            #tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PEPSLinear(nn.Module):\n",
    "    '''\n",
    "     MPSLinear(in_features: int, out_features: int, W:int,H:int\n",
    "               in_physics_bond: int, out_physics_bond: int, virtual_bond_dim:int,\n",
    "                            bias: bool = True, label_position: int or str):\n",
    "        input  (Batch, in_features , W, H,  in_physics_bond)\n",
    "        output (Batch, out_features,        out_physics_bond)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features,out_features,\n",
    "                       in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=2,\n",
    "                       bias=True,label_position='center',init_std=1e-10):\n",
    "        super(MPSLinear, self).__init__()\n",
    "        if label_position is 'center':label_position = in_features//2\n",
    "        assert type(label_position) is int\n",
    "        self.in_features   = in_features\n",
    "        self.out_features  = out_features\n",
    "        self.W             = W \n",
    "        self.H             = H\n",
    "        self.vbd           = virtual_bond_dim\n",
    "        self.ipb           = in_physics_bond\n",
    "        self.opb           = out_physics_bond\n",
    "\n",
    "        self.hn            = label_position\n",
    "        left_num           = self.hn\n",
    "        right_num          = in_features - left_num\n",
    "\n",
    "        bias_mat = torch.einsum(\"ij,kl->ijkl\",torch.eye(2),torch.eye(2)).unsqueeze(-1).repeat(1,1,1,1,self.ipb)\n",
    "        self.left_tensors = nn.Parameter(init_std * torch.randn(self.W, left_num  ,self.vbd,self.vbd,self.vbd,self.vbd, self.ipb)+ bias_mat)\n",
    "        self.rigt_tensors = nn.Parameter(init_std * torch.randn(self.W, right_num ,self.vbd,self.vbd,self.vbd,self.vbd, self.ipb)+ bias_mat)\n",
    "\n",
    "        bias_mat = torch.einsum(\"ij,kl->ijkl\",torch.eye(2),torch.eye(2)).unsqueeze(-1).repeat(1,1,1,1,self.opb)\n",
    "        self.cent_tensors = nn.Parameter(init_std * torch.randn(self.W,self.out_features,self.vbd,self.vbd,self.vbd,self.vbd, self.opb)+ bias_mat)\n",
    "\n",
    "    @staticmethod\n",
    "    def TRG_contraction(tensor):\n",
    "        '''\n",
    "        Tensor renormalization group Contraction method\n",
    "                     | \n",
    "           |      —○\n",
    "        —●—\t==>    \\ \n",
    "           |           ○—\n",
    "                       | \n",
    "        input: (W[2^N], H[2^N] , D,D,D,D)\n",
    "        '''\n",
    "        W,H = tensor.shape[:2]\n",
    "        lu_tensor = tensor[0:::2,0:::2]\n",
    "        ll_tensor = tensor[0:::2,1:::2]\n",
    "        ru_tensor = tensor[1:::2,0:::2]\n",
    "        rl_tensor = tensor[1:::2,1:::2]\n",
    "        size   = int(tensor.shape[0])\n",
    "        while size > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "        return tensor.squeeze(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_chain_contraction(tensor):\n",
    "        size   = int(tensor.shape[0])\n",
    "        while size > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "        return tensor.squeeze(0)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # the input data shape is (B,L,pd)\n",
    "        # expand to convolution patch\n",
    "        embedded_data= input_data\n",
    "        left_tensors = torch.einsum('wijp,nwp->wnij',self.left_tensors,embedded_data[:,:self.hn])#i.e. (K,NB,b,b)\n",
    "        rigt_tensors = torch.einsum('wijp,nwp->wnij',self.rigt_tensors,embedded_data[:,-self.hn:])#i.e.(K,NB,b,b)\n",
    "\n",
    "        left_tensors = self.get_chain_contraction(left_tensors) #i.e. (NB,b,b)\n",
    "        rigt_tensors = self.get_chain_contraction(rigt_tensors) #i.e. (NB,b,b)\n",
    "\n",
    "        tensor  = torch.einsum('bip,oplt,bli->bot',left_tensors,self.cent_tensors,rigt_tensors)\n",
    "        # (NB,b,b) <-> (T,b,b,o) <-> (NB,b,b) ==> (NB,T,t)\n",
    "        return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "from utils import preprocess_binary,preprocess_sincos,aggregation_patch\n",
    "import time\n",
    "from mltool.dataaccelerate import DataSimfetcher\n",
    "from mltool.loggingsystem import LoggingSystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(dataset=mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test , batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models import MPSLinear\n",
    "# model = MPSLinear(28*28,10,in_physics_bond = 2, out_physics_bond=1, virtual_bond_dim=100,\n",
    "#                   bias=False,label_position='center',init_std=0)\n",
    "#model = PEPS_einsum_uniform_shape(6,6,10,in_physics_bond=16,virtual_bond_dim=3,init_std=1)\n",
    "model = PEPS_einsum_uniform_shape_6x6_fast(10,in_physics_bond=16,virtual_bond_dim=7,init_std=1)\n",
    "#model  = AMPSShare(n=28*28, bond_dim=10, phys_dim=2)\n",
    "device = 'cuda'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _=torch.nn.init.orthogonal_(model.left_tensors)\n",
    "# _=torch.nn.init.orthogonal_(model.cent_tensors)\n",
    "# _=torch.nn.init.orthogonal_(model.rigt_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(x):\n",
    "    return (aggregation_patch(x,divide=4)-0.5)/0.5\n",
    "def preprocess_images(x):\n",
    "    return (1-(aggregation_patch(x[...,3:27,3:27],divide=4)))/30\n",
    "def preprocess_images(x):\n",
    "    return (1-(aggregation_patch(x[...,3:27,3:27],divide=4)))/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(124.4898, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "infiniter = DataSimfetcher(train_loader, device=device)\n",
    "image,label= infiniter.next()\n",
    "bs,c,w,h = image.shape\n",
    "with torch.no_grad():\n",
    "    binary     = preprocess_images(image)\n",
    "    #label = image.flatten().long()\n",
    "    #logits     = model(binary).reshape(bs*w*h,2)\n",
    "    print(model(binary).squeeze().norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log at log/test\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='29' class='' max='30', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      96.67% [29/30 1:49:28<03:46]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='609' class='' max='938', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      64.93% [609/938 02:19<01:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 000 \t Accu: 0.3740 \t Time: 226.52 s\n",
      "Epoch: 001 \t Accu: 0.3201 \t Time: 227.87 s\n",
      "Epoch: 002 \t Accu: 0.5996 \t Time: 226.47 s\n",
      "Epoch: 003 \t Accu: 0.6341 \t Time: 226.56 s\n",
      "Epoch: 004 \t Accu: 0.6629 \t Time: 226.07 s\n",
      "Epoch: 005 \t Accu: 0.7053 \t Time: 226.44 s\n",
      "Epoch: 006 \t Accu: 0.7368 \t Time: 226.36 s\n",
      "Epoch: 007 \t Accu: 0.7535 \t Time: 226.37 s\n",
      "Epoch: 008 \t Accu: 0.7505 \t Time: 226.38 s\n",
      "Epoch: 009 \t Accu: 0.7833 \t Time: 226.37 s\n",
      "Epoch: 010 \t Accu: 0.7909 \t Time: 226.07 s\n",
      "Epoch: 011 \t Accu: 0.7823 \t Time: 226.17 s\n",
      "Epoch: 012 \t Accu: 0.7685 \t Time: 226.26 s\n",
      "Epoch: 013 \t Accu: 0.7763 \t Time: 226.32 s\n",
      "Epoch: 014 \t Accu: 0.7887 \t Time: 226.48 s\n",
      "Epoch: 015 \t Accu: 0.7859 \t Time: 226.69 s\n",
      "Epoch: 016 \t Accu: 0.7973 \t Time: 226.70 s\n",
      "Epoch: 017 \t Accu: 0.7896 \t Time: 226.86 s\n",
      "Epoch: 018 \t Accu: 0.7442 \t Time: 226.65 s\n",
      "Epoch: 019 \t Accu: 0.7953 \t Time: 226.77 s\n",
      "Epoch: 020 \t Accu: 0.7804 \t Time: 226.84 s\n",
      "Epoch: 021 \t Accu: 0.7657 \t Time: 226.94 s\n",
      "Epoch: 022 \t Accu: 0.8135 \t Time: 226.17 s\n",
      "Epoch: 023 \t Accu: 0.7810 \t Time: 226.06 s\n",
      "Epoch: 024 \t Accu: 0.8152 \t Time: 226.21 s\n",
      "Epoch: 025 \t Accu: 0.8195 \t Time: 226.35 s\n",
      "Epoch: 026 \t Accu: 0.7774 \t Time: 226.35 s\n",
      "Epoch: 027 \t Accu: 0.7648 \t Time: 226.55 s\n",
      "Epoch: 028 \t Accu: 0.7747 \t Time: 226.49 s\r"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "logsys            = LoggingSystem(True,\"log/test\")\n",
    "metric_list       = ['loss','accu']\n",
    "lses=[]\n",
    "lres=[]\n",
    "metric_dict       = logsys.initial_metric_dict(metric_list)\n",
    "master_bar        = logsys.create_master_bar(30)\n",
    "master_bar.set_multiply_graph(figsize=(12,4),engine=[['plot']*len(metric_list)],labels=[metric_list])\n",
    "for epoch in master_bar:\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    infiniter = DataSimfetcher(train_loader, device=device)\n",
    "    inter_b   = logsys.create_progress_bar(len(train_loader))\n",
    "    while inter_b.update_step():\n",
    "        image,label= infiniter.next()\n",
    "        bs,c,w,h = image.shape\n",
    "        optimizer.zero_grad()\n",
    "        binary     = preprocess_images(image)\n",
    "        #label = image.flatten().long()\n",
    "        #logits     = model(binary).reshape(bs*w*h,2)\n",
    "        logits     = model(binary).squeeze()\n",
    "        loss       = torch.nn.CrossEntropyLoss()(logits,label)\n",
    "        loss.backward()\n",
    "        if torch.isnan(loss):raise \n",
    "        \n",
    "            \n",
    "        #nn.utils.clip_grad_norm_(p, max_norm=1)\n",
    "        optimizer.step()\n",
    "        lses.append(loss.log().item())\n",
    "        #lres.append(sum([(p.grad/p).norm() for p in model.parameters()]).log().item())\n",
    "        if inter_b.now%20==0:\n",
    "            master_bar.update_graph_multiply([[lses[-100:],lres[-100:]]])\n",
    "    model.eval()\n",
    "    prefetcher = DataSimfetcher(test_loader, device=device)\n",
    "    inter_b    = logsys.create_progress_bar(len(test_loader))\n",
    "    labels     = []\n",
    "    logits     = []\n",
    "    with torch.no_grad():\n",
    "        while inter_b.update_step():\n",
    "            image,label= prefetcher.next()\n",
    "            binary     = preprocess_images(image)\n",
    "            logit      = model(binary).squeeze()\n",
    "            loss       = torch.nn.CrossEntropyLoss()(logit ,label)\n",
    "            labels.append(label)\n",
    "            logits.append(logit)\n",
    "    labels  = torch.cat(labels)\n",
    "    logits  = torch.cat(logits)\n",
    "    pred_labels  = torch.argmax(logits,-1)\n",
    "    accu =  torch.sum(pred_labels == labels)/len(labels)\n",
    "\n",
    "    lres.append(accu)\n",
    "    master_bar.update_graph_multiply([[lses,lres]])\n",
    "    print('\\nEpoch: %.3i \\t Accu: %.4f \\t Time: %.2f s' %(epoch, accu.item(), time.time() - start_time),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sample(self, bs, random_start=False):\n",
    "    \"\"\"\n",
    "    Sample images/spin configurations\n",
    "    \"\"\"\n",
    "\n",
    "    device = self.tensors.device\n",
    "    samples = torch.empty([bs, self.n], device=device)\n",
    "\n",
    "    # if random_start = True, force s_1 = -1/+1 randomly\n",
    "    if random_start:\n",
    "        samples[:, 0] = torch.randint(2, size=(bs, ), dtype=torch.float, device=device)\n",
    "    else:\n",
    "        samples[:, 0] = 0.\n",
    "\n",
    "    for idx in range(self.n - 1):\n",
    "        if idx == 0:\n",
    "            # sample s_2 from p(s_2 | s_1)\n",
    "            embedded_data = torch.stack([samples[:, 0], 1.0 - samples[:, 0]], dim=1)  # (bs, 2)\n",
    "            mats          = torch.einsum('lri,bi->blr', self.tensors[0, :, :, :] , embedded_data)\n",
    "            left_vec      = mats[:, 0, :].unsqueeze(1)  # (bs, 1, D)\n",
    "            logits        = torch.einsum('blr, ri->bli', left_vec,(self.tensors[1, :, :, :] )[:, 0, :]).squeeze(1)\n",
    "            samples[:, 1] = torch.bernoulli(torch.softmax(logits, dim=1)[:, 0])\n",
    "        else:\n",
    "            # then sample s_3 from  p(s_3 | s_1, s_2) and so on\n",
    "            embedded_data = torch.stack([samples[:, idx], 1.0 - samples[:, idx]], dim=1)  # (bs, 2)\n",
    "            mats = torch.einsum('lri,bi->blr', self.tensors[idx, :, :, :] , embedded_data)\n",
    "            left_vec = torch.bmm(left_vec, mats)  # (bs, 1, D)\n",
    "            logits = torch.einsum('blr, ri->bli', left_vec,\n",
    "                                  (self.tensors[idx + 1, :, :, :] )[:, 0, :]).squeeze(1)\n",
    "            samples[:, idx + 1] = torch.bernoulli(torch.softmax(logits, dim=1)[:, 0])\n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
