{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook test different contractor performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Contractor Test perodic condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=4;L=H=W=5;B=10;\n",
    "tn2D_shape_list = [[(D,D)]+[(D,D,D)]*(H-1)]+[[(D,D,D)]+[(D,D,D,D)]*(H-1)]*(W-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path,sublist_list,outlist = get_best_path(tn2D_shape_list,store=path_recorder,type='sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tensornetwork as tn\n",
    "import opt_einsum as oe\n",
    "import os,json\n",
    "from tensornetwork.network_components import get_all_nondangling,get_all_dangling\n",
    "from tensornetwork.contractors.opt_einsum_paths.utils import get_subgraph_dangling,get_all_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation   = \"kab,kcdb,kefcg,kgha->khedf\"\n",
    "tensor_l   = [torch.randn(B,D,D) ,torch.randn(B,D,D,D),torch.randn(B,D,D,D,D),torch.randn(B,D,D,D)]\n",
    "oe.contract_path(equation, *tensor_l)[0]\n",
    "path = oe.contract_path(equation, *tensor_l)[0]\n",
    "oe.contract(equation,*tensor_l,optimize=path).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    x =  torch.autograd.Variable(x,requires_grad=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path_recorder={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from models.model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tn2D_shape_list = [[(D,D)]+[(D,D,D)]*(H-1)]+[[(D,D,D)]+[(D,D,D,D)]*(H-1)]*(W-1)\n",
    "path,sublist_list,outlist = get_best_path(tn2D_shape_list,store=path_recorder,type='sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tensor_list     = [rd_engine(B,*l) for t in tn2D_shape_list for l in t]\n",
    "operands=[]\n",
    "for tensor,sublist in zip(tensor_list,sublist_list):\n",
    "    operand = [tensor,[...,*sublist]]\n",
    "    operands+=operand\n",
    "operands+= [[...,*outlist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oe.contract(*operands,optimize=path).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### test_the_TRG_on_even_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Contractor2D/test_the_TRG_on_even_number.py\n"
     ]
    }
   ],
   "source": [
    "# #%%writefile Contractor2D/test_the_TRG_on_even_number.py\n",
    "# import torch\n",
    "# from Contractor2D.utils import apply_SVD\n",
    "\n",
    "# tensor = torch.randn(4,4,2,2,2,2)\n",
    "\n",
    "# print(uniform_shape_tensor_contractor_tn(tensor))\n",
    "\n",
    "# truncate= None\n",
    "# lu_tensor = tensor[0::2,0::2]\n",
    "# ld_tensor = tensor[0::2,1::2]\n",
    "# ru_tensor = tensor[1::2,0::2]\n",
    "# rd_tensor = tensor[1::2,1::2]\n",
    "# lu_lu,lu_rd = apply_SVD(lu_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk ,kcd\n",
    "# rd_lu,rd_rd = apply_SVD(rd_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk, kcd\n",
    "# ld_ld,ld_ru = apply_SVD(ld_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "# ru_ld,ru_ru = apply_SVD(ru_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "# tensor1     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                             lu_rd,\n",
    "#                             ld_ru,\n",
    "#                             rd_lu,\n",
    "#                             ru_ld)\n",
    "# tensor2     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                            rd_rd,\n",
    "#                            ru_ru.roll(-1,1),\n",
    "#                            lu_lu.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "#                            ld_ld.roll(-1,0))\n",
    "\n",
    "# print(tensor1_and_tensor2_contractor_tn(tensor1,tensor2))\n",
    "\n",
    "# #print(torch.einsum(\"abcd,cdab->\",tensor1[0,0],tensor2[0,0]))\n",
    "# left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "# lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# # new_tensor  = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "# #                            right,\n",
    "# #                            uppe,\n",
    "# #                            left.roll(shifts=-1,dims=0),\n",
    "# #                            lower.roll(shifts=1,dims=1))\n",
    "# new_tensor  = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\",\n",
    "#                            lower,\n",
    "#                            right.roll(-1,1),\n",
    "#                            uppe.roll(-1,1),\n",
    "#                            left.roll(shifts=(-1, -1), dims=(0, 1))\n",
    "#                            )\n",
    "# print(uniform_shape_tensor_contractor_tn(new_tensor))\n",
    "\n",
    "# torch.einsum(\"abn,nji,lkh,hdc,caw,wkj,ilm,mbd->\",\n",
    "#              lu_lu[0,0],lu_rd[0,0],\n",
    "#              rd_lu[0,0],rd_rd[0,0],\n",
    "#              ld_ld[0,0],ld_ru[0,0],\n",
    "#              ru_ld[0,0],ru_ru[0,0])\n",
    "\n",
    "# torch.einsum(\"abji,lkdc,jcak,dilb->\",\n",
    "#              torch.einsum(\"abn,nji->abji\",lu_lu[0,0],lu_rd[0,0]),\n",
    "#              torch.einsum(\"lkh,hdc->lkdc\",rd_lu[0,0],rd_rd[0,0]),\n",
    "#              torch.einsum(\"caw,wkj->jcak\",ld_ld[0,0],ld_ru[0,0]),\n",
    "#              torch.einsum(\"ilm,mbd->dilb\",ru_ld[0,0],ru_ru[0,0]))\n",
    "\n",
    "# torch.dist(torch.einsum(\"abn,nji->abji\",lu_lu[0,0],lu_rd[0,0]),lu_tensor)\n",
    "\n",
    "# torch.dist(torch.einsum(\"lkh,hdc->lkdc\",rd_lu[0,0],rd_rd[0,0]),rd_tensor)\n",
    "\n",
    "# torch.dist(torch.einsum(\"caw,wkj->jcak\",ld_ld[0,0],ld_ru[0,0]),ld_tensor)\n",
    "\n",
    "# torch.dist(torch.einsum(\"ilm,mbd->dilb\",ru_ld[0,0],ru_ru[0,0]),ru_tensor)\n",
    "\n",
    "# torch.dist(lu_tensor[0,0],tensor[0,0])\n",
    "\n",
    "# torch.dist(rd_tensor[0,0],tensor[1,1])\n",
    "\n",
    "# torch.dist(ld_tensor[0,0],tensor[0,1])\n",
    "\n",
    "# torch.dist(ru_tensor[0,0],tensor[1,0])\n",
    "\n",
    "# torch.einsum(\"abji,lkdc,jcak,dilb->\",\n",
    "#              torch.einsum(\"abn,nji->abji\",lu_lu[0,0],lu_rd[0,0]),\n",
    "#              torch.einsum(\"lkh,hdc->lkdc\",rd_lu[0,0],rd_rd[0,0]),\n",
    "#              torch.einsum(\"caw,wkj->jcak\",ld_ld[0,0],ld_ru[0,0]),\n",
    "#              torch.einsum(\"ilm,mbd->dilb\",ru_ld[0,0],ru_ru[0,0]))\n",
    "\n",
    "# torch.einsum(\"abcd,idjb,ckal,jlik->\",lu_tensor[0,0],ld_tensor[0,0],ru_tensor[0,0],rd_tensor[0,0])\n",
    "\n",
    "# torch.einsum(\"abcd,ciaj,ldkb,kjli->\",lu_tensor[0,0],ld_tensor[0,0],ru_tensor[0,0],rd_tensor[0,0])\n",
    "\n",
    "# tensor1 = tensor[0,0]\n",
    "# tensor2 = tensor[0,1]\n",
    "# tensor3 = tensor[1,0]\n",
    "# tensor4 = tensor[1,1]\n",
    "# torch.einsum(\"abcd,idjb,ckal,jlik->\",tensor1,tensor2,tensor3,tensor4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### check_the_brdc_coding_on_even_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Contractor2D/check_the_brdc_coding_on_even_number.py\n"
     ]
    }
   ],
   "source": [
    "# #%%writefile Contractor2D/check_the_brdc_coding_on_even_number.py\n",
    "# import torch\n",
    "# from Contractor2D.utils import apply_SVD\n",
    "\n",
    "# tensor     = torch.randn(8,8,2,2,2,2)/2\n",
    "# uniform_shape_tensor_contractor_tn(tensor)\n",
    "\n",
    "# lu_lu_origin,lu_rd_origin = apply_SVD(tensor[0::2,0::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk ,kcd\n",
    "# rd_lu_origin,rd_rd_origin = apply_SVD(tensor[1::2,1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld_origin,ld_ru_origin = apply_SVD(tensor[0::2,1::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# ru_ld_origin,ru_ru_origin = apply_SVD(tensor[1::2,0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "\n",
    "# lu_lu,lu_rd = apply_SVD(bulk_tensor[0::2,0::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk ,kcd\n",
    "# rd_lu,rd_rd = apply_SVD(bulk_tensor[1::2,1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld,ld_ru = apply_SVD(bulk_tensor[0::2,1::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# ru_ld,ru_ru = apply_SVD(bulk_tensor[1::2,0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "\n",
    "# print(lu_lu.shape)\n",
    "# print(rd_lu.shape)\n",
    "# print(ld_ld.shape)\n",
    "# print(ru_ld.shape)\n",
    "\n",
    "# lu_lu_should = lu_lu_origin[:,:]    ;lu_rd_should =lu_rd_origin[:,:]\n",
    "# rd_lu_should = rd_lu_origin[:-1,:-1];rd_rd_should =rd_rd_origin[:-1,:-1]\n",
    "# ld_ld_should = ld_ld_origin[:,:-1]  ;ld_ru_should =ld_ru_origin[:,:-1]\n",
    "# ru_ld_should = ru_ld_origin[:-1,:]  ;ru_ru_should =ru_ru_origin[:-1,:]\n",
    "\n",
    "# print(lu_lu_should.shape)\n",
    "# print(rd_lu_should.shape)\n",
    "# print(ld_ld_should.shape)\n",
    "# print(ru_ld_should.shape)\n",
    "\n",
    "# print(torch.dist(lu_lu_should,lu_lu),torch.dist(lu_rd_should,lu_rd))\n",
    "# print(torch.dist(rd_lu_should,rd_lu),torch.dist(rd_rd_should,rd_rd))\n",
    "# print(torch.dist(ld_ld_should,ld_ld),torch.dist(ld_ru_should,ld_ru))\n",
    "# print(torch.dist(ru_ld_should,ru_ld),torch.dist(ru_ru_should,ru_ru))\n",
    "\n",
    "# # right_edge == bulk_tensor[-1]\n",
    "# # down_edge  == bulk_tensor[:,-1]\n",
    "\n",
    "# rd_lu_r,rd_rd_r = apply_SVD(right_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ru_ld_r,ru_ru_r = apply_SVD(right_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_d,rd_rd_d = apply_SVD( down_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld_d,ld_ru_d = apply_SVD( down_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_c,rd_rd_c = apply_SVD(corn_tensor     ,left_bond=[0,1],right_bond=[2,3],truncate=None)# bck, kda\n",
    "\n",
    "# rd_lu_r_should=rd_lu_origin[-1,:-1];rd_rd_r_should=rd_rd_origin[-1,:-1]\n",
    "# ru_ld_r_should=ru_ld_origin[-1,:]  ;ru_ru_r_should=ru_ru_origin[-1,:]\n",
    "# rd_lu_d_should=rd_lu_origin[:-1,-1];rd_rd_d_should=rd_rd_origin[:-1,-1]\n",
    "# ld_ld_d_should=ld_ld_origin[:,-1]  ;ld_ru_d_should=ld_ru_origin[:,-1]\n",
    "# rd_lu_c_should=rd_lu_origin[ -1,-1];rd_rd_c_should=rd_rd_origin[ -1,-1]\n",
    "\n",
    "# print(rd_lu_r_should.shape)\n",
    "# print(ru_ld_r_should.shape)\n",
    "# print(rd_lu_d_should.shape)\n",
    "# print(ld_ld_d_should.shape)\n",
    "# print(rd_lu_c_should.shape)\n",
    "\n",
    "# print(torch.dist(rd_lu_r_should,rd_lu_r),torch.dist(rd_rd_r_should,rd_rd_r))\n",
    "# print(torch.dist(ru_ld_r_should,ru_ld_r),torch.dist(ru_ru_r_should,ru_ru_r))\n",
    "# print(torch.dist(rd_lu_d_should,rd_lu_d),torch.dist(rd_rd_d_should,rd_rd_d))\n",
    "# print(torch.dist(ld_ld_d_should,ld_ld_d),torch.dist(ld_ru_d_should,ld_ru_d))\n",
    "# print(torch.dist(rd_lu_c_should,rd_lu_c),torch.dist(rd_rd_c_should,rd_rd_c))\n",
    "\n",
    "# tensor1     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                                     lu_rd_origin,\n",
    "#                                     ld_ru_origin,\n",
    "#                                     rd_lu_origin,\n",
    "#                                     ru_ld_origin)\n",
    "# tensor2     = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "#                            rd_rd_origin,\n",
    "#                            ru_ru_origin.roll(shifts=-1,dims=1),\n",
    "#                            lu_lu_origin.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "#                            ld_ld_origin.roll(shifts=-1,dims=0))\n",
    "\n",
    "# bulk_tensor1_should= tensor1[:-1,:-1]\n",
    "# rigt_tensor1_should= tensor1[-1,:-1]\n",
    "# down_tensor1_should= tensor1[:-1,-1]\n",
    "# corn_tensor1_should= tensor1[-1,-1]\n",
    "# bulk_tensor2_should= tensor2[:-1,:-1]\n",
    "# rigt_tensor2_should= tensor2[-1,:-1]\n",
    "# down_tensor2_should= tensor2[:-1,-1]\n",
    "# corn_tensor2_should= tensor2[-1,-1]\n",
    "\n",
    "# print(bulk_tensor1_should.shape)\n",
    "# print(rigt_tensor1_should.shape)  \n",
    "# print(down_tensor1_should.shape)  \n",
    "# print(corn_tensor1_should.shape)  \n",
    "# print(bulk_tensor2_should.shape)  \n",
    "# print(rigt_tensor2_should.shape)  \n",
    "# print(down_tensor2_should.shape)  \n",
    "# print(corn_tensor2_should.shape)  \n",
    "\n",
    "# bulk_tensor1 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",lu_rd[:-1,:-1],ld_ru[:-1],rd_lu,ru_ld[:,:-1])\n",
    "# rigt_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[-1,:-1],ld_ru[-1]   ,rd_lu_r, ru_ld_r[:-1])\n",
    "# down_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[:-1,-1],ld_ru_d[:-1],rd_lu_d, ru_ld[:,-1])\n",
    "# corn_tensor1 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",lu_rd[-1,-1] ,ld_ru_d[-1] ,rd_lu_c, ru_ld_r[-1])\n",
    "# bulk_tensor2 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",rd_rd,ru_ru[:,1:],lu_lu[1:,1:],ld_ld[1:])\n",
    "# rigt_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_r, ru_ru_r[1:],lu_lu[0,1:], ld_ld[0])\n",
    "# down_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_d, ru_ru[:,0],lu_lu[1:,0], ld_ld_d[1:])\n",
    "# corn_tensor2 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",rd_rd_c, ru_ru_r[0],lu_lu[0,0] , ld_ld_d[0])\n",
    "\n",
    "# print(bulk_tensor1.shape)\n",
    "# print(rigt_tensor1.shape)  \n",
    "# print(down_tensor1.shape)  \n",
    "# print(corn_tensor1.shape)  \n",
    "# print(bulk_tensor2.shape)  \n",
    "# print(rigt_tensor2.shape)  \n",
    "# print(down_tensor2.shape)  \n",
    "# print(corn_tensor2.shape)  \n",
    "\n",
    "# print(torch.dist(bulk_tensor1,bulk_tensor1_should))\n",
    "# print(torch.dist(rigt_tensor1,rigt_tensor1_should))  \n",
    "# print(torch.dist(down_tensor1,down_tensor1_should))  \n",
    "# print(torch.dist(corn_tensor1,corn_tensor1_should))  \n",
    "# print(torch.dist(bulk_tensor2,bulk_tensor2_should))  \n",
    "# print(torch.dist(rigt_tensor2,rigt_tensor2_should))  \n",
    "# print(torch.dist(down_tensor2,down_tensor2_should))  \n",
    "# print(torch.dist(corn_tensor2,corn_tensor2_should)) \n",
    "\n",
    "# computer_vie_tn(tensor)\n",
    "\n",
    "# truncate = None\n",
    "# bulk_left ,bulk_right= apply_SVD(bulk_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# rigt_left ,rigt_right= apply_SVD(rigt_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# down_left ,down_right= apply_SVD(down_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# corn_left ,corn_right= apply_SVD(corn_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# bulk_lower,bulk_uppe = apply_SVD(bulk_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# rigt_lower,rigt_uppe = apply_SVD(rigt_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# down_lower,down_uppe = apply_SVD(down_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# corn_lower,corn_uppe = apply_SVD(corn_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "\n",
    "# bulk_tensor           = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\", bulk_lower[:-1,:-1],bulk_right[:-1,1:],bulk_uppe[:-1,1:],bulk_left[1:,1:])\n",
    "# rigt_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , rigt_lower[:-1]    ,rigt_right[1:]    ,rigt_uppe[1:]    ,bulk_left[0,1:])\n",
    "# down_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , down_lower[:-1]    ,bulk_right[:-1,0] ,bulk_uppe[:-1,0] ,bulk_left[1:,0])\n",
    "# bulk_rigt_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[-1,:-1] ,bulk_right[-1,1:] ,bulk_uppe[-1,1:] ,rigt_left[1:])\n",
    "# bulk_down_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[:-1,-1] ,down_right[:-1]   ,down_uppe[:-1]   ,down_left[1:])\n",
    "# bulk_down_corn_tensor = torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , bulk_lower[-1,-1]  ,down_right[-1]    ,down_uppe[-1]    ,corn_left)\n",
    "# corn_right_bulk_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , corn_lower         ,rigt_right[0]     ,rigt_uppe[0]     ,bulk_left[0,0])\n",
    "# right_corn_down_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , rigt_lower[-1]     ,corn_right        ,corn_uppe        ,down_left[0])\n",
    "# down_bulk_right_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , down_lower[-1]     ,bulk_right[-1,0]  ,bulk_uppe[-1,0]  ,rigt_left[0])\n",
    "\n",
    "# left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "# lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# new_tensor_should  = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\",\n",
    "#                    lower,\n",
    "#                    right.roll(-1,1),\n",
    "#                    uppe.roll(-1,1),\n",
    "#                    left.roll(shifts=(-1, -1), dims=(0, 1))\n",
    "#                    )\n",
    "\n",
    "# print(bulk_tensor.shape)           \n",
    "# print(rigt_bulk_tensor.shape)      \n",
    "# print(down_bulk_tensor.shape)      \n",
    "# print(bulk_rigt_tensor.shape)      \n",
    "# print(bulk_down_tensor.shape)      \n",
    "# print(bulk_down_corn_tensor.shape) \n",
    "# print(corn_right_bulk_tensor.shape)\n",
    "# print(right_corn_down_tensor.shape)\n",
    "# print(down_bulk_right_tensor.shape)\n",
    "\n",
    "# bulk_tensor_should            = new_tensor_should[:-2, :-2]\n",
    "# rigt_bulk_tensor_should       = new_tensor_should[-1,:-2]\n",
    "# down_bulk_tensor_should       = new_tensor_should[:-2,-1]\n",
    "# bulk_rigt_tensor_should       = new_tensor_should[-2,:-2]\n",
    "# bulk_down_tensor_should       = new_tensor_should[:-2,-2]\n",
    "# bulk_down_corn_tensor_should  = new_tensor_should[-2,-2]\n",
    "# corn_right_bulk_tensor_should = new_tensor_should[-1,-1]\n",
    "# right_corn_down_tensor_should = new_tensor_should[-1,-2]\n",
    "# down_bulk_right_tensor_should = new_tensor_should[-2,-1]\n",
    "\n",
    "# print(bulk_tensor_should.shape           )\n",
    "# print(rigt_bulk_tensor_should.shape      )\n",
    "# print(down_bulk_tensor_should.shape      )\n",
    "# print(bulk_rigt_tensor_should.shape      )\n",
    "# print(bulk_down_tensor_should.shape      )\n",
    "# print(bulk_down_corn_tensor_should.shape )\n",
    "# print(corn_right_bulk_tensor_should.shape)\n",
    "# print(right_corn_down_tensor_should.shape)\n",
    "# print(down_bulk_right_tensor_should.shape)\n",
    "\n",
    "# print(torch.dist(bulk_tensor_should            ,bulk_tensor           ))\n",
    "# print(torch.dist(rigt_bulk_tensor_should       ,rigt_bulk_tensor      ))\n",
    "# print(torch.dist(down_bulk_tensor_should       ,down_bulk_tensor      ))\n",
    "# print(torch.dist(bulk_rigt_tensor_should       ,bulk_rigt_tensor      ))\n",
    "# print(torch.dist(bulk_down_tensor_should       ,bulk_down_tensor      ))\n",
    "# print(torch.dist(bulk_down_corn_tensor_should  ,bulk_down_corn_tensor ))\n",
    "# print(torch.dist(corn_right_bulk_tensor_should ,corn_right_bulk_tensor))\n",
    "# print(torch.dist(right_corn_down_tensor_should ,right_corn_down_tensor))\n",
    "# print(torch.dist(down_bulk_right_tensor_should ,down_bulk_right_tensor))\n",
    "\n",
    "# new_tensor = torch.cat(\n",
    "#     [torch.cat([bulk_tensor,bulk_rigt_tensor.unsqueeze(0),rigt_bulk_tensor.unsqueeze(0)]),\n",
    "#      torch.cat([bulk_down_tensor,bulk_down_corn_tensor.unsqueeze(0),right_corn_down_tensor.unsqueeze(0)]).unsqueeze(1),\n",
    "#      torch.cat([down_bulk_tensor,down_bulk_right_tensor.unsqueeze(0),corn_right_bulk_tensor.unsqueeze(0)]).unsqueeze(1),\n",
    "#     ],dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #%%writefile Contractor2D/test_for_TRG_odd_number.py\n",
    "# import torch\n",
    "\n",
    "# from Contractor2D.utils import apply_SVD\n",
    "\n",
    "# tensor = torch.randn(5,5,2,2,2,2)\n",
    "# print(f\"the input tensor network store in {tensor.shape}\")\n",
    "# print(f\"the ultimate result should be {uniform_shape_tensor_contractor_tn(tensor)}\")\n",
    "\n",
    "\n",
    "# right_edge1,right_edge2 = tensor[-2:]\n",
    "# right_edge = torch.einsum(\"wabcd,wedfg->waebcfg\",right_edge1,right_edge2).flatten(1,2).flatten(-3,-2)\n",
    "# down_edge1, down_edge2 = tensor[:-2,-2:].transpose(1,0)\n",
    "# corn_tensor1,corn_tensor2= right_edge[-2:]\n",
    "# bulk_tensor= tensor[:-2,:-2]\n",
    "# right_edge = right_edge[:-2]\n",
    "# down_edge  = torch.einsum(\"wabcd,wcefg->wabefdg\",down_edge1,down_edge2).flatten(2,3).flatten(-2,-1)\n",
    "# corn_tensor= torch.einsum(\"abcd,cefg->abefdg\",corn_tensor1,corn_tensor2).flatten(1,2).flatten(-2,-1)\n",
    "\n",
    "# print(f\"the input bulk_right_left_corner tensor :\")\n",
    "# print(f\"bulk_tensor.shape = {bulk_tensor.shape}\")\n",
    "# print(f\"right_edge.shape  = {right_edge.shape }\")\n",
    "# print(f\"down_edge.shape   = {down_edge.shape  }\")\n",
    "# print(f\"corn_tensor.shape = {corn_tensor.shape}\")\n",
    "\n",
    "# print(f\"the bulk_right_left_corner_contractor result {bulk_right_left_corner_contractor_tn(bulk_tensor,right_edge,down_edge,corn_tensor)}\")\n",
    "\n",
    "# lu_lu,lu_rd = apply_SVD(bulk_tensor[0::2,0::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk ,kcd\n",
    "# rd_lu,rd_rd = apply_SVD(bulk_tensor[1::2,1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld,ld_ru = apply_SVD(bulk_tensor[0::2,1::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# ru_ld,ru_ru = apply_SVD(bulk_tensor[1::2,0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "\n",
    "# rd_lu_r,rd_rd_r = apply_SVD(right_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ru_ld_r,ru_ru_r = apply_SVD(right_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_d,rd_rd_d = apply_SVD( down_edge[1::2],left_bond=[0,1],right_bond=[2,3],truncate=None)# abk, kcd\n",
    "# ld_ld_d,ld_ru_d = apply_SVD( down_edge[0::2],left_bond=[1,2],right_bond=[3,0],truncate=None)# bck, kda\n",
    "# rd_lu_c,rd_rd_c = apply_SVD(corn_tensor     ,left_bond=[0,1],right_bond=[2,3],truncate=None)# bck, kda\n",
    "\n",
    "# bulk_tensor1 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",lu_rd[:-1,:-1],ld_ru[:-1],rd_lu,ru_ld[:,:-1])\n",
    "# rigt_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[-1,:-1],ld_ru[-1]   ,rd_lu_r, ru_ld_r[:-1])\n",
    "# down_tensor1 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",lu_rd[:-1,-1],ld_ru_d[:-1],rd_lu_d, ru_ld[:,-1])\n",
    "# corn_tensor1 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",lu_rd[-1,-1] ,ld_ru_d[-1] ,rd_lu_c, ru_ld_r[-1])\n",
    "# bulk_tensor2 = torch.einsum(\"whicd,whjac,whbak,whdbl->whijkl\",rd_rd,ru_ru[:,1:],lu_lu[1:,1:],ld_ld[1:])\n",
    "# rigt_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_r, ru_ru_r[1:],lu_lu[0,1:], ld_ld[0])\n",
    "# down_tensor2 = torch.einsum(\"wicd,wjac,wbak,wdbl->wijkl\",rd_rd_d, ru_ru[:,0],lu_lu[1:,0], ld_ld_d[1:])\n",
    "# corn_tensor2 = torch.einsum(\" icd, jac, bak, dbl-> ijkl\",rd_rd_c, ru_ru_r[0],lu_lu[0,0] , ld_ld_d[0])\n",
    "\n",
    "# print(\"phase1:result\")\n",
    "# print(f\"bulk_tensor1.shape={bulk_tensor1.shape}\")\n",
    "# print(f\"rigt_tensor1.shape={rigt_tensor1.shape}\")\n",
    "# print(f\"down_tensor1.shape={down_tensor1.shape}\")\n",
    "# print(f\"corn_tensor1.shape={corn_tensor1.shape}\")\n",
    "# print()\n",
    "# print(f\"bulk_tensor2.shape={bulk_tensor2.shape}\")\n",
    "# print(f\"rigt_tensor2.shape={rigt_tensor2.shape}\")\n",
    "# print(f\"down_tensor2.shape={down_tensor2.shape}\")\n",
    "# print(f\"corn_tensor2.shape={corn_tensor2.shape}\")\n",
    "\n",
    "# # if we don't truncate at this step, the map will become too complex and can not code as unique way,\n",
    "# # so in such case, we will require truncate 16 in here and the recommend input not big than 32.\n",
    "# truncate = None\n",
    "# bulk_left ,bulk_right= apply_SVD(bulk_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# rigt_left ,rigt_right= apply_SVD(rigt_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# down_left ,down_right= apply_SVD(down_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# corn_left ,corn_right= apply_SVD(corn_tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)\n",
    "# bulk_lower,bulk_uppe = apply_SVD(bulk_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# rigt_lower,rigt_uppe = apply_SVD(rigt_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# down_lower,down_uppe = apply_SVD(down_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "# corn_lower,corn_uppe = apply_SVD(corn_tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "\n",
    "# bulk_tensor_cent      = torch.einsum(\"whadi,whjba,whkcb,whdcl->whijkl\", bulk_lower[:-1,:-1],bulk_right[:-1,1:],bulk_uppe[:-1,1:],bulk_left[1:,1:])\n",
    "# rigt_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , rigt_lower[:-1]    ,rigt_right[1:]    ,rigt_uppe[1:]    ,bulk_left[0,1:])\n",
    "# down_bulk_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , down_lower[:-1]    ,bulk_right[:-1,0] ,bulk_uppe[:-1,0] ,bulk_left[1:,0])\n",
    "# bulk_rigt_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[-1,:-1] ,bulk_right[-1,1:] ,bulk_uppe[-1,1:] ,rigt_left[1:])\n",
    "# bulk_down_tensor      = torch.einsum(\"hadi,hjba,hkcb,hdcl->hijkl\"     , bulk_lower[:-1,-1] ,down_right[:-1]   ,down_uppe[:-1]   ,down_left[1:])\n",
    "# bulk_down_corn_tensor = torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , bulk_lower[-1,-1]  ,down_right[-1]    ,down_uppe[-1]    ,corn_left)\n",
    "# corn_right_bulk_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , corn_lower         ,rigt_right[0]     ,rigt_uppe[0]     ,bulk_left[0,0])\n",
    "# right_corn_down_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , rigt_lower[-1]     ,corn_right        ,corn_uppe        ,down_left[0])\n",
    "# down_bulk_right_tensor= torch.einsum(\"adi,jba,kcb,dcl->ijkl\"          , down_lower[-1]     ,bulk_right[-1,0]  ,bulk_uppe[-1,0]  ,rigt_left[0])\n",
    "# # down_bulk_tensor       = down_bulk_tensor[None]\n",
    "# # down_bulk_right_tensor = down_bulk_right_tensor[None]\n",
    "# # corn_right_bulk_tensor = corn_right_bulk_tensor[None]\n",
    "# # bulk_down_corn_tensor  = bulk_down_corn_tensor[None]\n",
    "\n",
    "# print(\"phase2 result:\")\n",
    "# print(f\"bulk_tensor_cent.shape      ={bulk_tensor_cent.shape      }\")\n",
    "# print(f\"down_bulk_tensor.shape      ={down_bulk_tensor.shape      }\")      \n",
    "# print(f\"bulk_rigt_tensor.shape      ={bulk_rigt_tensor.shape      }\")   \n",
    "# print(f\"down_bulk_right_tensor.shape={down_bulk_right_tensor.shape}\")\n",
    "# print(f\"rigt_bulk_tensor.shape      ={rigt_bulk_tensor.shape      }\")      \n",
    "# print(f\"corn_right_bulk_tensor.shape={corn_right_bulk_tensor.shape}\")\n",
    "# print(f\"bulk_down_tensor.shape      ={bulk_down_tensor.shape      }\") \n",
    "# print(f\"bulk_down_corn_tensor.shape ={bulk_down_corn_tensor.shape }\") \n",
    "# print(f\"right_corn_down_tensor.shape={right_corn_down_tensor.shape}\")\n",
    "\n",
    "# bulk_tensor = torch.cat([\n",
    "#     torch.cat([bulk_tensor_cent,down_bulk_tensor[None]],1),\n",
    "#     torch.cat([bulk_rigt_tensor,down_bulk_right_tensor[None]])[None]\n",
    "# ])\n",
    "\n",
    "# bulk_tensor.shape\n",
    "\n",
    "# bulk_tensor = down_bulk_right_tensor[None][None]\n",
    "\n",
    "# right_edge = torch.cat([rigt_bulk_tensor,corn_right_bulk_tensor[None]])\n",
    "# down_edge  = torch.cat([bulk_down_tensor,bulk_down_corn_tensor[None]])\n",
    "# corn_tensor =right_corn_down_tensor\n",
    "\n",
    "# print(f\"next tensor shape:\")\n",
    "# print(f\"bulk_tensor.shape = {bulk_tensor.shape}\")\n",
    "# print(f\"right_edge.shape  = {right_edge.shape }\")\n",
    "# print(f\"down_edge.shape   = {down_edge.shape  }\")\n",
    "# print(f\"corn_tensor.shape = {corn_tensor.shape}\")\n",
    "\n",
    "# print(f\"next tensor contraction result:\")\n",
    "# print(bulk_right_left_corner_contractor_tn(bulk_tensor,right_edge,down_edge,corn_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### time test for different engine (only bulk $2^n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import torch_semiring_einsum\n",
    "# def TRT_semiring(tensor,truncate=None, block_size=5):\n",
    "#     W,H = tensor.shape[:2]\n",
    "#     einsum_1 = torch_semiring_einsum.compile_equation(\"whicd,whjac,whbak,whdbl->whijkl\")\n",
    "#     einsum_2 = torch_semiring_einsum.compile_equation(\"whadi,whjba,whkcb,whdcl->whijkl\")\n",
    "#     einsum_3 = torch_semiring_einsum.compile_equation(\"abcd,ciaj,ldkb,kjli->\")\n",
    "#     while True:\n",
    "#         print(tensor.shape)\n",
    "#         W,H = tensor.shape[:2]\n",
    "#         #if W<=1 or H<=1:break\n",
    "#         lu_tensor = tensor[0::2,0::2]\n",
    "#         ld_tensor = tensor[0::2,1::2]\n",
    "#         ru_tensor = tensor[1::2,0::2]\n",
    "#         rd_tensor = tensor[1::2,1::2]\n",
    "#         if W<=2 or H<=2:break\n",
    "            \n",
    "#         lu_lu,lu_rd = apply_SVD(lu_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk ,kcd\n",
    "#         rd_lu,rd_rd = apply_SVD(rd_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk, kcd\n",
    "#         ld_ld,ld_ru = apply_SVD(ld_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "#         ru_ld,ru_ru = apply_SVD(ru_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "#         tensor1     = torch_semiring_einsum.einsum(einsum_1,\n",
    "#                                     lu_rd,\n",
    "#                                     ld_ru,\n",
    "#                                     rd_lu,\n",
    "#                                     ru_ld,block_size=block_size)\n",
    "#         tensor2     = torch_semiring_einsum.einsum(einsum_1,\n",
    "#                                    rd_rd,\n",
    "#                                    ru_ru.roll(shifts=-1,dims=1),\n",
    "#                                    lu_lu.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "#                                    ld_ld.roll(shifts=-1,dims=0),block_size=block_size)\n",
    "#         left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "#         lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "#         tensor  = torch_semiring_einsum.einsum(einsum_2,\n",
    "#                            lower,\n",
    "#                            right.roll(-1,1),\n",
    "#                            uppe.roll(-1,1),\n",
    "#                            left.roll(shifts=(-1, -1), dims=(0, 1)),block_size=block_size\n",
    "#                            )\n",
    "#     value   = torch_semiring_einsum.einsum(einsum_3,\n",
    "#                            lu_tensor[0,0],ld_tensor[0,0],\n",
    "#                            ru_tensor[0,0],rd_tensor[0,0],block_size=block_size)\n",
    "#     #value = torch.einsum(\"abab->\",tensor[0,0])\n",
    "#     return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.randn(14,14,2,2,2,2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def TRT(tensor,truncate=None,einsum_engin=torch.einsum):\n",
    "    W,H = tensor.shape[:2]\n",
    "    while True:\n",
    "        print(tensor.shape)\n",
    "        W,H = tensor.shape[:2]\n",
    "        #if W<=1 or H<=1:break\n",
    "        lu_tensor = tensor[0::2,0::2]\n",
    "        ld_tensor = tensor[0::2,1::2]\n",
    "        ru_tensor = tensor[1::2,0::2]\n",
    "        rd_tensor = tensor[1::2,1::2]\n",
    "        if W<=2 or H<=2:break\n",
    "#         print(\"SVD:time\"),\n",
    "#         start_time  = time.time()\n",
    "        lu_lu,lu_rd = apply_SVD(lu_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk ,kcd\n",
    "        rd_lu,rd_rd = apply_SVD(rd_tensor,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# abk, kcd\n",
    "        ld_ld,ld_ru = apply_SVD(ld_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "        ru_ld,ru_ru = apply_SVD(ru_tensor,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# bck, kda\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)       \n",
    "#         print(\"einsum:time\"),\n",
    "#         start_time  = time.time()\n",
    "        tensor1     = einsum_engin(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "                                    lu_rd,\n",
    "                                    ld_ru,\n",
    "                                    rd_lu,\n",
    "                                    ru_ld)\n",
    "        tensor2     = einsum_engin(\"whicd,whjac,whbak,whdbl->whijkl\",\n",
    "                                   rd_rd,\n",
    "                                   ru_ru.roll(shifts=-1,dims=1),\n",
    "                                   lu_lu.roll(shifts=(-1, -1), dims=(0, 1)),\n",
    "                                   ld_ld.roll(shifts=-1,dims=0))\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)\n",
    "#         print(\"SVD:time\"),\n",
    "#         start_time  = time.time()\n",
    "        \n",
    "        left,right  = apply_SVD(tensor1,left_bond=[0,1],right_bond=[2,3],truncate=truncate)# ABK,KCD\n",
    "        lower,uppe  = apply_SVD(tensor2,left_bond=[1,2],right_bond=[3,0],truncate=truncate)# BCK,KDA\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)\n",
    "#         print(\"einsum:time\"),\n",
    "#         start_time  = time.time()\n",
    "        tensor  = einsum_engin(\"whadi,whjba,whkcb,whdcl->whijkl\",\n",
    "                           lower,\n",
    "                           right.roll(-1,1),\n",
    "                           uppe.roll(-1,1),\n",
    "                           left.roll(shifts=(-1, -1), dims=(0, 1))\n",
    "                           )\n",
    "#         cost = time.time() - start_time\n",
    "#         print(cost)\n",
    "    value   = einsum_engin(\"abcd,ciaj,ldkb,kjli->\",lu_tensor[0,0],ld_tensor[0,0],ru_tensor[0,0],rd_tensor[0,0])\n",
    "    #value = torch.einsum(\"abab->\",tensor[0,0])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.randn(14,14,2,2,2,2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "engine=lambda equ,*karg:torch_semiring_einsum.einsum(torch_semiring_einsum.compile_equation(equ),*karg,block_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 ms ± 10 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(4,4,2,2,2,2)\n",
    "%timeit TRT(tensor,truncate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.67 ms ± 1.43 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit TRT(tensor,truncate=None,einsum_engin=contract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Boundary MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def rd_engine(*x,**kargs):\n",
    "    x =  torch.randn(*x,device='cpu',**kargs)\n",
    "    x/=  torch.norm(x).sqrt()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=4;P=4;L=7;\n",
    "# top_mps_list    = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]\n",
    "# middle_mpo_list = [[rd_engine(P,D,P)]+[rd_engine(L-2,D,P,D,P)]+ [rd_engine(D,P,P)]\n",
    "#                   for _ in range(L-2)]\n",
    "# bottom_mps_list = [rd_engine(P,D)] + [rd_engine(L-2,D,P,D)]  + [rd_engine(D,P)]\n",
    "# peps  = [top_mps_list]+middle_mpo_list+[bottom_mps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D=4;P=4;L=7;\n",
    "top_mps_list    = [rd_engine(D,D)]     + [rd_engine(L-2,D,D,D)]  + [rd_engine(D,D)]\n",
    "middle_mpo_list = [[rd_engine(D,D,D)]  + [rd_engine(L-2,D,D,D,D)]+ [rd_engine(D,D,D)]\n",
    "                  for _ in range(L-2)]\n",
    "bottom_mps_list = [rd_engine(D,D)]     + [rd_engine(L-2,D,D,D)]  + [rd_engine(D,D)]\n",
    "peps  = [top_mps_list]+middle_mpo_list+[bottom_mps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     3,
     64,
     94,
     127,
     131,
     159
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from engine.torch_dense import approxmate_mps_line\n",
    "import torch\n",
    "import numpy as np\n",
    "def truncated_SVD(tensor,output='RQ',max_singular_values= None,\n",
    "                  max_truncation_error= None,\n",
    "                  relative = True,\n",
    "                  normlized= True,\n",
    "                  verbose  = False,auto_check_diagonal=False):\n",
    "    # the canonocal\n",
    "    # tensor is batched\n",
    "    reduce = False\n",
    "    u=s=v = None\n",
    "    if len(tensor.shape)==2:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        reduce = True\n",
    "    if auto_check_diagonal:\n",
    "        out = diagonal_tensor_svd_torch_dense(tensor)\n",
    "        if out is not None:u, s, v = out\n",
    "    if u is None: u, s, v = torch.svd(tensor)\n",
    "\n",
    "\n",
    "    max_singular_values = s.shape[-1] if max_singular_values is None else max_singular_values\n",
    "\n",
    "    if max_truncation_error is not None:\n",
    "        # Cumulative norms of singular values in ascending order\n",
    "        s_sorted, _ = torch.sort(s**2,-1)\n",
    "        trunc_errs  = torch.sqrt(torch.cumsum(s_sorted, -1))\n",
    "        # If relative is true, rescale max_truncation error with the largest\n",
    "        # singular value to yield the absolute maximal truncation error.\n",
    "        abs_max_truncation_error = max_truncation_error * s[:,0:1] if relative else max_truncation_error\n",
    "        # We must keep at least this many singular values to ensure the\n",
    "        # truncation error is <= abs_max_truncation_error.\n",
    "        num_sing_vals_err = torch.sum(trunc_errs > abs_max_truncation_error,-1).max()\n",
    "        if max_singular_values>num_sing_vals_err and verbose:\n",
    "            print(f\"use {num_sing_vals_err}/{max_singular_values} sing vals\")\n",
    "    else:\n",
    "        num_sing_vals_err  = max_singular_values\n",
    "\n",
    "    num_sing_vals_keep = min(max_singular_values, num_sing_vals_err)\n",
    "\n",
    "\n",
    "    #s_rest = s[...,num_sing_vals_keep:]\n",
    "    u      = u[...,:num_sing_vals_keep]\n",
    "    s      = s[...,:num_sing_vals_keep]\n",
    "    v      = v[...,:num_sing_vals_keep]\n",
    "    v      = torch.transpose(v, -1, -2)#vh\n",
    "\n",
    "    if num_sing_vals_keep == s.shape[-1] and normlized:\n",
    "        Z = 1.0*torch.ones(s.shape[0])\n",
    "    else:\n",
    "        Z = torch.sum(s**2,-1).sqrt()\n",
    "\n",
    "    if output == 'RQ':\n",
    "        R = torch.einsum('iab,ibc->iac',u ,torch.diag_embed(s))\n",
    "        Q = v\n",
    "        output = [R,Q,Z]\n",
    "    elif output == 'QR':\n",
    "        Q = u\n",
    "        R = torch.einsum('iab,ibc->iac',torch.diag_embed(s),v)\n",
    "        output = [Q,R,Z]\n",
    "    else:\n",
    "        output = [u,s,v,Z]\n",
    "    if reduce:output = [t[0] for t in output]\n",
    "    return output\n",
    "def left_canonicalize_MPS(mps_line,Decomposition_Engine=torch.qr,\n",
    "                          normlization =True):\n",
    "    # for any not canonical mps line\n",
    "    # the chain size (D,P,D)\n",
    "    new_chain = []\n",
    "    R         = None\n",
    "    #Z_list    = []# record the scale information for each unit.\n",
    "    # for a perfect MPS state, we expect the norm for each tensor is 1.\n",
    "    for i,tensor in enumerate(mps_line):\n",
    "        if len(tensor.shape)==2:\n",
    "            new_tensor = torch.einsum('ab,bd->ad',R,tensor) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "        elif len(tensor.shape)==3:\n",
    "            new_tensor = torch.einsum('ab,bcd->acd',R,tensor) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "            a,b,c = shape\n",
    "            new_tensor = new_tensor.reshape(a*b,c)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if i == len(mps_line) - 1:\n",
    "            Z = torch.norm(new_tensor)\n",
    "            if normlization:new_tensor /= (Z)\n",
    "            new_chain.append(new_tensor.reshape(*shape[:-1],-1))\n",
    "        else:\n",
    "            Q,R = Decomposition_Engine(new_tensor)[:2]\n",
    "            Q   = Q.reshape(*shape[:-1],-1)\n",
    "            new_chain.append(Q)\n",
    "\n",
    "    return new_chain,[Z]\n",
    "def right_canonicalize_MPS(mps_line,Decomposition_Engine=truncated_SVD,\n",
    "                          #normlization =True\n",
    "                          ):\n",
    "    new_chain = []\n",
    "    R         = None\n",
    "    Z_list    = []\n",
    "    svd_Z         = torch.Tensor([1.0])#input has already been normalized\n",
    "    for i,tensor in enumerate(mps_line[::-1]):\n",
    "\n",
    "        if len(tensor.shape)==2:\n",
    "            new_tensor = torch.einsum('ab,bc->ac',tensor, R) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "        elif len(tensor.shape)==3:\n",
    "            new_tensor = torch.einsum('alb,bc->alc',tensor, R) if R is not None else tensor\n",
    "            shape      = new_tensor.shape\n",
    "            a,b,c      = shape\n",
    "            new_tensor = new_tensor.reshape(a,b*c)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        Z = torch.norm(new_tensor)\n",
    "        Z_list.append(Z)\n",
    "        new_tensor /= Z\n",
    "        # normlization is necessary; directly use the SVD_Z may cause problem due to precision\n",
    "        #print(f\"svd_Z{svd_Z.item()}<->all_Z {Z.item()} <-> after_Z{torch.norm(new_tensor).item()}\")\n",
    "        #if normlization:new_tensor /= Z\n",
    "        if i == len(mps_line) - 1:\n",
    "            new_chain.append(new_tensor.reshape(-1,*shape[1:]))\n",
    "        else:\n",
    "            R,Q,svd_Z = Decomposition_Engine(new_tensor)\n",
    "            Q   = Q.reshape(-1,*shape[1:])\n",
    "            new_chain.append(Q)\n",
    "    new_chain=new_chain[::-1]\n",
    "    return new_chain,Z_list\n",
    "def torchrq(tensor):\n",
    "    q, r = torch.qr(torch.transpose(tensor, -2, -1))\n",
    "    r, q = torch.transpose(r, -2, -1), torch.transpose(q, -2, -1)  #M=r*q at this point\n",
    "    return r,qx\n",
    "def approxmate_mps_line(mps_line,\n",
    "                        max_singular_values= None,max_truncation_error= None,relative = True,\n",
    "                        mode='full',left_method='qr'\n",
    "                       ):\n",
    "\n",
    "\n",
    "    scalar = 1\n",
    "    if mode != 'right':\n",
    "        if left_method == 'qr':\n",
    "            DCEngine = torch.linalg.qr if float(torch.__version__[:4])>1.07 else torch.qr\n",
    "        elif left_method == 'svd':\n",
    "            DCEngine = lambda x:truncated_SVD(x,output='QR')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        mps_line,Z_list = left_canonicalize_MPS(mps_line,Decomposition_Engine=DCEngine)\n",
    "        #print(get_mps_size_list(mps_line))\n",
    "        print(f\"   left canonical scalar:{np.prod(Z_list)}\")\n",
    "        scalar *= np.prod(Z_list)\n",
    "        #print(f\"now tensor norm: {torch.norm(mps_line[-1])}\")\n",
    "    SVD_Engine = lambda x:truncated_SVD(x,max_singular_values = max_singular_values,\n",
    "                                          max_truncation_error= max_truncation_error,\n",
    "                                          relative = relative)\n",
    "    mps_line,Z_list = right_canonicalize_MPS(mps_line,Decomposition_Engine=SVD_Engine)\n",
    "    #print(get_mps_size_list(mps_line))\n",
    "    #print(f\"   right canonical Z:{[np.round(t.item(),3) for t in Z_list]}\")\n",
    "    print(f\"   right canonical scalar:{np.prod(Z_list)}\")\n",
    "    scalar *= np.prod(Z_list)\n",
    "    return mps_line,scalar\n",
    "def diagonal_tensor_svd_torch_dense(tensor):\n",
    "    # support batch tensor\n",
    "    reduce = False\n",
    "    if len(tensor.shape)==2:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "        reduce = True\n",
    "    W,H   = tensor.shape[-2:]\n",
    "    batch_shape= tensor.shape[:-2]\n",
    "    u = s = v = None\n",
    "    if W>=H:\n",
    "        batch_diag = torch.matmul(tensor.transpose(-1,-2),tensor)#auto broadcast, or can use bmm\n",
    "    else:\n",
    "        batch_diag = torch.matmul(tensor,tensor.transpose(-1,-2))#auto broadcast, or can use bmm\n",
    "    diagnol_num = torch.diagonal(a,dim1=-2,dim2=-1).nelement()\n",
    "    tensor_num  = tensor.nelement()\n",
    "    if diagnol_num != tensor_num:return None\n",
    "    A,A        = batch_diag.shape[-2:]\n",
    "    batch_diag = batch_diag[...,range(A),range(A)]\n",
    "    batch_diag,batch_order= batch_diag.sort(-1,descending=True)\n",
    "    #fast_V      = [get_sort_matrix(order).to_dense() for order in batch_order]\n",
    "    batch_order = batch_order.flatten(start_dim=0,end_dim=-2)\n",
    "    K,A         = batch_order.shape\n",
    "    s           = batch_diag.sqrt()\n",
    "    s           = s.reshape(*batch_shape,A)\n",
    "    if W>=H:\n",
    "        v = torch.sparse_coo_tensor([list(range(K*A)),batch_order.flatten().tolist()], [1.0]*K*A,(K*A,A))\n",
    "        v = v.to_dense().reshape(-1,A,A)\n",
    "        u = torch.bmm(tensor,v.transpose(-1,-2)/s.unsqueeze(-2))\n",
    "        v = v.reshape(*batch_shape,A,A)\n",
    "        u = u.reshape(*batch_shape,W,A)\n",
    "    else:\n",
    "        u = torch.sparse_coo_tensor([list(range(K*A)),batch_order.flatten().tolist()], [1.0]*K*A,(K*A,A))\n",
    "        u = u.to_dense().reshape(-1,A,A).transpose(-1,-2)\n",
    "        v = torch.bmm(u.transpose(-1,-2)/s.unsqueeze(-1),tensor)#TODO: case when s==0\n",
    "        u = u.reshape(*batch_shape,A,A)\n",
    "        v = v.reshape(*batch_shape,A,H)\n",
    "    output = [u,s,v.transpose(-1,-2)]\n",
    "    if reduce:\n",
    "        output = [t[0] for t in output]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0+cu102'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left canonical scalar:0.5324269533157349\n",
      "   right canonical scalar:0.9999996423721313\n",
      "(4, 4)-(4, 4, 16)- 3x(16, 4, 16) -(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.17249836027622223\n",
      "   right canonical scalar:0.9999997615814209\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.141238734126091\n",
      "   right canonical scalar:0.9999996423721313\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.13460852205753326\n",
      "   right canonical scalar:0.9999997019767761\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n",
      "   left canonical scalar:0.1684599667787552\n",
      "   right canonical scalar:1.000000238418579\n",
      "(4, 4)-(4, 4, 16)-(16, 4, 64)-(64, 4, 64)-(64, 4, 16)-(16, 4, 4)-(4, 4)\n"
     ]
    }
   ],
   "source": [
    "tensor = peps[0]\n",
    "for mpo in peps[1:-1]:\n",
    "    tensor = contract_mps_mpo(tensor,mpo)\n",
    "    #print(get_mps_size_list(tensor))\n",
    "    tensor = right_mps_form(tensor)\n",
    "    tensor,scale = approxmate_mps_line(tensor,max_singular_values=100)\n",
    "    print(get_mps_size_list(tensor))\n",
    "tensor = contract_two_mps_tn(tensor,peps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def onebyoneBMPS(tensor,truncate=None,einsum_engin=torch.einsum):\n",
    "    W,H = tensor.shape[:2]\n",
    "    while W > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover  = tensor[nice_size:]\n",
    "            tensor    = torch.einsum(\"mbik,mbkj->mbij\",tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            #(k/2,NB,D,D),(k/2,NB,D,D) <-> (k/2,NB,D,D)\n",
    "            tensor   = torch.cat([tensor, leftover], axis=0)\n",
    "            size     = half_size + int(size % 2 == 1)\n",
    "    return value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
